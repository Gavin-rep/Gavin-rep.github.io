<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization | Gavin</title><meta name="author" content="Gavin"><meta name="copyright" content="Gavin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Arxiv  31 May 2023  摘要​	提取直接路径空间特征对于在不利的声学环境中定位声源至关重要。该文提出了一种全频带和窄带融合网络，用于估计麦克风信号的直接路径通道间相位差 （DP-IPD）。交替的全带层和窄带层分别负责学习 DP-IPD 的全带相关性和窄带提取。实验表明，所提出的网络在模拟和真实世界数据上都明显优于其他高级方法。 索引术语：移动声源定位、全频段、窄带、直接路径、通道间">
<meta property="og:type" content="article">
<meta property="og:title" content="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization">
<meta property="og:url" content="http://example.com/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFN-SSL%EF%BC%9AFull-Band%20and%20Narrow-Band%20Fusion%20for%20Sound%20Source%20Localization/index.html">
<meta property="og:site_name" content="Gavin">
<meta property="og:description" content="Arxiv  31 May 2023  摘要​	提取直接路径空间特征对于在不利的声学环境中定位声源至关重要。该文提出了一种全频带和窄带融合网络，用于估计麦克风信号的直接路径通道间相位差 （DP-IPD）。交替的全带层和窄带层分别负责学习 DP-IPD 的全带相关性和窄带提取。实验表明，所提出的网络在模拟和真实世界数据上都明显优于其他高级方法。 索引术语：移动声源定位、全频段、窄带、直接路径、通道间">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.ibb.co/Q7BGbDBZ/image.png">
<meta property="article:published_time" content="2025-03-19T22:39:09.062Z">
<meta property="article:modified_time" content="2025-03-19T22:39:27.528Z">
<meta property="article:author" content="Gavin">
<meta property="article:tag" content="SSL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.ibb.co/Q7BGbDBZ/image.png"><link rel="shortcut icon" href="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"><link rel="canonical" href="http://example.com/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFN-SSL%EF%BC%9AFull-Band%20and%20Narrow-Band%20Fusion%20for%20Sound%20Source%20Localization/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-20 06:39:27'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.ibb.co/0fP2mb0/image.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.ibb.co/Q7BGbDBZ/image.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Gavin"><img class="site-icon" src="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"/><span class="site-name">Gavin</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-19T22:39:09.062Z" title="发表于 2025-03-20 06:39:09">2025-03-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-19T22:39:27.528Z" title="更新于 2025-03-20 06:39:27">2025-03-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Arxiv  31 May 2023 </p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​	提取直接路径空间特征对于在不利的声学环境中定位声源至关重要。该文提出了一种全频带和窄带融合网络，用于估计麦克风信号的直接路径通道间相位差 （DP-IPD）。交替的全带层和窄带层分别负责学习 DP-IPD 的全带相关性和窄带提取。实验表明，所提出的网络在模拟和真实世界数据上都明显优于其他高级方法。</p>
<p>索引术语：移动声源定位、全频段、窄带、直接路径、通道间相位差</p>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><p>声源定位（SSL）利用多通道录音来估计声源的位置。它广泛应用于视频会议、机器人听觉等领域，同时也被用来提升语音增强和分离的性能 [1, 2]。传统的定位方法 [3–5] 通常首先估计空间特征，如时延、通道间相位&#x2F;幅度差（IPD&#x2F;ILD）以及相对传递函数（RTF），然后建立从特征到位置的映射。如果空间特征是干净的，将其映射到声源位置是比较直接的。然而，定位特征与直达路径信号的传播相关，但在实际环境中，直达路径信号往往受到噪声和混响的污染，从而严重降低了特征估计的准确性。</p>
<p>近年来，基于深度学习的定位方法得到了广泛研究 [6]。这些方法将定位任务视为特征&#x2F;位置回归问题 [7, 8] 或位置分类问题 [9, 10]。常用于运动声源定位的网络架构包括卷积神经网络（CNN） [7, 11] 和卷积递归神经网络（CRNN） [12–16]。其中，卷积层用于提取局部空间信息，而递归神经网络（RNN）则捕捉空间信息的长期时间上下文。深度学习模型的输入既可以是信号级别的（如时域信号、幅度&#x2F;相位频谱图），也可以是特征级别的（如 IPD [13]、广义互相关（GCC）函数 [4] 和空间谱 [7, 17]）。</p>
<p>基于以往的相关工作，我们知道直达路径特征既可以在窄带中估计，也可以在全带中估计。</p>
<ol>
<li><strong>窄带处理</strong>：带混响的信号通常在短时傅里叶变换（STFT）域中表达，且各个STFT频率可以独立处理，因为随机信号和混响在各频率之间在统计上是独立的。在 [18, 19] 中，直达路径特征是基于子带通道识别进行估计的。根据先行效应 [20]——即直达信号相对于反射信号更早到达，通过在 [21] 中进行相干性测试以及在 [22] 中进行直达优势测试，可以检测出直达主导的帧。</li>
<li><strong>全带处理</strong>：定向声源的直达路径特征在各频率间具有高度相关性，例如所有频率的 IPD 对应（且与之线性相关）于到达时延（TDOA） [23]。这一特性被 [13] 中的方法充分利用，该方法通过深度回归从嘈杂的 IPD 推导出位置或干净的 IPD。</li>
</ol>
<p>为了充分利用上述窄带和全带信息，本工作提出了一种用于直达路径 IPD（DP-IPD）估计的全带和窄带融合网络。该网络以多通道麦克风信号为输入，并预测在线 SSL 中的帧级 DP-IPD。这一设计遵循了 [12, 14] 中提出的 SSL 方法的原理。与处理嘈杂的 IPD [13] 或嘈杂的空间谱 [7] 相比，直接处理麦克风信号更能有效利用噪声和混响的自然特性（例如噪声的空间弥散性和后期混响）来抑制它们。</p>
<p>具体来说，所提出的全带和窄带融合网络分别采用专门的长短时记忆网络（LSTM）来处理全带和窄带信息。全带层（窄带层）独立处理每个时间帧（频率），且所有时间帧（频率）共享相同的网络参数。全带层（窄带层）的输入是沿频率轴（时间轴）上单个时间帧（频率）的序列。级联且交替的全带和窄带层分别侧重于学习 DP-IPD 的全带相关性和窄带提取。我们还设计了沿全带和窄带层的相应跳跃连接，以防止信息丢失。</p>
<p>在模拟和真实数据上的实验表明，所提出的方法明显优于近期提出的 SSL 方法，并且在强混响和噪声条件下表现优异。</p>
<p>此外，所提出的全带和窄带融合网络的设计灵感来源于最近提出的语音增强网络 [24, 25]，这些网络中通过级联一层全带 LSTM 和一层窄带 LSTM 来预测干净的语音信号。该全带和窄带融合方案在语音增强中展现了显著的性能优势，并正逐渐成为一个新的研究趋势。语音增强和 SSL 是两个密切相关的任务，分别用于估计直达语音信号和直达信号传播。在本工作中，我们重新设计了适用于 SSL 的全带和窄带融合网络，命名为 FN-SSL 网络，主要通过扩展网络使其在全带和窄带层之间交替，并设计出合适的 SSL 目标。</p>
<h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2. 方法"></a>2. 方法</h2><p>在封闭环境中，两个麦克风会观察到方向$\theta$ 处的移动声源。麦克风信号可以在 STFT 域中定义为：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250319210654465.png" alt="image-20250319210654465" style="zoom:50%;" />

<p>其中 $m\in[1,2]$，$t\in[1,T]$ 以及 $k\in[1,K]$ 分别表示麦克风、时间帧和频率的索引。$X_m(t,k)$、$S(t,k)$ 和 $N_m(t,k)$ 分别为麦克风信号、声源信号和噪声的 STFT 系数。$A_m(t,k,\theta)$ 是房间传递函数，即房间脉冲响应（RIR）的傅里叶变换。需要注意的是，由于本工作考虑运动声源，因此房间传递函数是时变的。</p>
<h3 id="2-1-学习目标"><a href="#2-1-学习目标" class="headerlink" title="2.1 学习目标"></a>2.1 学习目标</h3><h4 id="2-1-1-直接路径IPD"><a href="#2-1-1-直接路径IPD" class="headerlink" title="2.1.1 直接路径IPD"></a>2.1.1 直接路径IPD</h4><p>房间传递函数可以分解为：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250319210822736.png" alt="image-20250319210822736" style="zoom:50%;" />

<p>其中 $A_d^m(t, k, \theta)$ 和 $A_m^r(t, k, \theta)$ 分别表示直达路径和混响部分。声源方向仅与直达传播有关。在文献 [18, 19] 中，将直达路径相对传递函数（DP-RTF）定义为</p>
<p>$B^d(t,k,\theta)&#x3D;\frac{A_2^d(t,k,\theta)}{A_1^d(t,k,\theta)}$</p>
<p>并用作定位特征，该特征在其相位和幅度中分别编码了直达路径的 IPD 和 ILD。由于 IPD 比 ILD 更具判别性，因此本工作仅采用直达路径 IPD，即 $\angle B^d(t,k,\theta)$ 作为定位特征。DP-IPD 对应于声源到两个麦克风的直达传播的到达时延（TDOA）。给定方向 $\theta$ 的直达路径传递函数可通过对直达路径脉冲响应（或用于双耳定位的头相关脉冲响应）进行傅里叶变换获得。</p>
<p>FN-SSL 以双通道噪声信号（即 $X_m(t,f)$）作为输入，并输出&#x2F;预测直达路径 IPD（DP-IPD）特征。为了使实值网络的优化成为可能，对于时间帧 $t$，学习目标被设定为复值 DP-IPD 的实部和虚部（在频率维度上拼接），即</p>
<p>$[\cos \angle B_d(t,1,\theta),\sin \angle B_d(t,1,\theta),\dots,\cos \angle B_d(t,K,\theta),\sin \angle B_d(t,K,\theta)]^T,$</p>
<p>其中 $^T$ 表示向量转置。输出激活层采用 $\tanh$ 函数来预测 DP-IPD。训练过程中采用均方误差（MSE）作为损失函数。在推理阶段，将每一帧预测的 DP-IPD 向量与候选方向对应的 DP-IPD 向量进行内积计算，内积最大的候选方向即作为定位结果。候选方向在整个定位空间内进行采样。</p>
<h3 id="2-1-2-最优targets"><a href="#2-1-2-最优targets" class="headerlink" title="2.1.2 最优targets"></a>2.1.2 最优targets</h3><p>虽然所提出的网络设计用于 DP-IPD 估计，但只需将 DP-IPD 目标替换为定位类别或定位坐标，该网络也可以直接用于定位分类或回归的端到端 SSL。对于定位分类，将定位空间划分为若干定位类别，输出激活层设置为 Softmax，并采用交叉熵损失进行训练；对于定位回归，目标设为 $l_2$ 归一化后的坐标，网络输出同样进行 $l_2$ 归一化，并采用均方误差进行训练。</p>
<h3 id="2-2-网络结构"><a href="#2-2-网络结构" class="headerlink" title="2.2 网络结构"></a>2.2 网络结构</h3><p>所提出的 FN-SSL 网络结构如图 1 所示。网络输入为 STFT 系数的实部和虚部，因此输入通道数 $C$ 为麦克风数量的两倍。输入信号经过交替的全带和窄带 LSTM 层处理，其中一个全带层加上一个窄带层构成一个“全窄块”。图 1 中展示的网络包含两个全窄块，通过重复第二个块可以轻松扩展更多全窄块。全窄块的输出会经过平均池化模块以压缩帧率，随后送入一个全连接（FC）层（激活函数采用 $\tanh$）以转换到所需的输出维度。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250320060951424.png" alt="image-20250320060951424" style="zoom:50%;" />

<h3 id="2-2-1-全带（Full-band-BLSTM-层"><a href="#2-2-1-全带（Full-band-BLSTM-层" class="headerlink" title="2.2.1 全带（Full-band) BLSTM 层"></a>2.2.1 全带（Full-band) BLSTM 层</h3><p>全带 BLSTM 层独立处理时间帧，并且所有时间帧共享相同的网络参数。输入是沿单个时间帧的频率轴的序列：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250320061144189.png" alt="image-20250320061144189" style="zoom:50%;" />

<p>其中，上标 $f$ 表示全带层，$h^f(t,k) \in \mathbb{R}^{D \times 1}$ 表示某个时频（TF）单元的隐藏向量。对于第一个全带层，$h^f(t,k)$ 表示麦克风信号，而对于其他全带层，则为前一窄带层的输出。全带层主要用于学习空间&#x2F;定位线索的频间依赖关系。由于不同频率的 DP-IPD 都源自相同的到达时延（TDOA），因此它们之间具有很强的相关性。此外，那些直达能量较低的频率，其空间&#x2F;定位线索可以借助其他频率的信息得到较好的预测。全带层不学习任何时域信息，这部分信息留待后续的窄带层处理。</p>
<h3 id="2-2-2-窄带（B）LSTM层"><a href="#2-2-2-窄带（B）LSTM层" class="headerlink" title="2.2.2 窄带（B）LSTM层"></a>2.2.2 窄带（B）LSTM层</h3><p>窄带 （B）LSTM 层独立处理频率，所有频率共享相同的网络参数。输入是沿单个频率的时间轴的序列：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250320061421207.png" alt="image-20250320061421207" style="zoom:50%;" />

<p>其中，上标 $n$ 表示窄带层。输入隐藏向量 $h^n(t,k) \in \mathbb{R}^{D \times 1}$ 是上一全带层的输出向量。在窄带中估计直达路径定位特征已在许多传统方法中得到研究，例如 [18] 中的通道识别、[21] 中的相干性测试以及 [22] 中的直达优势测试。所提出的窄带层主要用于挖掘这些窄带通道间信息。此外，由于对于运动声源来说 DP-IPD 是时变的，窄带层还学习 DP-IPD 的时间演变。</p>
<h3 id="2-2-3-Skip-Connections"><a href="#2-2-3-Skip-Connections" class="headerlink" title="2.2.3 Skip Connections"></a>2.2.3 Skip Connections</h3><p>全带层和窄带层分别关注其对应的信息。经过全带层后可能会丢失窄带信息，反之亦然。因此，我们增加了跳跃连接以避免信息丢失。如图 1 所示，窄带麦克风信号与第一个全带层的输出进行拼接，作为第一个窄带层的输入。此后，我们将前一全带层（或窄带层）的输出添加到下一全带层（或窄带层）的输入中。</p>
<h3 id="2-2-4-模型因果性"><a href="#2-2-4-模型因果性" class="headerlink" title="2.2.4 模型因果性"></a>2.2.4 模型因果性</h3><p>所提出的网络可以很方便地应用于离线或在线 SSL，只需将窄带 LSTM 分别设置为双向或单向即可。为了使模型更易于优化，对网络输入进行了 Laplace 归一化，即</p>
<p>​										$\frac{X_m(t,f)}{\mu(t)},$</p>
<p>其中 $\mu(t)$ 是归一化因子。对于离线 SSL，$\mu(t)$ 的计算公式为</p>
<p>​							$\mu(t)&#x3D;\frac{1}{TK}\sum_{t&#x3D;1}^{T}\sum_{k&#x3D;1}^{K}\left|X_m(t,f)\right|.$</p>
<p>而对于在线 SSL，为确保方法的因果性，$\mu(t)$ 采用递归计算的方式：</p>
<p>​				      	$\mu(t)&#x3D;\alpha,\mu(t-1)+(1-\alpha)\frac{1}{K}\sum_{k&#x3D;1}^{K}\left|X_m(t,k)\right|,$</p>
<p>其中 $\alpha&#x3D;\frac{L-1}{L+1}$ 表示历史时间帧的平滑权重，$L$ 为平滑窗口的长度。</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h2><h3 id="3-1-数据集和配置"><a href="#3-1-数据集和配置" class="headerlink" title="3.1 数据集和配置"></a>3.1 数据集和配置</h3><p>所提出的方法在模拟数据集和真实数据集上均进行了评估。我们采用两只麦克风来定位 180° 方位角内的到达方向（DOA）。</p>
<h4 id="模拟数据集"><a href="#模拟数据集" class="headerlink" title="模拟数据集"></a>模拟数据集</h4><ul>
<li><strong>数据生成</strong><br>麦克风信号通过将房间脉冲响应（RIRs）与语音信号卷积得到。用于模型训练、验证和测试的语音信号分别从 LibriSpeech 语料库 [26] 的训练集、开发集和测试集中随机选择。RIRs 通过 gpuRIR 工具箱 [27] 生成。<ul>
<li>房间混响时间（RT60）在 [0.2, 1.3] 秒范围内随机设置。</li>
<li>房间尺寸随机设置在 6×6×2.5 m 至 10×8×6 m 之间。</li>
<li>语音信号的运动轨迹按照 [7] 生成，并且每个语音信号具有固定的高度。</li>
<li>两只麦克风以 8 cm 的间距随机放置在房间中，且与声源位于同一水平面上。</li>
</ul>
</li>
<li><strong>噪声设置</strong><br>我们采用 NOISEX-92 数据库 [28] 中的白噪声、餐厅噪声（babble）和工厂噪声作为噪声源，并按照 [29] 构建漫反射声场。生成的漫反射噪声信号根据随机选取的信噪比（SNR，范围为 -5 dB 至 15 dB）添加到干净的传感器信号中。</li>
<li><strong>数据量</strong><br>用于训练、验证和测试的语句数分别为 166,816、992 和 5,000。</li>
</ul>
<h4 id="真实数据集"><a href="#真实数据集" class="headerlink" title="真实数据集"></a>真实数据集</h4><ul>
<li><p>数据描述</p>
<p>我们在 LOCATA 数据集 [30] 的任务 3 和任务 5 上评估该方法。</p>
<ul>
<li>房间尺寸为 7.1×9.8×3 m，混响时间为 0.55 s。</li>
<li>使用 DICIT 阵列 [31] 中的麦克风 6 和 9 进行评估，其配置与模拟麦克风阵列相同。</li>
<li>注意：任务 5 中有四个录音的方位角超出了 [0, 180°] 的范围，实验中不考虑这些录音。</li>
<li>模型使用模拟数据集训练后，直接在 LOCATA 数据集上测试。</li>
</ul>
</li>
</ul>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><ul>
<li><strong>采样率及 STFT 参数</strong><ul>
<li>采样率为 16 kHz。</li>
<li>STFT 窗长为 512 个采样点（32 ms），帧移为 256 个采样点（16 ms），采用 Hanning 窗。</li>
<li>训练时使用的音频长度为 4.79 s。</li>
</ul>
</li>
<li><strong>输入与网络结构</strong><ul>
<li>将 STFT 系数的实部与虚部进行拼接，作为网络输入，输入通道数为 $C&#x3D;2M$（其中 $M&#x3D;2$ 为麦克风数量）。</li>
<li>训练过程中，时间帧从 298 帧池化为 24 帧。</li>
<li>每个（B）LSTM 层的输出维度均设为 256。</li>
</ul>
</li>
<li><strong>训练细节</strong><ul>
<li>损失函数采用均方误差（MSE）。</li>
<li>优化器为 Adam，批次大小设为 16。</li>
<li>初始学习率为 0.001，学习率以指数衰减因子 0.8988 进行衰减。</li>
<li>训练共进行 15 个 epoch。</li>
</ul>
</li>
<li><strong>评估指标</strong><br>采用平均绝对误差（MAE）和定位准确率（ACC）作为评估指标。<ul>
<li>ACC ($N^\circ$) 表示定位误差小于 $N^\circ$ 的帧的比例。</li>
<li>模拟数据集的候选方位角分辨率设为 5°，真实数据集的候选分辨率设为 2.5°。</li>
<li>仅使用非静音帧进行性能评估。</li>
</ul>
</li>
<li><strong>代码</strong><br>代码已在网站上公开1。</li>
</ul>
<h4 id="对比方法"><a href="#对比方法" class="headerlink" title="对比方法"></a>对比方法</h4><p>我们与以下运动声源定位方法进行了比较：</p>
<ol>
<li><strong>Cross3D [7]</strong><ul>
<li>以 SRP-PHAT 空间谱作为输入，采用因果 3D CNN 网络进行运动声源跟踪。</li>
</ul>
</li>
<li><strong>IcoCNN [11]</strong><ul>
<li>利用 Icosahedral CNN 从 SRP-PHAT 空间谱中提取定位特征，并使用因果 1D CNN 结合时间上下文进行运动声源跟踪。</li>
</ul>
</li>
<li><strong>SRP-DNN [12]</strong><ul>
<li>采用因果 CRNN 网络估计运动声源的 DP-RTF 特征。</li>
</ul>
</li>
<li><strong>SELDnet [16]</strong><ul>
<li>DCASE22 的 CRNN 基线方法，其输入特征为频率归一化的 IPD 与幅度谱拼接而成。</li>
</ul>
</li>
<li><strong>SE-Resnet [15]</strong><ul>
<li>DCASE22 中排名靠前的方法，采用 squeeze-and-excitation 残差网络作为编码器和门控循环单元（GRU）网络作为解码器。</li>
</ul>
</li>
<li><strong>SALSA-Lite [13]</strong><ul>
<li>采用与 SELDnet 相同的输入（频率归一化的 IPD 拼接幅度谱），并使用 ResNet-GRU 网络。</li>
</ul>
</li>
</ol>
<p>对于 SELDnet、SALSA-Lite 和 SE-Resnet 这三种方法，它们均用于联合声事件检测与定位，我们仅使用它们的定位分支。所有对比方法均在与所提方法相同的数据集上进行复现和训练。我们修改了各方法的时间池化核大小，以确保所有方法输出的帧率一致。</p>
<h3 id="3-2-实验结果"><a href="#3-2-实验结果" class="headerlink" title="3.2 实验结果"></a>3.2 实验结果</h3><p>本文在在线 SSL 任务上进行了消融实验，结果详见表 1。实验中，“Block×N”表示将 N 个全窄块级联，每个块均以 DP-IPD 作为学习目标。实验发现，随着块数的增加，定位性能逐步提升，尤其是从 1 块增加到 2 块时提升显著，这表明全窄块作为构建单元在 DP-IPD 估计上非常有效，并且堆叠更多块能够挖掘更深层的信息。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250320062645740.png" alt="image-20250320062645740" style="zoom:50%;" />

<p>另外，“Block×3 w&#x2F;o skip.” 的实验结果表明，当去除跳跃连接后，性能下降了，这验证了跳跃连接在信息传输中的重要作用，同时也能加速训练的收敛。</p>
<p>此外，本文还探讨了直接将学习目标设置为定位类别或定位坐标，以实现端到端的定位分类或回归。实验中：</p>
<ul>
<li>使用 180 个类别（对应每 1° 的方位角）进行定位分类，效果与使用 DP-IPD 学习目标相当；</li>
<li>定位回归的表现则略逊于前两者，并且训练时的收敛速度较慢。</li>
</ul>
<p>在后续的对比实验中，作者采用了 Block×3 版本的模型。</p>
<p>对于模拟数据，表 2 中报告了两种典型声学条件下的结果：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250320062828756.png" alt="image-20250320062828756" style="zoom:50%;" />

<ul>
<li>一种是 [SNR &#x3D; 0 dB, RT60 &#x3D; 1 s]（非常嘈杂）；</li>
<li>另一种是 [SNR &#x3D; 15 dB, RT60 &#x3D; 0.2 s]（相对安静）。<br>AVG 表示在整个测试集（SNR ∈ [−5, 15] dB, RT60 ∈ [0.2, 1.3] s）上的平均性能。<br>在在线实验中，所提出的方法与 SRP-DNN 均直接处理麦克风信号并预测直达路径定位特征，因此在抑制噪声和混响干扰方面表现更优，比使用 SRP-PHAT 空间谱输入的 Cross3D 和 IcoCNN 更具优势。特别是在非常嘈杂的条件下，<strong>本文方法相对于 SRP-DNN 取得了更好的性能，这说明全带和窄带融合网络在捕捉空间线索方面比 SRP-DNN 中的全带 CRNN 更为有效。</strong></li>
</ul>
<p>在离线实验中，所提出的方法同样达到了最佳的性能。相比之下，SALSA-Lite、SEResnet 和 SELDnet 都采用了嘈杂 IPD（与幅度谱拼接）作为输入，其在抑制噪声和混响干扰方面效果不如直接处理原始麦克风信号的方法。</p>
<p>对于真实数据，表 3 和图 2 展示了 LOCATA 数据集上的结果。需要注意的是，由于所有方法均存在大约 4° 的 DOA 估计偏差（可能由标注偏差引起），作者均通过减去 4° 来补偿该偏差。该数据集的声学条件较好（几乎无噪声且 RT60 为 0.55 s），因此各方法均取得了较为理想的定位结果。然而，由于数据中存在一些突然转向（如图 2 所示的“锯齿”现象），离线方法在这些区域出现了过平滑问题，反而表现不如在线方法。总体来看，所提出的方法明显优于所有对比方法。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250320063225819.png" alt="image-20250320063225819" style="zoom:50%;" />

<h2 id="4-结论"><a href="#4-结论" class="headerlink" title="4. 结论"></a>4. 结论</h2><p>该文提出了 FN-SSL 网络，它分别利用了专用的全频带层和窄带层的频间依赖性和信道间信息。FN-SSL 可以在线和离线方式执行，并用于定位特征提取和端到端位置分类或回归。实验结果表明，所提出的 FN-SSL 网络优于其他方法。此外，使用模拟数据训练的 FN-SSL 网络可以很好地推广到真实世界的数据。</p>
<h2 id="5-References"><a href="#5-References" class="headerlink" title="5. References"></a><strong>5. References</strong></h2><p>[1] H.-Y. Lee, J.-W. Cho, M. Kim, and H.-M. Park, “DNN-based feature enhancement using doa-constrained ICA for robust speech recognition,” <em>IEEE Signal Processing Letters</em>, vol. 23, no. 8, pp. 1091–1095, Aug. 2016.</p>
<p>[2] S. E. Chazan, H. Hammer, G. Hazan, J. Goldberger, and S. Gannot, “Multi-microphone speaker separation based on deep DOA estimation,” in <em>Proc. EUSIPCO 2019 European Signal Processing Conference</em>, A Coruña, Spain, Sep. 2019, pp. 1–5.</p>
<p>[3] W. Zhang and B. D. Rao, “A two microphone-based approach for source localization of multiple speech sources,” <em>IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 18, no. 8, pp. 1913–1928, Nov. 2010.</p>
<p>[4] C. H. Knapp and G. C. Carter, “The generalized correlation method for estimation of time delay,” <em>IEEE&#x2F;ACM Transactions on Acoustics, Speech, and Signal Processing</em>, vol. 24, no. 4, pp. 320–327, Aug. 1976.</p>
<p>[5] M. Raspaud, H. Viste, and G. Evangelista, “Binaural source localization by joint estimation of ILD and ITD,” <em>IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 18, no. 1, pp. 68–77, 2010.</p>
<p>[6] P.-A. Grumiaux, S. Kitić, L. Girin, and A. Guérin, “A survey of sound source localization with deep learning methods,” <em>The Journal of the Acoustical Society of America</em>, vol. 152, no. 1, pp. 107–151, Jul. 2022.</p>
<p>[7] D. Diaz-Guerra, A. Miguel, and J. R. Beltrán, “Robust sound source tracking using SRP-PHAT and 3D convolutional neural networks,” <em>IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 29, pp. 300–311, Nov. 2020.</p>
<p>[8] B. Yang, X. Li, and H. Liu, “Supervised direct-path relative transfer function learning for binaural sound source localization,” in <em>Proc. ICASSP 2021 IEEE International Conference on Acoustics, Speech and Signal Processing</em>, Toronto, Canada, Jun. 2021, pp. 825–829.</p>
<p>[9] S. Chakrabarty and E. Habets, “Multi-speaker DOA estimation using deep convolutional networks trained with noise signals,” <em>IEEE Journal of Selected Topics in Signal Processing</em>, vol. 13, pp. 8–21, 2018.</p>
<p>[10] N. Ma, T. May, and G. J. Brown, “Exploiting deep neural networks and head movements for robust binaural localization of multiple sources in reverberant environments,” <em>IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 25, no. 12, pp. 2444–2453, 2017.</p>
<p>[11] D. Diaz-Guerra, A. Miguel, and J. R. Beltrán, “Direction of arrival estimation of sound sources using Icosahedral CNNs,” <em>IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 31, pp. 313–321, Nov. 2022.</p>
<p>[12] B. Yang, H. Liu, and X. Li, “SRP-DNN: Learning direct-path phase difference for multiple moving sound source localization,” in <em>Proc. ICASSP 2022 IEEE International Conference on Acoustics, Speech and Signal Processing</em>, Singapore, May 2022, pp. 721–725.</p>
<p>[13] T. N. T. Nguyen, D. L. Jones, K. N. Watcharasupat, H. A. Phan, and W. Gan, “SALSA-Lite: A fast and effective feature for polyphonic sound event localization and detection with microphone arrays,” in <em>Proc. ICASSP 2022 IEEE International Conference on Acoustics, Speech and Signal Processing</em>, Singapore, May 2022, pp. 716–720.</p>
<p>[14] B. Yang, H. Liu, and X. Li, “Learning deep direct-path relative transfer function for binaural sound source localization,” <em>IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 29, pp. 3491–3503, Oct. 2021.</p>
<p>[15] J. S. Kim, H. J. Park, W. Shin, and S. W. Han, “A robust framework for sound event localization and detection on real recordings,” <em>DCASE2022 Challenge, Technical Report</em>, Jun. 2022.</p>
<p>[16] S. Adavanne, A. Politis, J. Nikunen, and T. Virtanen, “Sound event localization and detection of overlapping sources using convolutional recurrent neural networks,” <em>IEEE Journal of Selected Topics in Signal Processing</em>, vol. 13, no. 1, pp. 34–48, Mar. 2018.</p>
<p>[17] J. H. DiBiase, H. F. Silverman, and M. S. Brandstein, <em>Robust Localization in Reverberant Rooms</em>. Berlin, Heidelberg: Springer, 2001.</p>
<p>[18] X. Li, L. Girin, R. Horaud, and S. Gannot, “Estimation of the direct-path relative transfer function for supervised sound-source localization,” <em>IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 24, no. 11, pp. 2171–2186, 2016.</p>
<p>[19] X. Li, Y. Ban, L. Girin, X. Alameda-Pineda, and R. Horaud, “Online localization and tracking of multiple moving speakers in reverberant environments,” <em>IEEE Journal of Selected Topics in Signal Processing</em>, vol. 13, no. 1, pp. 88–103, Mar. 2019.</p>
<p>[20] R. Y. Litovsky, H. S. Colburn, W. A. Yost, and S. J. Guzman, “The precedence effect,” <em>The Journal of the Acoustical Society of America</em>, vol. 106, no. 4, pp. 1633–1654, 1999.</p>
<p>[21] S. Mohan, M. E. Lockwood, M. L. Kramer, and D. L. Jones, “Localization of multiple acoustic sources with small arrays using a coherence test,” <em>The Journal of the Acoustical Society of America</em>, vol. 123, no. 4, pp. 2136–2147, 2008.</p>
<p>[22] O. Nadiri and B. Rafaely, “Localization of multiple speakers under high reverberation using a spherical microphone array and the direct-path dominance test,” <em>IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 22, no. 10, pp. 1494–1505, Oct. 2014.</p>
<p>[23] S. Gannot, E. Vincent, S. Markovich-Golan, and A. Ozerov, “A consolidated perspective on multimicrophone speech enhancement and source separation,” <em>IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 25, no. 4, pp. 692–730, Jan. 2017.</p>
<p>[24] K. Tesch and T. Gerkmann, “Insights into deep non-linear filters for improved multi-channel speech enhancement,” <em>IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 31, pp. 563–575, Nov. 2022.</p>
<p>[25] Y. Yang, C. Quan, and X. Li, “McNet: Fuse multiple cues for multichannel speech enhancement,” in <em>Proc. ICASSP 2023 IEEE International Conference on Acoustics, Speech and Signal Processing</em>, Greek island of Rhodes, Jun. 2023.</p>
<p>[26] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An ASR corpus based on public domain audio books,” in <em>Proc. ICASSP 2015 IEEE International Conference on Acoustics, Speech and Signal Processing</em>, Brisbane, Australia, Apr. 2015, pp. 5206–5210.</p>
<p>[27] D. Diaz-Guerra, A. Miguel, and J. R. Beltrán, “gpuRIR: a python library for room impulse response simulation with GPU acceleration,” <em>Multimedia Tools and Applications</em>, vol. 80, pp. 5653–5671, Oct. 2018.</p>
<p>[28] A. Varga and H. J. M. Steeneken, “Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems,” <em>Speech Communication</em>, vol. 12, pp. 247–251, Jul. 1993.</p>
<p>[29] E. Habets, I. Cohen, and S. Gannot, “Generating nonstationary multisensor signals under a spatial coherence constraint,” <em>The Journal of the Acoustical Society of America</em>, vol. 124, pp. 2911–2917, Aug. 2008.</p>
<p>[30] H. W. Löllmann, C. Evers, A. Schmidt, H. Mellmann, H. Barfuss, P. A. Naylor, and W. Kellermann, “The LOCATA challenge data corpus for acoustic source localization and tracking,” in <em>Proc. SAM 2018 IEEE Sensor Array and Multichannel Signal Processing Workshop</em>, Sheffield, UK, Jul. 2018, pp. 410–414.</p>
<p>[31] A. Brutti, L. Cristoforetti, W. Kellermann, L. Marquardt, and M. Omologo, “WOZ acoustic data collection for interactive TV,” in <em>Proc. LREC 2008 International Conference on Language Resources and Evaluation</em>, Marrakech, Morocco, May 2008.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Gavin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFN-SSL%EF%BC%9AFull-Band%20and%20Narrow-Band%20Fusion%20for%20Sound%20Source%20Localization/">http://example.com/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFN-SSL%EF%BC%9AFull-Band%20and%20Narrow-Band%20Fusion%20for%20Sound%20Source%20Localization/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Gavin</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/SSL/">SSL</a></div><div class="post_share"><div class="social-share" data-image="https://i.ibb.co/Q7BGbDBZ/image.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/" title="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization"><img class="cover" src="https://i.ibb.co/S7ZQZGLX/image.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization</div></div></a></div><div class="next-post pull-right"><a href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/" title="文献阅读: Masked Autoencoders that Listen"><img class="cover" src="https://i.ibb.co/9HZr35S/image.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">文献阅读: Masked Autoencoders that Listen</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/" title="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization"><img class="cover" src="https://i.ibb.co/S7ZQZGLX/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-20</div><div class="title">文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.ibb.co/0fP2mb0/image.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Gavin</div><div class="author-info__description">我的过去常常追赶着我</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/alexandergwm"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/alexandergwm" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:wenmiaogao@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">May you stay young forever, do the thing in your own zone.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text">1. 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">2. 方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 学习目标</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-1-%E7%9B%B4%E6%8E%A5%E8%B7%AF%E5%BE%84IPD"><span class="toc-number">3.1.1.</span> <span class="toc-text">2.1.1 直接路径IPD</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2-%E6%9C%80%E4%BC%98targets"><span class="toc-number">3.2.</span> <span class="toc-text">2.1.2 最优targets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">3.3.</span> <span class="toc-text">2.2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-%E5%85%A8%E5%B8%A6%EF%BC%88Full-band-BLSTM-%E5%B1%82"><span class="toc-number">3.4.</span> <span class="toc-text">2.2.1 全带（Full-band) BLSTM 层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-%E7%AA%84%E5%B8%A6%EF%BC%88B%EF%BC%89LSTM%E5%B1%82"><span class="toc-number">3.5.</span> <span class="toc-text">2.2.2 窄带（B）LSTM层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-3-Skip-Connections"><span class="toc-number">3.6.</span> <span class="toc-text">2.2.3 Skip Connections</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-4-%E6%A8%A1%E5%9E%8B%E5%9B%A0%E6%9E%9C%E6%80%A7"><span class="toc-number">3.7.</span> <span class="toc-text">2.2.4 模型因果性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.</span> <span class="toc-text">3. 实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E9%85%8D%E7%BD%AE"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 数据集和配置</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E6%8B%9F%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">4.1.1.</span> <span class="toc-text">模拟数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9C%9F%E5%AE%9E%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">4.1.2.</span> <span class="toc-text">真实数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE"><span class="toc-number">4.1.3.</span> <span class="toc-text">配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E6%96%B9%E6%B3%95"><span class="toc-number">4.1.4.</span> <span class="toc-text">对比方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 实验结果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E7%BB%93%E8%AE%BA"><span class="toc-number">5.</span> <span class="toc-text">4. 结论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-References"><span class="toc-number">6.</span> <span class="toc-text">5. References</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/" title="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization"><img src="https://i.ibb.co/S7ZQZGLX/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization"/></a><div class="content"><a class="title" href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/" title="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization">文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization</a><time datetime="2025-03-19T22:40:18.592Z" title="发表于 2025-03-20 06:40:18">2025-03-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFN-SSL%EF%BC%9AFull-Band%20and%20Narrow-Band%20Fusion%20for%20Sound%20Source%20Localization/" title="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization"><img src="https://i.ibb.co/Q7BGbDBZ/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization"/></a><div class="content"><a class="title" href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFN-SSL%EF%BC%9AFull-Band%20and%20Narrow-Band%20Fusion%20for%20Sound%20Source%20Localization/" title="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization">文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization</a><time datetime="2025-03-19T22:39:09.062Z" title="发表于 2025-03-20 06:39:09">2025-03-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/" title="文献阅读: Masked Autoencoders that Listen"><img src="https://i.ibb.co/9HZr35S/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Masked Autoencoders that Listen"/></a><div class="content"><a class="title" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/" title="文献阅读: Masked Autoencoders that Listen">文献阅读: Masked Autoencoders that Listen</a><time datetime="2024-11-19T13:34:57.816Z" title="发表于 2024-11-19 21:34:57">2024-11-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners"><img src="https://i.ibb.co/gD7cJGh/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Masked Autoencoders Are Scalable Vision Learners"/></a><div class="content"><a class="title" href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners">文献阅读: Masked Autoencoders Are Scalable Vision Learners</a><time datetime="2024-11-17T12:30:01.804Z" title="发表于 2024-11-17 20:30:01">2024-11-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%20-%20Robustness%20and%20Regularization%20of%20Personal%20Audio%20Systems/" title="文献阅读: Robustness and Regularization of Personal Audio Systems"><img src="https://i.ibb.co/c1YpYsm/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Robustness and Regularization of Personal Audio Systems"/></a><div class="content"><a class="title" href="/2024/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%20-%20Robustness%20and%20Regularization%20of%20Personal%20Audio%20Systems/" title="文献阅读: Robustness and Regularization of Personal Audio Systems">文献阅读: Robustness and Regularization of Personal Audio Systems</a><time datetime="2024-10-27T13:47:25.740Z" title="发表于 2024-10-27 21:47:25">2024-10-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Gavin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Have a nice day!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>