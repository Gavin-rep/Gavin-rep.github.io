<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization | Gavin</title><meta name="author" content="Gavin"><meta name="copyright" content="Gavin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Arxiv  11 May 2024 摘要在恶劣声学环境下，提取直达路径的空间特征对于声源定位至关重要。本文提出了 IPDnet，一种能够从麦克风阵列信号中估计声源直达路径通道间相位差（DP-IPD）的神经网络。基于已知的麦克风阵列几何结构，估计得到的 DP-IPD 可轻松转换为声源位置。首先，本文提出了一种用于 DP-IPD 估计的全带与窄带融合网络，其中交替的窄带层和全带层分别负责在单个频段中">
<meta property="og:type" content="article">
<meta property="og:title" content="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization">
<meta property="og:url" content="http://example.com/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/index.html">
<meta property="og:site_name" content="Gavin">
<meta property="og:description" content="Arxiv  11 May 2024 摘要在恶劣声学环境下，提取直达路径的空间特征对于声源定位至关重要。本文提出了 IPDnet，一种能够从麦克风阵列信号中估计声源直达路径通道间相位差（DP-IPD）的神经网络。基于已知的麦克风阵列几何结构，估计得到的 DP-IPD 可轻松转换为声源位置。首先，本文提出了一种用于 DP-IPD 估计的全带与窄带融合网络，其中交替的窄带层和全带层分别负责在单个频段中">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.ibb.co/S7ZQZGLX/image.png">
<meta property="article:published_time" content="2025-03-19T22:40:18.592Z">
<meta property="article:modified_time" content="2025-03-20T23:25:02.268Z">
<meta property="article:author" content="Gavin">
<meta property="article:tag" content="SSL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.ibb.co/S7ZQZGLX/image.png"><link rel="shortcut icon" href="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"><link rel="canonical" href="http://example.com/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-21 07:25:02'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.ibb.co/0fP2mb0/image.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.ibb.co/S7ZQZGLX/image.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Gavin"><img class="site-icon" src="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"/><span class="site-name">Gavin</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-19T22:40:18.592Z" title="发表于 2025-03-20 06:40:18">2025-03-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-20T23:25:02.268Z" title="更新于 2025-03-21 07:25:02">2025-03-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Arxiv  11 May 2024</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在恶劣声学环境下，提取直达路径的空间特征对于声源定位至关重要。本文提出了 IPDnet，一种能够从麦克风阵列信号中估计声源直达路径通道间相位差（DP-IPD）的神经网络。基于已知的麦克风阵列几何结构，估计得到的 DP-IPD 可轻松转换为声源位置。首先，本文提出了一种用于 DP-IPD 估计的全带与窄带融合网络，其中交替的窄带层和全带层分别负责在单个频段中估计粗略的 DP-IPD 信息以及捕捉 DP-IPD 的频率相关性。其次，针对灵活数量声源的定位问题，提出了一种新的多轨 DP-IPD 学习目标。第三，IPDnet 被扩展为可处理可变的麦克风阵列——一旦训练完成，该模型能够处理具有不同通道数和阵列拓扑结构的任意麦克风阵列。在模拟数据和真实数据上进行的多运动说话人定位实验表明，所提出的全带与窄带融合网络与多轨 DP-IPD 学习目标相结合，能够实现卓越的声源定位性能。此外，所提出的可变阵列模型对未见过的麦克风阵列具有良好的泛化能力。代码已在我们的 GitHub 页面上公开。</p>
<p>索引术语：声源定位，直接路径IPD，成频段和窄带融合网络，麦克风阵列概括，多源。</p>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><p>声源定位（SSL）的目标是从麦克风阵列信号中估计一个或多个声源的位置。SSL 广泛应用于视频会议和人机交互中。SSL 的空间线索还可以用于提升语音增强和源分离任务的性能 [1]、[2]。传统的 SSL 方法通常依赖于估计与直达信号传播相关的空间特征，以便建立特征与声源位置之间的映射。常用的空间特征包括时延、通道间相位&#x2F;幅度差（IPD&#x2F;ILD） [3]、[4] 以及相对传递函数（RTF） [5]、[6]。实际上，在理想的声学条件下（无噪声和无混响），上述空间特征可以直接而简单地估计出来。但在真实场景中，由于噪声、混响以及多个运动声源的存在，这些会给麦克风信号引入不确定的声学失真，而混响则会对原本无混响的信号产生遮蔽效应或音色改变，从而使得从麦克风信号中估计出可靠特征变得具有挑战性 [7]。与此同时，声源运动使得空间线索呈现出时变特性，这些因素共同导致了定位性能的显著下降。</p>
<p>近年来，基于深度学习的 SSL 方法得到了广泛发展，并在复杂环境下展现出优于传统方法的性能 [8]–[20]。这种优势源于神经网络具备学习复杂模式和捕捉声学信号中细微差异的能力。</p>
<p><strong>本文工作</strong>提出了一种全新的 SSL 网络，用于在噪声和混响环境下进行多运动声源定位，这是我们之前会议论文 FN-SSL [20] 的扩展版本。FN-SSL 是一种针对双麦克风单声源定位设计的全带与窄带融合网络，因其高效的网络结构而取得了出色的 SSL 性能。本文工作在此基础上将 FN-SSL 扩展到多麦克风和多声源定位，并提出了一种新的学习目标。此外，基于扩展后的网络，还提出了一种新的可变阵列模型，该模型可应用于具有不同阵列拓扑结构的各种麦克风阵列。</p>
<p>具体贡献如下：</p>
<ol>
<li><strong>全带与窄带融合 SSL 网络</strong>：<br> 本文提出的网络受到最近提出的语音增强网络 [21]–[23] 的启发，这些网络中全带层与窄带层级联用于预测干净语音信号，在语音增强领域已显示出显著性能优势，并逐渐成为新的研究趋势。所提网络以多通道麦克风信号（短时傅里叶变换，STFT）作为输入，预测作为定位特征的直达路径 IPD（DP-IPD）。与处理嘈杂 IPD [24] 或嘈杂空间谱 [12]、[13] 相比，麦克风信号保留了噪声和混响的自然属性（例如噪声的空间弥散性和后期混响），从而更有效地利用这些属性进行抑制。在该网络中，全带&#x2F;窄带层分别独立处理时间帧&#x2F;频率，并且所有时间帧&#x2F;频率共享相同的网络权重。在窄带中存在丰富的用于提取定位特征的信息，这在传统方法中已得到广泛利用，例如通过窄带通道识别提取定位特征 [25]、通过相干性测试 [26] 以及通过直达优势测试 [27]。窄带层沿时间序列处理，侧重于学习这些窄带信息；而全带层沿频率序列处理，侧重于学习空间线索的全带相关性，比如 DP-IPD 与频率之间的线性关系。</li>
<li><strong>多轨 DP-IPD 学习目标</strong>：<br> 目前流行的 SSL 学习目标包括定位分类 [10]、定位回归 [28] 和空间谱回归 [9]。本文提出采用多轨 DP-IPD 作为多声源定位的学习目标。DP-IPD 指的是直达传播的 IPD，从理论上讲它与麦克风阵列的几何结构及声源位置密切相关，因此利用已知阵列几何结构，DP-IPD 可以很容易转换为声源位置。本文通过简单地将估计的 DP-IPD 与候选位置的理论 DP-IPD 进行匹配来实现声源定位。多轨 DP-IPD 意味着让网络同时输出&#x2F;估计多个声源的 DP-IPD。DP-IPD 是一种信号级定位特征，仅基于信号级信息即可从麦克风信号中估计；而相较之下，其他目标通常依赖于阵列，并且需要经过从定位特征到目标的转换步骤，这可能增加难度。我们在实验中（见 Section V-E）验证了这一点。DP-IPD 对于活跃声源来说定义明确，但对于非声源帧则不适用。一个简单的非声源表示可以是全零向量，但直接在输出空间中同时学习 DP-IPD 和全零向量并不容易。本文工作提出采用整个定位空间的 DP-IPD 均值点作为非声源的表示，使得在声源和非声源帧之间的切换更加容易（如图 7 所示）。</li>
<li><strong>可变阵列 SSL</strong>：<br> 目前大多数 SSL 网络都是阵列依赖型的，即训练和测试使用相同的麦克风阵列。对于新阵列，网络需要重新训练，这不仅耗时，而且当使用真实数据进行训练时，为新阵列收集大量带注释的数据更是费时费力。本文提出了一种可变阵列 SSL 模型，训练完成后可以直接应用于任何未见过的麦克风阵列。具体来说，该方法采用成对处理麦克风，并利用成对隐藏单元的均值池化实现麦克风对之间的信息交流。这种成对处理加均值池化的方案能够适应不同数量麦克风的情况，其灵感来源于可变阵列语音增强网络 [29]–[32]。该网络输出针对不同麦克风对的 DP-IPD 估计，由于 DP-IPD 估计是一项信号级任务，因此单个网络可以轻松应对不同阵列的任务。在测试阶段，利用已知阵列几何结构，可以将估计的 DP-IPD 用于声源定位。</li>
</ol>
<p>实验在模拟数据和真实数据上均已进行，结果表明所提出的学习目标优于所有比较目标，而整体方法相比最近提出的基线方法有显著优势。此外，所提出的可变阵列模型在未见过的模拟和真实麦克风阵列上均展现出良好的泛化能力。</p>
<p>本文的其余部分安排如下：</p>
<ul>
<li><strong>第二部分</strong>综述了相关文献；</li>
<li><strong>第三部分</strong>定义了多运动声源定位问题；</li>
<li><strong>第四部分</strong>详细介绍了所提方法；</li>
<li><strong>第五部分</strong>展示了实验结果及讨论；</li>
<li><strong>第六部分</strong>对本文进行了总结，并对未来研究方向提出了建议。</li>
</ul>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2.相关工作"></a>2.相关工作</h2><h3 id="A-基于深度学习的声源定位"><a href="#A-基于深度学习的声源定位" class="headerlink" title="A. 基于深度学习的声源定位"></a>A. 基于深度学习的声源定位</h3><p>​	近年来，使用神经网络进行声源定位（SSL）领域取得了显著的研究进展[8]–[20]。表I提供了部分代表性声源定位方法的时间顺序概览。尽管这些方法通常能够实现良好的SSL性能，但它们通常只能处理某些有限的任务，比如源的数量、逐帧或逐块SSL、固定阵列或可变阵列。多源定位有时需要专门设计的输出格式，逐帧方法为每个时间帧输出SSL结果，适用于在线和移动源的定位，而可变阵列模型可以应用于未见过的麦克风阵列。</p>
<p>​	各种网络架构被用于SSL，其中卷积神经网络（CNN）[9]、[13]、[14]、[19]和卷积递归神经网络（CRNN）[11]、[16]、[18]是最常用的网络。这些网络都是为同时处理所有频率而设计的。网络输入可以是信号级别的，如时域信号[33]、STFT系数[14]、[20]或STFT系数的幅度和相位[9]、[11]、[16]，也可以是特征级别的，如IPD、IID、广义互相关（GCC）函数[34]–[36]和噪声空间谱[12]、[13]、[19]。</p>
<p>​	根据学习目标，SSL方法可分为特征&#x2F;位置回归方法或位置分类方法。特征&#x2F;位置回归方法估计定位特征（如DP-RTF、DP-IPD和通道间时间差（ITD））[11]、[16]、[20]、[36]–[39]或直接估计源位置[9]、[12]–[14]、[19]，这些是从噪声信号或噪声定位特征中估计出来的。大多数研究输出一个源的特征&#x2F;位置，较少有研究探讨如何将特征&#x2F;位置回归扩展到多个源。位置分类方法[10]、[15]、[17]、[18]、[40]–[42]将候选位置作为类别，且多源定位可以简单地作为多类分类任务来处理[10]、[40]–[42]。由于相邻位置之间的非正交关系，分类输出通常会在主峰周围出现模糊响应，从而降低定位性能。根据表I和上述概述，显然所提出的方法在网络架构和学习目标方面与现有方法完全不同。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321061008135.png" alt="image-20250321061008135"></p>
<h3 id="B-多种阵列的SSL"><a href="#B-多种阵列的SSL" class="headerlink" title="B. 多种阵列的SSL"></a>B. 多种阵列的SSL</h3><p>​	从麦克风信号和&#x2F;或定位特征映射到源位置，本质上是一个阵列依赖的问题，这要求了解麦克风阵列的几何结构或使用固定的麦克风阵列。在[17]中，通过将麦克风阵列几何信息与定位特征一起作为输入提供给网络，网络可以对可变阵列执行SSL。然而，由于固定的输入大小的限制，一个网络只能处理具有相同数量麦克风的可变阵列。在我们之前的工作[11]、[16]中，干净的定位特征，即DP-RTF或DP-IPD，被作为网络输出，且处理的是2通道阵列，在这种情况下，一个网络可以直接通过可变阵列进行训练，并在未见过的阵列上进行测试。在本研究中，所提出的可变阵列模型能够处理任何麦克风阵列，无论麦克风数量如何。</p>
<h2 id="3-问题制定"><a href="#3-问题制定" class="headerlink" title="3. 问题制定"></a>3. 问题制定</h2><p>​	假设在封闭的环境中有多种声音来源，并带有噪音和混响。麦克风阵列记录的多通道信号表示为</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321061505675.png" alt="image-20250321061505675" style="zoom:50%;" />

<p>其中，$m \in [1, M]$、$k \in [1, K]$ 和 $t \in [1, T]$ 分别表示麦克风、声源和时间样本的索引。对于第 $k$ 个声源，$s_k(t)$、$\theta_k$ 和 $a_m(t, \theta_k)$ 分别表示源信号、到达方向（DOA）和第 $m$ 个麦克风的直达响应（在房间脉冲响应，RIR 内），并且 $*$ 表示卷积操作。噪声信号 $v_m(t)$ 包括了环境噪声以及声源的反射&#x2F;混响。	应用短时傅里叶变换（STFT），多通道信号可以表示为</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321061638498.png" alt="image-20250321061638498" style="zoom:50%;" />

<p>其中，$n \in [1, N]$ 和 $f \in [1, F]$ 分别表示时间帧索引和频率索引。在这里，$X_m(n, f)$、$S_k(n, f)$ 和 $V_m(n, f)$ 分别是麦克风、声源和噪声信号的短时傅里叶变换（STFT）系数。$A_m(f, \theta_k)$ 是直达响应的传输函数（傅里叶变换）。两个麦克风的直达相对传输函数（DP-RTF）分别通过其相位和幅度编码了直达的相位差（IPD）和强度差（ILD），因此它是一个可靠的定位特征。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321061750956.png" alt="image-20250321061750956" style="zoom:50%;" />

<p>其中，$r$ 是所选参考通道的索引。为了简化，本研究中仅使用并学习直达相位差（DP-IPD），即相位部分 $\angle B_m(f, \theta_k)$，用于声源定位（SSL）。</p>
<p>​	在自由场和远场情况下，对于给定的麦克风对和源的到达方向（DOA）$\theta$（相对于麦克风对），复值的直达相位差（DP-IPD）可以理论上计算为：</p>
<img src="/../../../Library/Application%20Support/typora-user-images/image-20250321061925692.png" alt="image-20250321061925692" style="zoom:50%;" />

<p>其中，$v_f$ 是频率（单位：赫兹），$d$ 是麦克风间距，$c$ 是空气中的声速，$\frac{d \cos(\theta)}{c}$ 是从方向 $\theta$ 到两个麦克风的到达时间差（TDOA）。</p>
<p>​	在本研究中，声源定位的任务是使用神经网络从多通道麦克风信号中估计DP-IPD。对于 $M$ 个麦克风，其中一个麦克风被选为参考通道，网络预测 $M - 1$ 对麦克风（其他通道相对于参考通道）的DP-IPD。然后，通过将预测的DP-IPD与DP-IPD模板（即一组预定义候选方向的理论DP-IPD）进行匹配，可以获得DOA估计，如图1所示。具体而言，计算预测的DP-IPD向量与理论DP-IPD向量之间的内积，并将具有最大内积值的候选方向作为DOA估计。DP-IPD的训练目标和DP-IPD模板都通过公式（4）计算。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321062241001.png" alt="image-20250321062241001" style="zoom:50%;" />

<p>​	此外，所提出的DP-IPD估计网络被设计为能够处理以下方面的更实际和复杂的应用：</p>
<ul>
<li><strong>灵活的声源数量</strong>。我们考虑声源的数量是灵活且随时间变化的。然而，我们依赖一个强有力且合理的假设，即在任何时候最多有 $K$ 个（例如，本研究中为2个）声源存在，在此基础上，预测固定数量的 $K$ 条DP-IPD轨迹。DP-IPD的估计值可以在不同时间属于不同的声源。对于非声源，设置一个无效的DP-IPD值。总体而言，网络可以在一个时间内检测并定位0到 $K$ 个声源。</li>
<li><strong>移动声源</strong>。对于移动源，DOA（即 $\theta_k$）以及其传输函数 $A_m(f, \theta_k)$ 都是时间依赖的&#x2F;变化的。为了解决这个问题，我们逐帧地预测DP-IPD并定位声源，无论是在线（因果）方式还是离线（非因果）方式。</li>
<li><strong>可变麦克风阵列</strong>。网络可以设计为适用于具有不同拓扑结构和通道数的可变阵列。一个网络通过许多不同的阵列进行训练，并预测变化的 $M - 1$ 个麦克风对的DP-IPD。然后，网络可以用于具有任意测试阵列的DP-IPD估计，对于该阵列，DP-IPD模板可以在测试期间理论上计算得到。通过这种方式，将DP-IPD估计步骤和定位步骤（即模板匹配）分开，从而形成一个通用的声源定位网络。这是合理的，因为DP-IPD估计步骤可以是一个阵列无关的信号级任务，而定位步骤依赖于阵列，但理论上简单。DP-IPD估计的准确性对定位至关重要，因为完美的DP-IPD估计将导致几乎完美的定位。</li>
</ul>
<p>麦克风和声源的数量，即 $M$ 和 $K$，以及DOA $\theta_k$ 都可以在不同设置或随时间变化，但为了符号简洁，除非另有说明，我们在以下内容中不会具体说明这些变化。</p>
<h3 id="4-方法"><a href="#4-方法" class="headerlink" title="4. 方法"></a>4. 方法</h3><p>​	本节介绍了所提出的IPDNet。为固定麦克风阵列和可变阵列分别提出了两个版本，其网络架构分别如图3（a）和（b）所示。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321062729765.png" alt="image-20250321062729765"></p>
<h4 id="A-学习目标"><a href="#A-学习目标" class="headerlink" title="A. 学习目标"></a>A. 学习目标</h4><p>​	所提出的网络以多通道录音的STFT（短时傅里叶变换）为输入，并输出&#x2F;预测DP-IPD特征。为了使实值网络的优化成为可能，沿用了我们之前工作的设置[11]、[16]、[20]，学习目标被设置为复值DP-IPD的实部（R）和虚部（I）（沿频率进行拼接，对于一个麦克风对）。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321062843454.png" alt="image-20250321062843454" style="zoom:50%;" />

<p>其中⊤表示矢量转置。将输出激活层设置为$tanh$，以预测DP-IPD。</p>
<p>​	$q(\theta)$ 定义了一个声源在到达方向（DOA）$\theta$ 处的DP-IPD目标向量，跨所有DOA，它形成了一个DP-IPD流形。然而，对于非声源帧，定义目标并不简单。在我们之前的工作中，使用全零向量作为非声源帧的目标，但这似乎与DP-IPD向量没有显著的相关性。网络需要学习由DP-IPD流形和全零向量扩展的输出空间，并在流形（用于源帧）和全零向量（用于非源帧）之间迅速切换，这可能并不容易。在本研究中，为了促进网络学习，我们提出将非声源帧的目标定义为（复值）DP-IPD流形的均值点，可以推导为：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321063100127.png" alt="image-20250321063100127" style="zoom:50%;" />

<p>其中，$J_0(\cdot)$ 表示零阶贝塞尔第一类函数。DP-IPD在一个麦克风对上对所有可能的 $\theta \in [0, 2\pi)$ 进行积分&#x2F;平均，这类似于计算圆柱对称漫射声场的空间相干性[43]。非声源目标值是一个频率和麦克风距离的实数函数。对于给定的麦克风距离，使用公式（6）计算F个离散频率的非声源目标值，然后将它们拼接起来形成目标向量。图2显示了对于两种不同麦克风距离，非声源目标值随频率变化的情况。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321063151246.png" alt="image-20250321063151246" style="zoom:50%;" />

<p>​	为了确定一个源在一个帧中是否处于活动状态，即进行框架的每一源活性检测，我们计算参考通道处的以下直接路径与嘈杂的幅度比：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321063218420.png" alt="image-20250321063218420" style="zoom:50%;" />

<p>其中，$| \cdot |$ 表示绝对值。如果 $v_k(n)$ 大于预设的阈值，则认为第 $k$ 个声源在第 $n$ 帧中是激活的，否则是非激活的。对于激活的声源，我们使用DP-IPD目标向量；否则，我们使用非声源目标向量。</p>
<h4 id="B-Full-Narrow-Network-Block"><a href="#B-Full-Narrow-Network-Block" class="headerlink" title="B.Full-Narrow Network Block"></a>B.Full-Narrow Network Block</h4><p>所提出的（两个版本的）网络由一系列全带宽（full-band）和窄带（narrow-band）网络模块级联而成。一个全带宽层和一个窄带层组成一个全窄带模块。</p>
<ol>
<li><strong>全带宽BLSTM层</strong>：全带宽BLSTM层独立地处理时间帧，所有时间帧共享相同的网络权重。输入是一个单独时间帧沿频率轴的序列：</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321063646950.png" alt="image-20250321063646950" style="zoom:50%;" />

<p>其中，上标“full”表示全带宽层，$h(n, k) \in \mathbb{R}^D$ 表示单个时频（TF）单元的隐藏向量，$D$ 是隐藏维度。注意，对于第一个全带宽层，$h(n, k)$ 是麦克风信号，如公式（10）所示，而在其他层中，$h(n, k)$ 是前一层的输出。全带宽层的重点是学习空间&#x2F;定位线索的频率间依赖性。不同频率的DP-IPD具有强相关性，因为它们都是从相同的TDOA推导而来。此外，低直达能量的频率的空间&#x2F;定位线索可以通过其他频率的帮助进行良好的预测。全带宽层不学习任何时间信息，这部分留给后续的窄带层。</p>
<ol start="2">
<li><strong>窄带（B）LSTM层</strong>：窄带（B）LSTM层独立地处理频率，所有频率共享相同的网络权重。输入是一个单独频率沿时间轴的序列：</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321063833336.png" alt="image-20250321063833336" style="zoom:50%;" />

<p>其中，上标“narrow”表示窄带层。输入向量 $h(n, f)$ 是前一全带宽层的输出向量。窄带中估计直达定位特征已在许多传统方法中研究，例如在[25]中通过通道识别、在[26]中通过相干性测试、在[27]中通过直达主导性测试。所提出的窄带层重点利用这些窄带的通道间信息。此外，对于移动声源，DP-IPD是时间变化的，窄带层还学习DP-IPD的时间演化。</p>
<p> 3.<strong>跳跃连接（Skip Connections）</strong>：全带宽和窄带层被设计为强调各自特定的信息领域。然而，在通过全带宽层处理后，可能会丢失窄带信息，反之亦然。为了解决这个问题，引入了跳跃连接以防止这种信息丢失。如图3所示，这意味着将每个全带宽层和窄带层的输入序列与原始输入信号的STFT系数进行拼接（经过适当的维度变换）。</p>
<h4 id="C-Fixed-Array-Model"><a href="#C-Fixed-Array-Model" class="headerlink" title="C. Fixed Array Model"></a>C. Fixed Array Model</h4><p>​	对于训练和测试时使用固定麦克风阵列的场景，模型架构如图3（a）所示。为了不失一般性，参考通道设置为 $r &#x3D; 1$。该模型以 $M$通道麦克风信号为输入，并输出 $M - 1$ 个DP-IPD向量。具体来说，输入通过将多通道麦克风信号的实部和虚部拼接形成：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321064345245.png" alt="image-20250321064345245" style="zoom:50%;" />

<p>其中，$[:]$ 是取张量一个维度所有值的操作。输入首先由全窄带模块处理，以提取多个源的空间特征嵌入，可以表示为：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321064355738.png" alt="image-20250321064355738" style="zoom:50%;" />

<p>​	然后，使用卷积模块来分离并提取多个声源的DP-IPD向量。卷积模块的结构也如图3（a）所示，包含三层二维因果卷积层和两层时间平均池化层。卷积层用于捕捉局部特征，以分离麦克风对和声源。平均池化层用于压缩帧率。前两层卷积的激活函数使用ReLU，最后一层卷积的激活函数设置为tanh，以进行DP-IPD估计。最终输出表示为：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321064404993.png" alt="image-20250321064404993" style="zoom:50%;" />

<p>其中 $O &#x3D; 2$（实部和虚部）$\times (M - 1)$（麦克风对）$\times K$（声源数）。</p>
<p>​	网络输出定义为固定数量的 $K$ 条轨迹，每条轨迹代表一个声源（可以是非声源），并包含 $M - 1$ 个估计的DP-IPD向量。对于一个时间帧，例如 $n$，输出可以写作：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321064413008.png" alt="image-20250321064413008" style="zoom:50%;" />

<p>其中，$\hat{q}_{1,m}(\theta_k)$ 表示在时间帧 $n$ 时，第1个和第$m$个麦克风对以及第$k$个声源的估计DP-IPD。</p>
<p>关于源排列问题，采用帧级排列不变训练（frame-level PIT）。麦克风对的顺序根据麦克风索引固定。</p>
<h4 id="D-Variable-Array-Model"><a href="#D-Variable-Array-Model" class="headerlink" title="D. Variable Array Model"></a>D. Variable Array Model</h4><p>​	所提出的IPDNet是一个端到端的DP-IPD估计网络，旨在通过网络充分学习多通道麦克风信号中的有用信息，从而去除噪声、混响和重叠语音等干扰。当麦克风阵列固定时，网络可以很容易地设计，即输入数据具有固定大小。然而，在使用一个网络处理不同阵列并且输入大小可变时，实现端到端的DP-IPD估计网络并非易事。为此，本工作提出了一种成对处理方案，即麦克风对（将参考通道与其他通道配对）通过共享权重的网络单独处理，这使得处理任意数量的麦克风（对）成为可能。所有麦克风对的中间隐藏单元进行均值池化，并广播到每个麦克风对，这使得麦克风对之间能够相互通信。尽管这种麦克风对通信方案不如固定阵列模型中直接学习多个麦克风之间的依赖关系高效，但与没有麦克风对之间通信的情况相比，它大大提高了DP-IPD估计的准确性。</p>
<p>​	可变阵列模型的网络架构如图3（b）所示。具体来说，参考通道设置为 $r &#x3D; 1$，其他麦克风与参考通道单独配对，形成网络输入：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321064935424.png" alt="image-20250321064935424" style="zoom:50%;" />

<p>​	然后通过共享权重的全窄带模块处理：	</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321064944285.png" alt="image-20250321064944285" style="zoom:50%;" />

<p>设 $h_{1,m}^{fn} \in \mathbb{R}^{N \times F \times D}$ 表示1，m麦克风对的全带宽BLSTM或窄带LSTM的隐藏单元。每个全带宽BLSTM和窄带（B）LSTM后执行一个平均操作：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321064951763.png" alt="image-20250321064951763" style="zoom:50%;" />

<p>其中包含从所有麦克风对提取的中间信息。作为下一层的输入，$c^{f&#x2F;n}$ 首先被拼接到每个 $h_{1,m}^{fn}$ 上，然后通过线性层从2D维度转换回D维度。麦克风对之间的这种交互提供了整个阵列的依赖关系，从而帮助每个麦克风对更好地学习。</p>
<p>​	最后，使用与固定阵列模型相同的卷积模块，从全窄带模块的输出中分离出多个源的DP-IPD：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321064959854.png" alt="image-20250321064959854" style="zoom:50%;" />

<p>现在 $O’ &#x3D; 2$（实部和虚部）$\times K$（源数）。</p>
<p>​	与固定阵列模型类似，对于每个 $M - 1$ 个麦克风对，网络输出定义为固定数量的 $K$ 条轨迹，每条轨迹表示一个声源（可以是非声源）。对于一个时间帧 $n$，输出可以写作：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321065008955.png" alt="image-20250321065008955" style="zoom:50%;" />

<p>训练时同样使用帧级排列不变训练（frame-level PIT）。麦克风对的顺序现在可以任意设置。</p>
<h4 id="E-Frame-level-PIT-and-Sound-Source-Localization"><a href="#E-Frame-level-PIT-and-Sound-Source-Localization" class="headerlink" title="E. Frame-level PIT and Sound Source Localization"></a>E. Frame-level PIT and Sound Source Localization</h4><p>训练阶段的源排列问题通过帧级排列不变训练（frame-level permutation invariant training, PIT）得到解决。设 $\alpha \in P$ 表示所有可能的源顺序之一。对于一个时间帧 $n$，帧级PIT损失可以计算为：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321065523472.png" alt="image-20250321065523472" style="zoom:50%;" />

<p>注意，所有 $M - 1$ 个麦克风对共享相同的源顺序。帧级PIT损失对所有帧进行平均，作为整体训练损失。均方误差（MSE）被用作损失函数。</p>
<p>​	在推理阶段，每个DP-IPD估计轨迹与一个源（或非源）相关联。通过计算估计的DP-IPD与DP-IPD模板的内积，可以推导出一个源的空间谱：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321070429766.png" alt="image-20250321070429766" style="zoom:50%;" />

<p>其中，$\theta_i$ 表示第 $i$-个（$i \in [1, I]$）候选位置。空间谱 $s_k(i)$ 对每个源是独立的，作为候选位置的函数。如果 $s_k(i)$ 的最大值超过预定义的阈值，则认为源存在于相应的候选位置，否则认为它是非源。</p>
<h4 id="F-Configurations"><a href="#F-Configurations" class="headerlink" title="F. Configurations"></a>F. Configurations</h4><p>所提出的网络可以轻松实现离线或在线声源定位（SSL），方法是将窄带LSTM设置为双向或单向，以及将卷积层设置为非因果或因果层。为了使模型更容易优化，对网络输入进行拉普拉斯归一化，形式为 $X_m(n, f) &#x2F; \mu(n)$，其中 $\mu(n)$ 是归一化因子。</p>
<p>对于离线SSL，$\mu(n)$ 的计算方式为：</p>
<p>​							$\mu(n) &#x3D; \frac{1}{NF} \sum_{n&#x3D;1}^{N} \sum_{f&#x3D;1}^{F} |X_m(n, f)|$</p>
<p>对于在线SSL，$\mu(n)$ 通过递归计算来确保方法的因果性，公式为：</p>
<p>​						$\mu(n) &#x3D; \beta \mu(n - 1) + (1 - \beta) \frac{1}{F} \sum_{f&#x3D;1}^{F} |X_m(n, f)|$</p>
<p>其中，$\beta &#x3D; \frac{L - 1}{L + 1}$ 表示历史时间帧的平滑权重，$L$ 表示平滑窗口的长度。</p>
<h3 id="5-Experiments-And-Discussions"><a href="#5-Experiments-And-Discussions" class="headerlink" title="5. Experiments And Discussions"></a>5. Experiments And Discussions</h3><h4 id="A-数据集"><a href="#A-数据集" class="headerlink" title="A. 数据集"></a>A. 数据集</h4><p>我们在模拟数据集上训练模型，并在模拟和真实数据上进行评估。</p>
<h5 id="模拟数据集："><a href="#模拟数据集：" class="headerlink" title="模拟数据集："></a>模拟数据集：</h5><p>多通道麦克风信号通过将房间脉冲响应（RIR）与语音源信号进行卷积来模拟。RIR（移动声源）使用gpuRIR工具箱[44]生成。清洁的语音信号随机选择自Librivox语音库[45]的训练集、开发集和测试集。将单一源的麦克风信号进行叠加，生成多源麦克风信号。声源数量设定为2。房间混响时间（RT60）随机设置在[0.2, 1.3]秒范围内。房间尺寸随机设置在6×6×2.5米到10×8×6米之间。根据[43]的方法生成的漫射噪声信号被加入到语音信号中，信噪比（SNR）随机选取范围为-5 dB到15 dB。</p>
<p>对于可变阵列模型，用于训练的麦克风阵列是随机生成的。我们预定义了6种类别的麦克风阵列，分别是均匀&#x2F;非均匀线性麦克风阵列、圆形麦克风阵列、带中心麦克风的圆形麦克风阵列、2D&#x2F;3D自适应麦克风阵列。每种麦克风阵列的出现比例相同。麦克风的数量随机设定在[2, 8]范围内。任意两个麦克风的距离限制在[3, 25]厘米之间。为了测试，设置了五种常用的麦克风阵列拓扑，包括2通道、4通道和6通道线性阵列，以及4通道和6通道圆形阵列，如图4所示。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321071037110.png" alt="image-20250321071037110" style="zoom:50%;" />

<p>对于固定阵列模型，每个测试阵列都训练一个网络。无论是固定阵列还是可变阵列模型，训练、验证和测试的发音数量分别为300,000、4,000和4,000。每个发音包括两个源，每个源的音频片段长度在[5, 25]秒范围内，两个音频片段根据重叠率在[0, 1]范围内重叠。说话者的移动速度在[0, 1]米&#x2F;秒范围内。</p>
<h5 id="真实数据集："><a href="#真实数据集：" class="headerlink" title="真实数据集："></a>真实数据集：</h5><p>LOCATA数据集[46]提供了在柏林洪堡大学计算机科学系录制的音频信号。房间大小为7.1×9.8×3米，混响时间为0.55秒。该数据集包括用于定位单一&#x2F;多个静态声源（任务1和2）以及单一&#x2F;多个移动声源（任务3、4、5和6）的任务，使用四种不同的麦克风阵列。我们评估了所有6个任务，使用常用的benchmark2（12麦克风伪球形阵列）和DICIT（15麦克风平面阵列）[47]阵列。请注意，我们只考虑LOCATA数据集中不超过2个说话人的发音。</p>
<h4 id="B-Configurations"><a href="#B-Configurations" class="headerlink" title="B. Configurations"></a>B. Configurations</h4><p>​	麦克风信号的采样率为16 kHz。STFT的窗口长度为512个样本（32毫秒），帧移为256个样本（16毫秒）。训练音频片段的长度为4.5秒。模型每12帧（192毫秒）输出一次定位结果。最大源数设定为 $K &#x3D; 2$。源活动检测的阈值设定为0.001。估计空间谱的阈值用于判断源的存在，设定为0.5。级联的全窄带模块的数量设定为2。网络的隐藏维度已经过充分搜索。</p>
<p>​	对于可变阵列模型，每个（B）LSTM层的隐藏维度均设置为128。对于2通道和4&#x2F;6通道固定阵列模型，每个（B）LSTM层的隐藏维度分别设置为128和256，其中通道数较高的模型承载了更多的信息，因此需要更大的网络。优化器使用Adam [48]。固定阵列模型和可变阵列模型的批处理大小分别设为16和4。学习率初始设置为0.0005，并以衰减因子0.975进行指数衰减。我们为固定阵列模型训练了大约30个epoch，为可变阵列模型训练了15个epoch。</p>
<h4 id="C-评估指标"><a href="#C-评估指标" class="headerlink" title="C. 评估指标"></a>C. 评估指标</h4><p>性能仅在声音激活期间进行评估。候选方位角和仰角的分辨率均为1°。角度估计误差计算为估计角度与真实角度的差值。容忍度：n表示如果方位角估计误差不大于n°，则认为源被成功定位。评估指标包括漏检率（MDR）、误报率（FAR）和平均绝对误差（MAE）。MDR和FAR分别表示源处于激活状态但未成功定位的帧比例，以及检测到的源但实际上没有源的帧比例。MAE表示所有成功定位的源和时间帧的绝对角度估计误差。</p>
<h4 id="D-消融实验"><a href="#D-消融实验" class="headerlink" title="D. 消融实验"></a>D. 消融实验</h4><p>为了分析所提出网络的各个组件的贡献，进行了固定阵列模型的消融实验，针对模拟的2通道阵列进行180°方位角定位。表II展示了通过移除一个子网络所导致的定位性能、模型大小以及浮动点操作（FLOPs）2，对于所提出的网络及其变体的影响。结果表明，当移除任何一个子网络时，SSL性能明显下降，这表明这些子网络对整体性能的重要贡献。特别是，全带宽BLSTM模块似乎发挥了极其重要的作用。全带宽BLSTM学习了定位信息的跨频率依赖关系。如公式（4）所示，DP-IPD基本上是频率的线性函数，利用DP-IPD的强跨频率关系对于提高估计精度非常有帮助。图5显示了DP-IPD估计的两个例子。可以看到，当SNR为0 dB时，DP-IPD估计在那些可能具有非常低SNR的频率上存在高度偏差，而全带宽BLSTM帮助根据DP-IPD的跨频率关系来修正这些偏差。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321071657356.png" alt="image-20250321071657356"></p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321071748240.png" alt="image-20250321071748240" style="zoom:50%;" />

<h4 id="E-与不同学习目标的比较"><a href="#E-与不同学习目标的比较" class="headerlink" title="E. 与不同学习目标的比较"></a>E. 与不同学习目标的比较</h4><p>基于深度学习的声源定位方法使用了各种不同的学习目标，其中位置分类和回归是两种常用的目标。我们将所提出的DP-IPD目标与其他目标进行比较。比较实验在模拟的2通道阵列上进行，进行180°方位角定位。与所提出的2通道固定阵列模型相同的主干网络被使用，在其上根据需要添加额外的头部。比较的目标如下，所有目标都经过充分调整以在当前任务上达到最佳性能。</p>
<ol>
<li><strong>多类位置分类</strong>[10]：每个候选位置被视为一个类。多类分类可以同时定位多个源。在推理时，输出概率大于阈值的类别被检测为激活源。</li>
<li><strong>多轨道位置回归</strong>[28]：预测多个源的位置。使用单位圆上的2D坐标表示方位角，且[0,0]表示非源。训练时使用帧级PIT。</li>
<li><strong>空间谱回归</strong>[9]：预测多源空间谱，其中每个频谱峰值代表一个源。在推理时，大于阈值的频谱峰被检测为激活源。</li>
<li><strong>多轨道空间谱回归</strong>[15]：使用独立的空间谱表示每个源。两个空间谱轨道按方位角的降序排列。</li>
<li><strong>多轨道位置分类</strong>：受多轨道空间谱回归方法启发，我们还测试了一种新的学习目标，即多轨道位置分类，输出两个分类轨道，每个轨道表示一个源。这两个轨道也按方位角的降序排列。</li>
<li><strong>混合IPD回归</strong>[16]：预测与所提出方法相同的DP-IPD，但多个源的DP-IPD通过一个混合权重（基于每个源的功率比例）混合在一起。这样可以避免源排列问题。在推理时，应用迭代的源检测和定位技术来迭代提取每个源的DP-IPD。</li>
<li><strong>多轨道DP-IPD回归（0向量）</strong>：即所提出的DP-IPD目标，但使用全零向量表示非源。</li>
<li><strong>所提出的多轨道DP-IPD回归</strong>：对于所有基于回归的方法，损失函数使用MSE。除多轨道位置回归外，所有这些目标都需要设置候选位置，并且它们都使用每1°作为一个候选位置。</li>
</ol>
<p>图6展示了在两个源角度差异的函数下，结果（MDR和FAR平方和的平方根，表示为MDR&amp;FAR）。可以观察到：</p>
<ul>
<li><p><strong>多轨道方法在两个源角度差较小的情况下优于单轨方法</strong>，例如多轨道与多源空间谱回归、多轨道与多类位置分类、多轨道与混合DP-IPD回归。当两个源靠近时，多源空间谱中的多个峰值往往会合并为一个峰值，而通过设置两个独立的轨道可以很好地将其分开。值得注意的是，单轨方法可以处理灵活数量的源，而多轨道方法只能输出固定的最大源数量。</p>
</li>
<li><p><strong>与基于分类的方法相比，基于回归的方法在容差增大时能获得更大的精度提升</strong>。例如，比较多轨道空间谱回归与多轨道位置分类，当误差容忍度为5°时，后者表现更好；而当误差容忍度为10°时，前者表现更好。由于源位置在空间中是连续的，当网络未能预测准确的定位结果时，使用MSE回归损失可以确保误差结果不会偏离真实值。而交叉熵损失则缺乏这种约束，可能导致较大的定位误差。</p>
</li>
<li><p><strong>所提出的多轨道DP-IPD回归方法始终优于多轨道位置和空间谱回归方法</strong>，后两者表现相似。DP-IPD估计是一个信号级任务，可以从麦克风信号中直接学习。相比之下，位置和空间谱估计需要进一步的阵列依赖转换，直接从麦克风信号中学习时可能会更加困难。</p>
</li>
<li><p><strong>与使用全零向量表示非源相比，所提出的非源目标获得了更好的性能</strong>，这表明网络确实更容易学习所提出的非源目标。为了进一步验证这一点，在图7中，我们展示了从一个非源帧切换到一个源帧的示例。可以看到，所提出的非源目标能够更快地实现从非源帧到源帧的过渡。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321071816416.png" alt="image-20250321071816416" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321071945470.png" alt="image-20250321071945470" style="zoom:50%;" /></li>
</ul>
<h4 id="F-Results-on-Simulation"><a href="#F-Results-on-Simulation" class="headerlink" title="F. Results on Simulation"></a>F. Results on Simulation</h4><p>我们在模拟数据集上进行了在线和离线的多源定位实验。与所提出方法相比，以下先进的SSL方法被纳入比较：</p>
<ol>
<li><strong>SRP-DNN</strong> [16]：一种在线多源定位方法。它使用因果CRNN网络来估计多个声源的混合DP-IPD，然后采用迭代的源检测和定位方法来获得每个声源的DP-IPD估计³。</li>
<li><strong>SALADNet</strong> [10]：使用级联的卷积模块和自注意力模块实现多源定位。SALADNet原本以一阶Ambisonics的强度向量作为输入特征，但在本实验中，我们将其输入改为直接使用麦克风信号。</li>
<li><strong>SALSA-Lite</strong> [24]：针对联合声事件定位与检测而设计。其网络输入为经过频率归一化的IPD与幅度谱的拼接，并采用ResNet-GRU网络。我们只使用其定位分支，并将目标从与声类对齐的DOA轨迹改为简单的多源DOA。</li>
<li><strong>SE-Resnet</strong> [49]：在DCASE22中联合声事件定位与检测的排名靠前的方法。它使用Squeeze-and-Excitation残差网络（作为编码器）和门控递归单元网络（作为解码器）。与SALSA-Lite的处理方式相同，我们将其目标更改为多源DOA。</li>
</ol>
<p>SALADNet、SALSA-Lite和SE-Resnet均执行离线定位。表III展示了在五个模拟测试阵列上的定位性能，误差容忍度设为10°。对于固定阵列模型，每个测试阵列都单独训练一个网络，而所提出的可变阵列模型只训练一次，然后用于所有测试阵列。</p>
<p>在固定阵列实验中，可以看出所提出的方法在所有条件下均明显优于所有对比方法。其优势主要包括：<br> i) 所提出的方法（以及SRP-DNN和SALADNet）直接以麦克风信号作为输入，而SALSA-Lite和SE-Resnet则以噪声干扰的IPD（拼接幅度谱）作为输入。直接处理麦克风信号更有效地抑制了噪声和混响的干扰，因为原始信号中呈现的噪声和混响的自然特性（如其空间扩散性）能够更好地被利用；<br> ii) 正如上一节讨论的，所提出的学习目标——DP-IPD——比其他学习目标更为有效；<br> iii) 所提出的全带宽和窄带融合网络能够高效地利用窄带空间信息的时间演化和定位线索的跨频率相关性。</p>
<p>对于可变阵列模型，虽然缺乏与之直接对应的对比方法，但将所提出的可变阵列模型与固定阵列模型进行比较仍能评估其性能。对于2通道阵列，固定阵列模型和可变阵列模型均直接在一个网络中学习一个麦克风对的DP-IPD。固定阵列模型表现更好，因为它只处理固定的4厘米麦克风距离；而可变阵列模型则需要处理较大范围的麦克风距离，即[3, 25]厘米，这要求覆盖更大、更复杂的学习空间。对于4通道&#x2F;6通道阵列，可变阵列模型通过隐藏单元均值池化实现通道间通信，这种方式相比于固定阵列模型中直接的通道间通信可能略显次优。不过，令人欣慰的是，可变阵列模型相对于固定阵列模型仅有合理的性能下降，这表明均值池化方案在某种程度上是有效的，这一点也可以从当去除均值池化方案（“w&#x2F;o mean”）时所导致的性能大幅下降（见表III）得到进一步证明。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321071846516.png" alt="image-20250321071846516" style="zoom:50%;" />

<p>此外，所提出的可变阵列模型的性能优于所有其他固定阵列的对比方法，这展示了其更广泛的适用性以及经济的训练要求。尤其是，当未来考虑使用真实录音数据训练SSL模型时，就无需为每个新的麦克风阵列收集新的数据。</p>
<p>与其他方法相比，所提出的模型具有较小的模型体积，但计算复杂度较高。所提出的全带宽&#x2F;窄带网络对帧&#x2F;频率进行独立处理，因此网络尺寸较小（因为单个帧&#x2F;频率内信息较少），但由于需要多次运行，整体计算复杂度较大。较高的计算复杂度可能会限制其在某些实时应用中的使用，这一问题将在我们未来的工作中得到解决。</p>
<h4 id="G-Results-on-the-LOCATA-dataset"><a href="#G-Results-on-the-LOCATA-dataset" class="headerlink" title="G. Results on the LOCATA dataset"></a>G. Results on the LOCATA dataset</h4><h5 id="1-比较实验："><a href="#1-比较实验：" class="headerlink" title="1) 比较实验："></a>1) 比较实验：</h5><p>我们首先在所有六个任务上评估所提出的方法和对比方法的方位角定位性能。根据LOCATA的设置，我们仅执行在线（因果）SSL。使用了两个子阵列：<br> i) <strong>Benchmark2阵列中的麦克风5、8、11和12</strong>，它形成了一个几乎矩形的阵列，位于机器人头部顶部；<br> ii) <strong>DICIT阵列中的麦克风6、7和9</strong>，它形成一个3通道线性阵列，麦克风间距为4厘米。使用模拟数据重新训练了这两个阵列的固定阵列模型。与前一节使用的相同的可变阵列模型直接用于本实验。</p>
<p>对于在线SSL，除了SRP-DNN [16]，我们还比较了两个额外的方法：</p>
<ol>
<li><strong>Cross3D</strong> [12]：该方法以SRP-PHAT空间谱为输入，使用因果3D CNN网络进行移动源定位。</li>
<li><strong>IcoDOA</strong> [13]：该方法使用20面体CNN从SRP-PHAT空间谱中提取定位特征，并使用因果CNN结合时间上下文进行移动源定位。</li>
</ol>
<p>与所提出的模型自动检测活动扬声器的数量不同，Cross3D和IcoDOA只定位一个固定的扬声器，因此它们仅在单一源任务（即任务1、3和5）上进行比较，并使用定位误差率作为评估指标。定位误差率被转换为相等的MDR和FAR。</p>
<p>表IV展示了定位性能。在所有方法中，在DICIT数据上观察到大约4°的偏差，可能来源于注释偏差。为减轻这一影响，我们通过减去这个偏差来调整所有DOA估计。可以看出，所提出的固定阵列模型在所有条件下仍然优于其他方法，而所提出的可变阵列模型与固定阵列模型的性能相当。这验证了所提出的使用模拟数据训练的模型能够很好地推广到真实数据。此外，所提出的可变阵列模型能够很好地推广到未见过的真实麦克风阵列。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321072202304.png" alt="image-20250321072202304" style="zoom:50%;" />

<h5 id="2-跨通道数和仰角估计的泛化："><a href="#2-跨通道数和仰角估计的泛化：" class="headerlink" title="2) 跨通道数和仰角估计的泛化："></a>2) 跨通道数和仰角估计的泛化：</h5><p>我们为训练可变阵列模型使用的最大麦克风数量为8。在本实验中，我们在Benchmark2的8通道和12通道子阵列上测试可变阵列模型。8通道子阵列包括麦克风1、3、4、5、8、10、11和12，形成一个几乎立方形的阵列。12通道阵列包括Benchmark2的所有12个麦克风，形成一个几乎球形的阵列。此外，由于8通道和12通道阵列都是3D并提供垂直方向的区分能力，因此仰角也进行了定位。表V展示了定位性能，并给出了前一节中使用的4通道子阵列的结果以作比较。报告了所有六个任务的平均性能。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321072249271.png" alt="image-20250321072249271" style="zoom:50%;" />

<p>可以观察到，随着麦克风数量的增加，性能度量逐渐改善，特别是在误差容忍度为5°时，这表明更多麦克风可以获得更准确的DP-IPD估计。这验证了所提出的可变阵列模型能够很好地推广到比训练阵列更多通道的麦克风阵列。仰角也得到了很好的定位。这表明，通过分离特征估计步骤和定位步骤，所提出的方法能够灵活适应各种SSL配置。</p>
<p>图8展示了一个示例中两个移动源的方位角和仰角定位结果，使用的是8通道阵列。可以看到，方位角和仰角都得到了很好的定位，但仰角的定位误差较大。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250321072303357.png" alt="image-20250321072303357" style="zoom:50%;" />

<h2 id="6-结论"><a href="#6-结论" class="headerlink" title="6.结论"></a>6.结论</h2><p>​	本文提出了一个名为IPDNET的多轨DP-IPD学习网络，用于本地化多个移动声音源。提出的网络架构，即全频段和窄带融合网络，有效地了解噪声和混响的属性，从而提取可靠的声源DP-IPD。所提出的多轨DP-IPD回归目标井将特征提取步骤和源定位步骤删除，因此优于其他常用的SSL目标。此外，提出的变量阵列模型促进了SSL网络的培训。在这项工作中，根据模拟RIR和多通道噪声，对所提出的模型进行了纯模拟数据，这可能具有模拟到真实的问题。为了解决这个问题，在未来的工作中，可以使用跨数据库&#x2F;数组实录的数据对所提出的变量阵列模型进行培训，该数据提供了一种合理的方法来减轻注释难度和收集真实数据时的数据稀缺性。</p>
<h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a><strong>REFERENCES</strong></h2><p>[1] H.-Y. Lee, J.-W. Cho, M. Kim, and H.-M. Park, “DNN-based feature enhancement using doa-constrained ica for robust speech recognition,” <em>IEEE Signal Process. Lett.</em>, vol. 23, no. 8, pp. 1091–1095, 2016.</p>
<p>[2] S. E. Chazan, H. Hammer, G. Hazan, J. Goldberger, and S. Gannot, “Multi-microphone speaker separation based on deep DOA estimation,” in <em>Proc. Euro. Signal Process. Conf.</em>, 2019, pp. 1–5.</p>
<p>[3] M. Raspaud, H. Viste, and G. Evangelista, “Binaural source localization by joint estimation of ILD and ITD,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Lang. Process.</em>, vol. 18, no. 1, pp. 68–77, 2010.</p>
<p>[4] W. Zhang and B. D. Rao, “A two microphone-based approach for source localization of multiple speech sources,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Lang. Process.</em>, vol. 18, no. 8, pp. 1913–1928, 2010.</p>
<p>[5] S. Braun, W. Zhou, and E. Habets, “Narrowband direction-of-arrival estimation for binaural hearing aids using relative transfer functions,” in <em>Proc. IEEE Workshop Appl. Signal Process. Audio Acoust.</em>, 2015, pp. 1–5.</p>
<p>[6] Z. Wang, J. Li, Y. Yan, and E. Vincent, “Semi-supervised learning with deep neural networks for relative transfer function inverse regression,” in <em>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</em>, 2018, pp. 191–195.</p>
<p>[7] M. Jeub, M. Schaefer, T. Esch, and P. Vary, “Model-based dereverberation preserving binaural cues,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Lang. Process.</em>, vol. 18, pp. 1732–1745, 2010.</p>
<p>[8] P.-A. Grumiaux, S. Kitić, L. Girin, and A. Guérin, “A survey of sound source localization with deep learning methods,” <em>The Journal of the Acoustical Society of America</em>, vol. 152, no. 1, pp. 107–151, 2022.</p>
<p>[9] W. He, P. Motlíček, and J.-M. Odobez, “Neural network adaptation and data augmentation for multi-speaker direction-of-arrival estimation,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Lang. Process.</em>, vol. 29, pp. 1303–1317, 2021.</p>
<p>[10] P.-A. Grumiaux, S. Kitic, P. Srivastava, L. Girin, and A. Guérin, “SALADNet: Self-attentive multisource localization in the ambisonics domain,” <em>Proc. IEEE Workshop Appl. Signal Process. Audio Acoust.</em>, pp. 336–340, 2021.</p>
<p>[11] B. Yang, H. Liu, and X. Li, “Learning deep direct-path relative transfer function for binaural sound source localization,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Lang. Process.</em>, vol. 29, pp. 3491–3503, 2021.</p>
<p>[12] D. Diaz-Guerra, A. Miguel, and J. R. Beltrán, “Robust sound source tracking using SRP-PHAT and 3D convolutional neural networks,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Lang. Process.</em>, vol. 29, pp. 300–311, 2020.</p>
<p>[13] D. Diaz-Guerra, A. Miguel, and J. R. Beltran, “Direction of arrival estimation of sound sources using Icosahedral CNNs,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Lang. Process.</em>, vol. 31, pp. 313–321, 2022.</p>
<p>[14] Y. Fu, M. Ge, H. Yin, X. Qian, L. Wang, G. Zhang, and J. Dang, “Iterative sound source localization for unknown number of sources,” in <em>Proc. INTERSPEECH</em>, 2022.</p>
<p>[15] H. Yin, M. Ge, Y. Fu, G. Zhang, L. Wang, L. Zhang, L. Qiu, and J. Dang, “MIMO-DoAnet: Multi-channel input and multiple outputs doa network with unknown number of sound sources,” in <em>Proc. INTERSPEECH</em>, 2022.</p>
<p>[16] B. Yang, H. Liu, and X. Li, “SRP-DNN: Learning direct-path phase difference for multiple moving sound source localization,” in <em>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</em>, Singapore, 2022, pp. 721–725.</p>
<p>[17] U. Kowalk, S. Doclo, and J. Bitzer, “Geometry-Aware DOA estimation using a deep neural network with mixed-data input features,” in <em>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</em>, 2022, pp. 1–5.</p>
<p>[18] L. Wang, Z. Jiao, Q. Zhao, J. Zhu, and Y. Fu, “Framewise multiple sound source localization and counting using binaural spatial audio signals,” in <em>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</em>, 2023, pp. 1–5.</p>
<p>[19] J.-H. Cho and J. Chang, “SR-SRP: Super-Resolution based SRP-PHAT for sound source localization and tracking,” in <em>Proc. INTERSPEECH</em>, 2023, pp. 3769–3773.</p>
<p>[20] Y. Wang, B. Yang, and X. Li, “FN-SSL: Full-band and narrow-band fusion for sound source localization,” in <em>Proc. INTERSPEECH</em>, 2023, pp. 3779–3783.</p>
<p>[21] K. Tesch and T. Gerkmann, “Insights into deep non-linear filters for improved multi-channel speech enhancement,” <em>IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 31, pp. 563–575, Nov. 2022.</p>
<p>[22] Y. Yang, C. Quan, and X. Li, “McNet: Fuse multiple cues for multichannel speech enhancement,” in <em>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</em>, 2023, pp. 1–5.</p>
<p>[23] C. Quan and X. Li, “SpatialNet: Extensively learning spatial information for multichannel joint speech separation, denoising and dereverberation,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Lang. Process.</em>, vol. 32, pp. 1310–1323, 2024.</p>
<p>[24] T. N. T. Nguyen, D. L. Jones, K. N. Watcharasupat, H. A. Phan, and W. Gan, “SALSA-Lite: A fast and effective feature for polyphonic sound event localization and detection with microphone arrays,” in <em>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</em>, 2022, pp. 716–720.</p>
<p>[25] X. Li, L. Girin, R. Horaud, and S. Gannot, “Estimation of the direct-path relative transfer function for supervised sound-source localization,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Lang. Process.</em>, vol. 24, no. 11, pp. 2171–2186, 2016.</p>
<p>[26] S. Mohan, M. E. Lockwood, M. L. Kramer, and D. L. Jones, “Localization of multiple acoustic sources with small arrays using a coherence test,” <em>J. Acoust. Soc. Amer.</em>, vol. 123, no. 4, pp. 2136–2147, 2008.</p>
<p>[27] O. Nadiri and B. Rafaely, “Localization of multiple speakers under high reverberation using a spherical microphone array and the direct-path dominance test,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Lang. Process.</em>, vol. 22, no. 10, pp. 1494–1505, Oct. 2014.</p>
<p>[28] K. Shimada, Y. Koyama, S. Takahashi, N. Takahashi, E. Tsunoo, and Y. Mitsufuji, “Multi-ACCDOA: Localizing and detecting overlapping sounds from the same class with auxiliary duplicating permutation invariant training,” <em>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</em>, pp. 316–320, 2021.</p>
<p>[29] Y. Luo, Z. Chen, N. Mesgarani, and T. Yoshioka, “End-to-end microphone permutation and number invariant multi-channel speech separation,” in <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2020, pp. 6394–6398.</p>
<p>[30] H. Taherian, S. E. Eskimez, T. Yoshioka, H. Wang, Z. Chen, and X. Huang, “One model to enhance them all: Array geometry agnostic multi-channel personalized speech enhancement,” in <em>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</em>, 2021, pp. 271–275.</p>
<p>[31] S. Zhang and X. Li, “Microphone array generalization for multichannel narrowband deep speech enhancement,” in <em>Proc. INTERSPEECH</em>, 2021, pp. 666–670.</p>
<p>[32] T. Yoshioka, X. Wang, D. Wang, M. Tang, Z. Zhu, Z. Chen, and N. Kanda, “VarArray: Array-geometry-agnostic continuous speech separation,” in <em>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</em>, 2021, pp. 6027–6031.</p>
<p>[33] P. Vecchiotti, N. Ma, S. Squartini, and G. J. Brown, “End-to-end binaural sound localisation from the raw waveform,” in <em>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</em>, 2019, pp. 451–455.</p>
<p>[34] X. Xiao, S. Zhao, X. Zhong, D. L. Jones, C. E. Siong, and H. Li, “A learning-based approach to direction of arrival estimation in noisy and reverberant environments,” in <em>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</em>, 2015, pp. 2814–2818.</p>
<p>[35] N. Ma, T. May, and G. J. Brown, “Exploiting deep neural networks and head movements for robust binaural localization of multiple sources in reverberant environments,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Lang. Process.</em>, vol. 25, pp. 2444–2453, 2017.</p>
<p>[36] P. Pertila and M. Parviainen, “Time difference of arrival estimation of speech signals using deep neural networks with integrated time-frequency masking,” in <em>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</em>, 2019, pp. 436–440.</p>
<p>[37] B. Yang, R. Ding, Y. Ban, X. Li, and H. Liu, “Enhancing direct-path relative transfer function using deep neural network for robust sound source localization,” <em>CAAI Trans. Intell. Technol.</em>, vol. 7, pp. 446–454, 2022.</p>
<p>[38] D. Tang, M. Taseska, and T. van Waterschoot, “Supervised contrastive embeddings for binaural source localization,” in <em>Proc. IEEE Workshop Appl. Signal Process. Audio Acoust.</em>, 2019, pp. 358–362.</p>
<p>[39] J. Pak and J. W. Shin, “Sound localization based on phase difference enhancement using deep neural networks,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Lang. Process.</em>, vol. 27, pp. 1335–1345, 2019.</p>
<p>[40] S. Adavanne, A. Politis, and T. Virtanen, “Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network,” in <em>Proc. Euro. Signal Process. Conf.</em>, 2017, pp. 1462–1466.</p>
<p>[41] T. N. T. Nguyen, W. S. Gan, R. Ranjan, and D. L. Jones, “Robust source counting and doa estimation using spatial pseudo-spectrum and convolutional neural network,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Lang. Process.</em>, vol. 28, pp. 2626–2637, 2020.</p>
<p>[42] S. Adavanne, A. Politis, J. Nikunen, and T. Virtanen, “Sound event localization and detection of overlapping sources using convolutional recurrent neural networks,” <em>IEEE J. Selected Topics Signal Process.</em>, vol. 13, pp. 34–48, 2018.</p>
<p>[43] E. A. P. Habets, I. Cohen, and S. Gannot, “Generating nonstationary multisensor signals under a spatial coherence constraint,” <em>J. Acoust. Soc. Amer.</em>, vol. 124, no. 5, pp. 2911–2917, 2008.</p>
<p>[44] D. Diaz-Guerra, A. Miguel, and J. R. Beltran, “gpuRIR: A python library for room impulse response simulation with GPU acceleration,” <em>Multimedia Tools Appl.</em>, vol. 80, no. 4, pp. 5653–5671, 2021.</p>
<p>[45] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An ASR corpus based on public domain audio books,” in <em>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</em>, 2015, pp. 5206–5210.</p>
<p>[46] H. W. Löllmann, C. Evers, A. Schmidt, H. Mellmann, H. Barfuss, P. A. Naylor, and W. Kellermann, “The LOCATA challenge data corpus for acoustic source localization and tracking,” in <em>Proc. IEEE Sensor Array and Multichannel Signal Processing Workshop</em>, 2018, pp. 410–414.</p>
<p>[47] A. Brutti, L. Cristoforetti, W. Kellermann, L. Marquardt, and M. Omologo, “WOZ acoustic data collection for interactive TV,” in <em>Proc. International Conference on Language Resources and Evaluation</em>, 2008.</p>
<p>[48] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in <em>International Conference on Learning Representations (ICLR)</em>, 2015.</p>
<p>[49] J. S. Kim, H. J. Park, W. Shin, and S. W. Han, “A robust framework for sound event localization and detection on real recordings,” in <em>Proc. Detect. and Classification of Acoust. Scenes Events Workshop</em>, 2022.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Gavin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/">http://example.com/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Gavin</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/SSL/">SSL</a></div><div class="post_share"><div class="social-share" data-image="https://i.ibb.co/S7ZQZGLX/image.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFN-SSL%EF%BC%9AFull-Band%20and%20Narrow-Band%20Fusion%20for%20Sound%20Source%20Localization/" title="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization"><img class="cover" src="https://i.ibb.co/Q7BGbDBZ/image.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFN-SSL%EF%BC%9AFull-Band%20and%20Narrow-Band%20Fusion%20for%20Sound%20Source%20Localization/" title="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization"><img class="cover" src="https://i.ibb.co/Q7BGbDBZ/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-20</div><div class="title">文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.ibb.co/0fP2mb0/image.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Gavin</div><div class="author-info__description">我的过去常常追赶着我</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/alexandergwm"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/alexandergwm" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:wenmiaogao@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">May you stay young forever, do the thing in your own zone.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text">1. 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">3.</span> <span class="toc-text">2.相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%A3%B0%E6%BA%90%E5%AE%9A%E4%BD%8D"><span class="toc-number">3.1.</span> <span class="toc-text">A. 基于深度学习的声源定位</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-%E5%A4%9A%E7%A7%8D%E9%98%B5%E5%88%97%E7%9A%84SSL"><span class="toc-number">3.2.</span> <span class="toc-text">B. 多种阵列的SSL</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E9%97%AE%E9%A2%98%E5%88%B6%E5%AE%9A"><span class="toc-number">4.</span> <span class="toc-text">3. 问题制定</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%96%B9%E6%B3%95"><span class="toc-number">4.1.</span> <span class="toc-text">4. 方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#A-%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87"><span class="toc-number">4.1.1.</span> <span class="toc-text">A. 学习目标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#B-Full-Narrow-Network-Block"><span class="toc-number">4.1.2.</span> <span class="toc-text">B.Full-Narrow Network Block</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#C-Fixed-Array-Model"><span class="toc-number">4.1.3.</span> <span class="toc-text">C. Fixed Array Model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#D-Variable-Array-Model"><span class="toc-number">4.1.4.</span> <span class="toc-text">D. Variable Array Model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#E-Frame-level-PIT-and-Sound-Source-Localization"><span class="toc-number">4.1.5.</span> <span class="toc-text">E. Frame-level PIT and Sound Source Localization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#F-Configurations"><span class="toc-number">4.1.6.</span> <span class="toc-text">F. Configurations</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Experiments-And-Discussions"><span class="toc-number">4.2.</span> <span class="toc-text">5. Experiments And Discussions</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#A-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">4.2.1.</span> <span class="toc-text">A. 数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A8%A1%E6%8B%9F%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9A"><span class="toc-number">4.2.1.1.</span> <span class="toc-text">模拟数据集：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%9C%9F%E5%AE%9E%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9A"><span class="toc-number">4.2.1.2.</span> <span class="toc-text">真实数据集：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#B-Configurations"><span class="toc-number">4.2.2.</span> <span class="toc-text">B. Configurations</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#C-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">4.2.3.</span> <span class="toc-text">C. 评估指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#D-%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.2.4.</span> <span class="toc-text">D. 消融实验</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#E-%E4%B8%8E%E4%B8%8D%E5%90%8C%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">4.2.5.</span> <span class="toc-text">E. 与不同学习目标的比较</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#F-Results-on-Simulation"><span class="toc-number">4.2.6.</span> <span class="toc-text">F. Results on Simulation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#G-Results-on-the-LOCATA-dataset"><span class="toc-number">4.2.7.</span> <span class="toc-text">G. Results on the LOCATA dataset</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E6%AF%94%E8%BE%83%E5%AE%9E%E9%AA%8C%EF%BC%9A"><span class="toc-number">4.2.7.1.</span> <span class="toc-text">1) 比较实验：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E8%B7%A8%E9%80%9A%E9%81%93%E6%95%B0%E5%92%8C%E4%BB%B0%E8%A7%92%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%B3%9B%E5%8C%96%EF%BC%9A"><span class="toc-number">4.2.7.2.</span> <span class="toc-text">2) 跨通道数和仰角估计的泛化：</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E7%BB%93%E8%AE%BA"><span class="toc-number">5.</span> <span class="toc-text">6.结论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#REFERENCES"><span class="toc-number">6.</span> <span class="toc-text">REFERENCES</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/" title="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization"><img src="https://i.ibb.co/S7ZQZGLX/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization"/></a><div class="content"><a class="title" href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/" title="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization">文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization</a><time datetime="2025-03-19T22:40:18.592Z" title="发表于 2025-03-20 06:40:18">2025-03-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFN-SSL%EF%BC%9AFull-Band%20and%20Narrow-Band%20Fusion%20for%20Sound%20Source%20Localization/" title="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization"><img src="https://i.ibb.co/Q7BGbDBZ/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization"/></a><div class="content"><a class="title" href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFN-SSL%EF%BC%9AFull-Band%20and%20Narrow-Band%20Fusion%20for%20Sound%20Source%20Localization/" title="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization">文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization</a><time datetime="2025-03-19T22:39:09.062Z" title="发表于 2025-03-20 06:39:09">2025-03-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/" title="文献阅读: Masked Autoencoders that Listen"><img src="https://i.ibb.co/9HZr35S/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Masked Autoencoders that Listen"/></a><div class="content"><a class="title" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/" title="文献阅读: Masked Autoencoders that Listen">文献阅读: Masked Autoencoders that Listen</a><time datetime="2024-11-19T13:34:57.816Z" title="发表于 2024-11-19 21:34:57">2024-11-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners"><img src="https://i.ibb.co/gD7cJGh/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Masked Autoencoders Are Scalable Vision Learners"/></a><div class="content"><a class="title" href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners">文献阅读: Masked Autoencoders Are Scalable Vision Learners</a><time datetime="2024-11-17T12:30:01.804Z" title="发表于 2024-11-17 20:30:01">2024-11-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%20-%20Robustness%20and%20Regularization%20of%20Personal%20Audio%20Systems/" title="文献阅读: Robustness and Regularization of Personal Audio Systems"><img src="https://i.ibb.co/c1YpYsm/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Robustness and Regularization of Personal Audio Systems"/></a><div class="content"><a class="title" href="/2024/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%20-%20Robustness%20and%20Regularization%20of%20Personal%20Audio%20Systems/" title="文献阅读: Robustness and Regularization of Personal Audio Systems">文献阅读: Robustness and Regularization of Personal Audio Systems</a><time datetime="2024-10-27T13:47:25.740Z" title="发表于 2024-10-27 21:47:25">2024-10-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Gavin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Have a nice day!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>