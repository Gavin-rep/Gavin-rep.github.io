<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>比赛相关：DCASE 2019 Task3 SELD | Gavin</title><meta name="author" content="Gavin"><meta name="copyright" content="Gavin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1. 比赛相关1.1 描述给定一个多声道音频输入，声音事件定位和检测（SELD）方法的目标是输出录音中声音标签的所有实例、它们各自的onset-offset以及azimuth和elevation的到达方向（DOA）。通过有效实施这种 SELD 方法，可以自动描述具有空间维度的人类活动，并帮助机器与世界进行更无缝的交互。更具体地说，SELD 可以成为辅助聆听系统、场景信息可视化系统、沉浸式交互媒体以">
<meta property="og:type" content="article">
<meta property="og:title" content="比赛相关：DCASE 2019 Task3 SELD">
<meta property="og:url" content="http://example.com/2025/03/03/%E6%AF%94%E8%B5%9B%E7%9B%B8%E5%85%B3%EF%BC%9ADCASE%202019%20Task3%20SELD/index.html">
<meta property="og:site_name" content="Gavin">
<meta property="og:description" content="1. 比赛相关1.1 描述给定一个多声道音频输入，声音事件定位和检测（SELD）方法的目标是输出录音中声音标签的所有实例、它们各自的onset-offset以及azimuth和elevation的到达方向（DOA）。通过有效实施这种 SELD 方法，可以自动描述具有空间维度的人类活动，并帮助机器与世界进行更无缝的交互。更具体地说，SELD 可以成为辅助聆听系统、场景信息可视化系统、沉浸式交互媒体以">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.ibb.co/ZRtszJys/image.png">
<meta property="article:published_time" content="2025-03-03T14:26:06.037Z">
<meta property="article:modified_time" content="2025-03-11T13:40:16.771Z">
<meta property="article:author" content="Gavin">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="DCASE">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.ibb.co/ZRtszJys/image.png"><link rel="shortcut icon" href="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"><link rel="canonical" href="http://example.com/2025/03/03/%E6%AF%94%E8%B5%9B%E7%9B%B8%E5%85%B3%EF%BC%9ADCASE%202019%20Task3%20SELD/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '比赛相关：DCASE 2019 Task3 SELD',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-11 21:40:16'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.ibb.co/0fP2mb0/image.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.ibb.co/ZRtszJys/image.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Gavin"><img class="site-icon" src="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"/><span class="site-name">Gavin</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">比赛相关：DCASE 2019 Task3 SELD</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-03T14:26:06.037Z" title="发表于 2025-03-03 22:26:06">2025-03-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-11T13:40:16.771Z" title="更新于 2025-03-11 21:40:16">2025-03-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%AF%94%E8%B5%9B%E7%9B%B8%E5%85%B3/">比赛相关</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="比赛相关：DCASE 2019 Task3 SELD"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="1-比赛相关"><a href="#1-比赛相关" class="headerlink" title="1. 比赛相关"></a>1. 比赛相关</h2><h3 id="1-1-描述"><a href="#1-1-描述" class="headerlink" title="1.1 描述"></a>1.1 描述</h3><p>给定一个多声道音频输入，声音事件定位和检测（SELD）方法的目标是输出录音中声音标签的所有实例、它们各自的onset-offset以及azimuth和elevation的到达方向（DOA）。通过有效实施这种 SELD 方法，可以自动描述具有空间维度的人类活动，并帮助机器与世界进行更无缝的交互。更具体地说，SELD 可以成为辅助聆听系统、场景信息可视化系统、沉浸式交互媒体以及基于场景部署服务的空间机器认知中的一个重要模块。一个直接的实际应用是机器人识别和跟踪感兴趣的声源。在当前的挑战中，只考虑了静态场景，这意味着所提供录音中的每个声音事件实例在其整个持续时间内都是固定位置的。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310202205437.png" alt="image-20250310202205437"></p>
<h3 id="1-2-音频数据集"><a href="#1-2-音频数据集" class="headerlink" title="1.2 音频数据集"></a>1.2 音频数据集</h3><p>该任务提供了两个数据集：TAU Spatial Sound Events 2019 - Ambisonic 或 TAU Spatial Sound Events 2019 - Microphone Array，这两个数据集的声音场景完全相同，唯一的区别在于音频格式不同。TAU Spatial Sound Events 2019 - Ambisonic 提供四声道一阶 Ambisonic（FOA）录音，而 TAU Spatial Sound Events 2019 - Microphone Array 则提供来自四面体阵列配置的四声道定向麦克风录音。这两种格式都是从同一个麦克风阵列中提取的，有关每种格式空间特性的更多信息，请参阅下文。参与者可以根据自己喜欢的音频格式选择其中一个数据集或两个数据集。两个数据集都由开发集和评估集组成。开发集由 400 段一分钟长的录音组成，采样频率为 48000 Hz，分为四个交叉验证片段，每个片段 100 段录音。评估集由 100 个一分钟录音组成。这些录音是利用从五个室内地点收集到的空间房间脉冲响应（IR），在 504 个独特的azimuth-elevation-distance组合下合成的。此外，为了合成录音，收集到的 IR 与 DCASE 2016 任务 2 中的孤立声音事件数据集进行了卷积。最后，为了创建逼真的声音场景录音，在IR录音地点收集的自然环境噪音被添加到合成录音中，使其平均信噪比为30dB。</p>
<p>IRs 由坦佩雷大学于 2017 年 12 月至 2018 年 6 月期间在芬兰收集。数据收集工作得到了欧洲研究理事会（European Research Council）的资助，资助协议号为 637422 EVERYSOUND。</p>
<h3 id="1-3-记录过程"><a href="#1-3-记录过程" class="headerlink" title="1.3 记录过程"></a>1.3 记录过程</h3><p>现实生活中的IR录音是使用 Eigenmike 球形麦克风阵列收集的。在 Eigenmike 周围使用 Genelec G Two 扬声器播放最大长度序列 (MLS)。在录音过程中，MLS 的播放音量确保比环境音量高出 30 分贝。在 STFT 域，使用已知测量信号（MLS）和远场录音之间的最小二乘回归，在每个频率上独立获得 IR。这些IR在以下方向收集：</p>
<ul>
<li>在距离 Eigenmike 1 米处，从-40°到 40°，每 10°方位角有 9 个仰角，每 10°增量 36 个IRs，从而得到 324 个离散的 DOAs。</li>
<li>在距离 Eigenmike 2 米处，从-20°到 20°，每 10°方位角有 5 个仰角，每个仰角递增 10°，每个仰角有 36 个IRs，这样就有 180 个离散的 DOAs。</li>
</ul>
<p>我们在芬兰埃尔万塔坦佩雷大学校园内的五个不同室内地点记录了IRs。此外，我们还在这五个地点收集了 30 分钟的环境噪声记录，红外记录设置保持不变。室内地点说明如下：</p>
<ol>
<li>语言中心 - 大型公共区域，带有多个座椅桌和地毯地板。人们聊天和工作。</li>
<li>Reaktori建筑 - 大型自助餐厅，带有多个座椅桌和地毯地板。人们聊天和吃饭。</li>
<li>Festia大楼 - 高天花板走廊，带有硬地板。人们走来聊天聊天。</li>
<li>Tietotalo建筑 - 走廊，周围有教室和硬地板。人们走来聊天聊天。</li>
<li>Sähkötalo建筑 - 大走廊，有多个沙发和桌子，在不同部分的硬地板和地毯地板。人们走来聊天聊天。</li>
</ol>
<h3 id="1-4-录制格式和数据集规范"><a href="#1-4-录制格式和数据集规范" class="headerlink" title="1.4 录制格式和数据集规范"></a>1.4 录制格式和数据集规范</h3><p>来自 DCASE 2016 任务 2 的孤立声音事件数据集由 11 个类别组成，每个类别有 20 个示例。这些示例被随机分成五组，每类示例数量相等，前四组用于合成开发数据集的四个分集，剩余一组用于评估数据集。对于数据集的每个分集，我们都要合成 100 个录音。每段录音都是从相应的录音集中随机选择声音事件示例并指定开始时间，然后从收集到的 IR 中随机选择一段。最后，通过将每个指定的声音示例与各自的IR信号进行卷积，我们将它们定位在与 Eigenmike 之间的给定距离、方位角和仰角上。我们确保录音中的所有声音事件都使用来自单一位置的红外信号。此外，在每个分段中，有一半的录音是由最多两个时间上重叠的声音事件合成的，而其他录音则是由没有重叠的声音事件合成的。最后，在合成录音中加入在相应IR位置采集的环境噪声，使声音事件的平均信噪比为 30 dB。</p>
<p>由于 IR 中的通道数等于 Eigenmike 中的麦克风数（32），为了创建 TAU 空间声音事件 2019 - 麦克风阵列数据集，我们使用了通道 6、10、26 和 22，分别对应麦克风位置（45°，35°，4.2 cm）、（-45°，-35°，4.2 cm）、（135°，-35°，4.2 cm）和（-135°，35°，4.2 cm）。使用的球面坐标系为右旋坐标系，前端为（0°，0°），左端为（90°，0°），顶部为（0°，90°）。此外，TAU Spatial Sound Events 2019 - Ambisonic 数据集是通过将 32 个声道的麦克风信号转换为 FOA，并根据 Eigenmike 阵列响应的消声测量结果使用编码滤波器获得的。</p>
<p>对于基于模型的定位方法，阵列响应可视为已知。以下对两种格式建模的理论空间响应（转向矢量）描述了每个信道对从方位角 ϕ 和仰角 θ 给定的 DOA 入射源的定向响应。</p>
<p>对于一阶Ambisonics::</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310203145405.png" alt="image-20250310203145405"></p>
<p>对于安装在球形障板上的四面体传声器阵列，其指向性阵列响应的分析表达式由展开式给出：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310203218681.png" alt="image-20250310203218681"></p>
<p>其中$m$ 为通道号, $(\phi_m,\theta_m)$ 为特别的麦克风的azimuth和elevation， $w&#x3D;2\pi f$ 为角频率，$R&#x3D;0.042cm$ 为阵列半径，$cos(\gamma_m)$ 为麦克风和DOA之间的cos角度，$P_n$ 是没有进行规范化的Legendre多项式，$h_n’^{2}$ 为相对于第二类球Hankel函数的导数。扩展仅限于 30 个项，在 20kHz 以下的建模误差可以忽略不计。请注意，Ambisonics 格式与频率无关，在使用特定麦克风阵列时，频率可达 9kHz 左右，而在频率较高时，实际编码响应开始逐渐偏离上述理想响应。</p>
<p>总之，在评估数据集和开发数据集的四个分集中，每个分集共有 100 条录音。这 100 条录音由 10 条录音组成，这些录音有的有两个声音事件，有的没有时间上重叠的声音事件，是用五个地点的IR信号合成的（10 * 2 * 5 &#x3D; 100）。每个开发数据集和评估数据集之间的唯一明显区别是所使用的孤立声音事件示例。虽然每个开发数据集分集和评估数据集都由来自所有五个地点的IR信号组成，但数据集只保证了 36 个方位角和 9 个仰角中每个分集中声音事件的均衡分布，而不保证在单个地点采集的IR信号在单个分集中全部出现。例如，Reaktori 大楼的一些IR信号可能不在第一个分段中，但可能出现在其他任何一个分段中。</p>
<p>有关IR记录收集和数据集合成的更多详情，请参阅</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310203711713.png" alt="image-20250310203711713"></p>
<h3 id="1-5-参考labels"><a href="#1-5-参考labels" class="headerlink" title="1.5 参考labels"></a>1.5 参考labels</h3><p>作为标签，我们为开发数据集中的每个录音提供了 CSV 格式文件，其中列出了声音事件、各自的时间起始-偏移时间、方位角和仰角。由于开发数据集提供了四个交叉验证分区，因此可作为独立数据集用于今后的工作。如果您正在准备基于 DCASE 挑战赛设置的出版物，并希望用官方挑战赛评估设置来评估您提出的系统，请联系任务协调人。任务协调人可以为数量有限的系统输出提供非官方评分。</p>
<h3 id="1-6-任务设置"><a href="#1-6-任务设置" class="headerlink" title="1.6 任务设置"></a>1.6 任务设置</h3><p>如下表所示，开发数据集由预先定义的四个交叉验证分集组成。这些数据集由音频记录和相应的元数据组成，元数据描述了声音事件及其在每个记录中的位置。参赛者需要报告其方法在四个分折测试中的表现。为了对开发数据集上的方法进行公平比较，参赛者不得更改已定义的分片。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310203820046.png" alt="image-20250310203820046"></p>
<p>评估数据集在最终提交截止日期前几周发布。该数据集仅由录音组成，不含任何元数据&#x2F;标签。参与者可以决定训练程序，即开发数据集中训练和验证文件的数量、集合模型的数量，并提交 SELD 在评估数据集中的性能结果。</p>
<h3 id="1-7-训练集"><a href="#1-7-训练集" class="headerlink" title="1.7 训练集"></a>1.7 训练集</h3><p>开发数据集中的记录遵循命名规则：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310203948352.png" alt="image-20250310203948352"></p>
<p>使用其脉冲响应合成录音的位置信息或录音中重叠声音事件的数量信息仅供学员了解其方法在不同条件下的性能。我们鼓励参赛者针对这些条件进行单独研究，并在 DCASE 2019 研讨会上发表报告。但在挑战赛中，我们只考虑在训练或推理过程中不使用重叠声音事件的位置或数量信息的通用方法。</p>
<h3 id="1-8-验证集"><a href="#1-8-验证集" class="headerlink" title="1.8 验证集"></a>1.8 验证集</h3><p>评估数据集由 100 个录音组成，没有任何位置信息，也没有重叠声音事件的数量，命名规则如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310204051349.png" alt="image-20250310204051349"></p>
<h3 id="1-9-任务规则"><a href="#1-9-任务规则" class="headerlink" title="1.9 任务规则"></a>1.9 任务规则</h3><p>不允许使用外部数据。<br>不允许对所提供的开发数据集交叉验证分割进行操作。<br>开发数据集可在不使用外部数据的情况下进行扩充（如使用音调移动或时间拉伸等技术）。<br>参赛者不得对评估数据进行主观判断，也不得对其进行注释。评估数据集不能用于训练提交的系统。</p>
<h3 id="1-10-提交"><a href="#1-10-提交" class="headerlink" title="1.10 提交"></a>1.10 提交</h3><p>评估数据集中 100 条记录中每条记录的结果都应以 CSV 文件形式收集。同样，我们也会为开发数据集中的 400 个录音（四个折叠的测试分割结果）中的每个录音收集文件形式的 CSV。这种按文件分类的 CSV 与录音的名称相同，但扩展名为 .csv，每行包含以下信息。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310204147879.png" alt="image-20250310204147879"></p>
<p>输出文件示例如下。如果使用基线代码，则会自动生成该输出。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310204204933.png" alt="image-20250310204204933"></p>
<p>输出文件显示，在第 10 至第 13 连续四帧的位置（10° ，-20°）和（-50° ，30°），有两个类 1 处于活动状态。在第 13 帧中，除类别 1 外，类别 2 也处于活动状态。最后，第 4 类在第 112-114 帧的位置 (-40° , 0°) 活跃。</p>
<p>评估以 20 毫秒的hop length进行，即 60 秒长的音频记录有 3000 个帧。如果参与者在研究中使用了不同的跳变长度，我们希望参与者使用合适的后处理方法提取 20 毫秒跳变长度下的信息，并将其作为评估结果提交。所提供数据集中 11 个类别中每个类别的索引可在随数据集和基线代码提供的元数据中找到。方位角的预期范围为 -180° 至 170°，仰角的预期范围为 -40° 至 40°，任何超出这些限制的值都将被剪切为相应的最小值或最大值。</p>
<p>除 CSV 文件外，参赛者还需在提供的文件中更新其方法的信息，并提交一份介绍该方法的技术报告。我们允许每位参赛者&#x2F;团队最多提交 4 个系统输出。每个系统的元信息应在一个单独的文件中提供，其中包含任务的具体信息。所有文件应打包成 zip 文件提交。有关挑战信息的详细信息可在提交页面找到。<br>有关提交的详细信息，请参见提交页面。</p>
<h3 id="1-11-评估"><a href="#1-11-评估" class="headerlink" title="1.11 评估"></a>1.11 评估</h3><p>SELD 任务使用 SED 和 DOA 估计的单个指标进行评估。对于 SED，我们使用以一秒为单位计算的 F score和Error Rate (ER)。有关 SED 指标的简短描述请参见此处，详细信息请参见：</p>
<p>对于DOA的估计，我们用两个帧级指标: DOA error和frame recall。DOA 误差是预测 DOA 与参考 DOA 之间的平均角度误差。对于长度为 $ T $ 的录音，设 $ \mathbf{DOA}<em>{R}^{t} $ 为时间帧 $t $ 处所有参考 DOA 的列表，$\mathbf{DOA}</em>{E}^{t} $ 为时间帧 $ t $ 处所有估计 DOA 的列表。DOA 误差定义如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310204833445.png" alt="image-20250310204833445"></p>
<p>其中，$D_{E}^{t} $ 是在时间帧 $ t $ 处 $ \mathbf{DOA}_{E}^{t} $ 中 DOA 的数量。<strong>Hungarian 算法</strong>用于解决匹配问题，即将单个估计的 DOA 与相应的参考 DOA 进行匹配。Hungarian 算法通过计算它们之间的中心角来估计成对的匹配成本：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310204928807.png" alt="image-20250310204928807"></p>
<p>其中，参考 DOA 由方位角 $ \phi_R $$( \phi_R \in [-\pi, \pi] $）和仰角 $ \lambda_R $($\lambda_R \in [-\pi&#x2F;2, \pi&#x2F;2] $）表示，估计 DOA 也以相同的范围表示为 ( $\phi_E, \lambda_E) $。</p>
<p>为了考虑估计 DOA 和参考 DOA 数量不相等的时间帧，我们报告了第二个指标帧召回率，计算公式为</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310205127971.png" alt="image-20250310205127971"></p>
<p>其中$D_R^t$ 在$t$ 帧处 在$DOA_R^t$ 中DOA的数量，$\mathbf{1}() $ 是指示函数，当 $(D_{R}^{t} &#x3D; D_{E}^{t}) $ 时返回 1，否则返回 0。</p>
<p>有关 DOA 指标的更多详情，请查阅 </p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310205307903.png" alt="image-20250310205307903"></p>
<p>理想的 SELD 方法的error rate为 0，F score为 1（以 % 为单位），DOA 误差为 0°，帧召回率为 1（以 % 为单位）。为了对提交的方法进行比较，我们将对每种方法的所有四项指标进行单独排名，最终排名将使用排名的累计最小值。<br>请注意：四个交叉验证折叠被视为一个实验，这意味着只有在训练和测试所有折叠后才会计算指标，而不是计算单个折叠的平均值或单个类性能的平均值。在计算指标之前，所有折叠的中间指标（插入、删除、替换）都会累积起来。有关原因的更多信息，请参阅以下论文：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310205350661.png" alt="image-20250310205350661"></p>
<h3 id="1-12-Baseline"><a href="#1-12-Baseline" class="headerlink" title="1.12 Baseline"></a>1.12 Baseline</h3><p>作为基准，我们使用了最近发布的 SELDnet，这是一种基于 CRNN 的方法，它使用 SED 的置信度来估计每个声音类别的一个 DOA。SED 是通过多类多标签分类获得的，而 DOA 则是通过多输出回归获得的。SELDnet 使用 FFT 的幅值和相位分量作为输入特征，SED labels表示为one-hot encoding，DOA 表示为以弧度为单位的方位角和仰角。有关 SELDnet 的更多详情，请参阅：SELDnet：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310205523110.png" alt="image-20250310205523110"></p>
<p>此处的基线实现与上述出版物中的实现不同之处在于，基线直接输出方位角和仰角，而不是 DOA 向量的笛卡尔分量。<br>请注意：SELD 任务正处于初级阶段。虽然作为基线的 SELDnet 架构在一系列条件下都能有效执行 SELD 任务，但在数据驱动和基于模型的估计方面仍有许多方法尚未开发。我们相信，提议的任务和数据集将有助于探索新方法，而获得的结果，无论较差或较好，都应被视为有效研究，并与社区共享。这只会有利于未来的研究人员了解哪些方法有效，哪些方法无效。</p>
<hr>
<h2 id="2-Baseline-Sound-Event-Localization-and-Detection-of-Overlapping-Sources-Using-Convolutional-Recurrent-Neural-Networks"><a href="#2-Baseline-Sound-Event-Localization-and-Detection-of-Overlapping-Sources-Using-Convolutional-Recurrent-Neural-Networks" class="headerlink" title="2. Baseline: Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks"></a>2. Baseline: Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks</h2><p>摘要——本文提出了一种卷积-递归神经网络（Convolutional Recurrent Neural Network, CRNN），用于三维（3D）空间中多个重叠声事件的联合声事件定位与检测（SELD）。该网络以一系列连续的spectrogram时间帧作为输入，并并行映射到两个输出。首先，声事件检测（SED）被建模为多标签分类任务，在每个时间帧上预测所有声事件类别的时间活动性。其次，定位任务通过多输出回归估计每个声事件类别的三维笛卡尔坐标系下的入射方向（DOA）。该方法能够将多个DOA与相应的声事件标签进行关联，并进一步随时间跟踪这一关联关系。该方法分别使用spectrogram的相位和幅度分量作为特征，这些特征是在每个音频通道上计算的，从而避免了任何特定于方法或阵列的特征提取。本文在五个Ambisonic格式数据集和两个环形阵列格式数据集上对该方法进行了评估，这些数据集包含了不同程度重叠的声事件，并涉及自由声场（无混响）、混响环境和真实场景。本方法与两种SED基线方法、三种DOA估计基线方法和一种SELD基线方法进行了比较。结果表明，所提出的方法具有通用性，可适用于任何阵列结构，并且对未知DOA值、混响环境以及低信噪比（SNR）场景具有鲁棒性。此外，与最优基线方法相比，本方法在所有数据集上均表现出更高的DOA估计召回率，尤其在高重叠声事件数量的情况下，其召回率显著优于最优基线方法。</p>
<p>索引项 - 声音事件检测、到达方向估计、卷积递归神经网络</p>
<h3 id="I-介绍"><a href="#I-介绍" class="headerlink" title="I. 介绍"></a>I. 介绍</h3><p>声事件定位与检测（SELD）是一项综合任务，包括识别每个声事件的时间活动性、估计其在活动时的空间位置轨迹，并进一步将文本标签与声事件关联。这种方法可以自动描述社会和人类活动，并帮助听障人士可视化声音。例如，机器人可以利用该技术进行导航并自然地与周围环境交互[1–4]。智能城市、智能家居和工业领域可以将其用于音频监控[5–8]。智能会议室能够识别语音及其他事件，并利用这些信息进行波束成形，从而增强语音，以用于远程会议或稳健的自动语音识别[9–13]。自然学家可以使用该技术进行生物多样性监测[14–16]。此外，在带有360°音频的虚拟现实（VR）应用中，SELD可以帮助用户可视化声事件。</p>
<h4 id="A-Sound-event-detection"><a href="#A-Sound-event-detection" class="headerlink" title="A. Sound event detection"></a>A. Sound event detection</h4><p>​	SELD 任务可以大致分为两个子任务：声事件检测（SED）和声源定位。SED 旨在检测声事件的时间起点（onset）和终点（offset），并进一步将文本标签与检测到的事件关联。在现实生活中，声事件通常会与其他声事件在时间上重叠，而识别所有重叠声事件的任务被称为<strong>复调 SED（polyphonic SED）</strong>。在文献中，SED 任务通常采用不同的监督分类方法来预测每个声事件类别的逐帧活动性。一些常见的分类器包括<strong>高斯混合模型（GMM）-隐马尔可夫模型（HMM）</strong>[27]，<strong>全连接（FC）神经网络</strong>[28]，<strong>递归神经网络（RNN）</strong>[29-32]，以及<strong>卷积神经网络（CNN）</strong>[33, 34]。最近，研究人员通过<strong>依次堆叠 CNN、RNN 和 FC 层</strong>，形成<strong>卷积-递归神经网络（CRNN）</strong>，并取得了最先进的结果[35-39]。</p>
<p>​	近年来，为了提高对重叠声事件的识别能力，提出了多种多通道 SED 方法[39–43]，这些方法在 DCASE 2016 和 2017 评测挑战的真实场景 SED 任务中表现优异。更近期的研究中，我们分析了相同声场场景下，使用<strong>单通道、双耳（binaural）和一阶 Ambisonics（FOA）麦克风</strong>采集的音频数据上的 SED 性能[35]。其中，阶数（order）代表格式的空间分辨率<strong>，</strong>一阶（first-order）对应四个通道。结果表明，随着空间采样的增加，重叠声事件的识别性能得到提升，其中FOA 方式取得了最佳性能。</p>
<h4 id="B-Sound-source-localization"><a href="#B-Sound-source-localization" class="headerlink" title="B. Sound source localization"></a>B. Sound source localization</h4><p>​	声源定位是指确定声源相对于麦克风的方向或位置。在本文中，我们仅涉及声事件方向的估计，这通常被称为入射方向（DOA）估计。文献中的 DOA 方法可大致分为参数化方法和基于深度神经网络（DNN）的方法。一些常见的参数化方法包括基于<strong>到达时间差（TDOA）</strong>[44]、<strong>引导响应功率（SRP）</strong>[45]、<strong>多重信号分类（MUSIC）</strong>[46]，以及<strong>基于旋转不变技术的信号参数估计（ESPRIT）</strong>[47]。这些方法在算法复杂度、阵列几何结构的约束以及对声学场景的模型假设等方面有所不同。MUSIC 等子空间方法可适用于不同的阵列类型，并能够对多个声源进行高分辨率 DOA 估计。然而，这类方法需要准确估计<strong>活跃声源的数量</strong>，这一点通常难以获得。此外，它们对混响环境和低信噪比（SNR）场景较为敏感[48]。</p>
<p>​	近年来，DNN 方法被应用于克服参数化方法的一些缺点，并在混响环境和低 SNR 场景下表现出更强的鲁棒性。此外，在 DNN 框架中实现定位任务，使其能够无缝集成到更广泛的 DNN 任务中，例如 SELD[20]。机器人可以利用该技术进行基于声源的导航，并在多说话人场景中实现自然交互[1–4]。表 I 总结了近年来基于 DNN 的 DOA 估计方法。所有这些方法均针对静态点声源进行 DOA 估计，并且在混响环境下的性能等同或优于参数化方法。此外，文献[4, 18, 20, 25]提出了一些方法，可通过从数据本身估计活跃声源的数量来同时检测多个重叠声事件的 DOA。大多数方法采用分类方式，在一组固定角度上估计声源存在的可能性，而文献[22, 23]采用回归方法，使 DNN 产生连续输出。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310211500035.png" alt="image-20250310211500035"></p>
<p>​	所有过去的研究都是在不同的阵列几何结构上进行评估的，因此直接进行性能比较较为困难。大多数方法使用安装在机器人、环形或分布式阵列上的麦克风来估计<strong>完整方位角</strong>（表 I 中的“Full”），而其余方法则使用线性阵列，仅估计<strong>180°范围内的方位角</strong>。尽管已有少数方法同时估计了<strong>方位角（azimuth）和俯仰角（elevation）</strong>[24, 25]，但大多数方法仅估计了方位角[1–4, 17–20]。在[25]中，我们特别研究了<strong>方位角与俯仰角的联合估计</strong>，这得益于使用了通过球形阵列获得的<strong>Ambisonic 信号（FOA）</strong>。在阵列信号处理中，Ambisonics 也被称为<strong>球谐（SH）信号</strong>，它可以通过对不同阵列配置（如环形或平面阵列用于 2D 采集，球形或体积阵列用于 3D 采集）的录音进行适当的线性变换获得[49]。Ambisonic 信号的空间特性与录音设备无关，因此，基于这种硬件无关格式的研究能够提高未来评估和结果的可比性。</p>
<p>​	大多数先前提出的基于 DNN 的 DOA 估计算法依赖于<strong>单个阵列或分布式全向麦克风阵列</strong>，主要利用麦克风之间的<strong>相位或时间延迟差异</strong>来捕捉声源位置信息。然而，<strong>具有完整方位角和俯仰角覆盖的紧凑型麦克风阵列</strong>（如球形麦克风阵列）主要依赖于传感器的<strong>方向性</strong>来捕获空间信息，这主要体现在<strong>通道之间的幅度差异</strong>。基于这一事实，我们在[25]中提出使用<strong>谱图的幅度和相位分量</strong>作为输入特征。这使得 DOA 估计方法[25]能够适用于不同的阵列配置，而无需使用特定于方法的特征提取，如<strong>双耳间级差（ILD）、双耳间时间差（ITD）、广义互相关（GCC）或空间协方差矩阵的特征向量</strong>等，这些特征在先前的方法（表 I）中曾被使用。</p>
<h4 id="C-Joint-localization-and-detection"><a href="#C-Joint-localization-and-detection" class="headerlink" title="C. Joint localization and detection"></a>C. Joint localization and detection</h4><p>​	在多个重叠声事件的情况下，DOA 估计任务变成了<strong>经典的目标跟踪问题</strong>，即在不必识别声源的情况下，将多个 DOA 估计正确地与各自的声源关联[50, 51]。如果 SED 和 DOA 估计是<strong>分开进行</strong>的，那么在<strong>复调 SELD 任务</strong>中，该问题进一步扩展为<strong>已识别声事件与估计 DOA 之间的数据关联问题</strong>[13]。解决数据关联问题的一种方法是<strong>联合预测 SED 和 DOA</strong>。在这方面，据作者所知，文献[20]是<strong>唯一一个执行 SELD 任务的基于 DNN 的方法</strong>。其他结合 SED 和参数化 DOA 估计的方法包括[6, 13, 52, 53]。Lopatka 等人[53]使用了<strong>三维声强矢量传感器</strong>，结合 MPEG-7 频谱和时域特征，并采用支持向量机（SVM）分类器来估计<strong>五类非重叠声事件的方位角 DOA</strong>。Butko 等人[13]使用<strong>分布式麦克风阵列</strong>，采用 GMM-HMM 分类器来识别<strong>14 种不同的声事件（每次最多两个重叠）</strong>，并使用 SRP 方法对其在会议室内进行定位。Chakraborty 等人[52]改进了[13]中的方法，将 SRP 方式的定位替换为<strong>基于声学模型的定位</strong>，从而解决了[13]中面临的数据关联问题。相比之下，Hirvonen[20]从环形阵列的每个麦克风中提取逐帧的<strong>频谱功率</strong>，并使用 CNN 分类器将其映射到数据集中每个声事件类别的<strong>八个方位角</strong>。在这种输出格式下，DOA 估计的<strong>方位角分辨率受限于训练数据中的角度</strong>，因此对于<strong>未见过的 DOA 值，性能未知</strong>。对于包含更多声事件、且方位角和俯仰角分辨率更高的大型数据集，这种方法会导致<strong>输出节点数量大幅增加</strong>。在这种情况下，由于<strong>每帧的正类标签数（1 或 2）远少于负类标签数</strong>，训练这样一个 DNN 可能会面临<strong>数据不均衡问题</strong>。此外，训练如此多类别的网络需要<strong>庞大的数据集</strong>，以确保每个类别有足够的样本。另一方面，这种输出格式使得网络能够<strong>在同一时间帧内，同时识别多个不同位置的相同声事件</strong>。</p>
<h4 id="D-本篇论文的贡献"><a href="#D-本篇论文的贡献" class="headerlink" title="D. 本篇论文的贡献"></a>D. 本篇论文的贡献</h4><p>总体而言，现有的 SELD 方法数量有限[6, 13, 20, 52, 53]，其中只有<strong>一种基于 DNN 的方法</strong>[20]。另一方面，在 SELD 的子任务——SED 和 DOA 估计方面，文献中已有多个基于 DNN 的方法。然而，目前尚无<strong>全面的研究</strong>探讨影响这些基于 DNN 的 SED、DOA 和 SELD 方法性能的各种选择，也<strong>没有研究对这些方法进行多基线对比，并在不同声学条件下进行广泛评估</strong>。</p>
<p>此外，现有的 SELD 方法[6, 13, 52, 53]<strong>最多只能定位一个或两个重叠声事件</strong>，无法扩展到更多的重叠声源。而<strong>唯一的基于 DNN 的 SELD 方法</strong>[20]仅能在<strong>预定义的网格方向</strong>上进行声事件定位，并且随着声事件类别的增加和空间分辨率的提高，其输出类别数量大幅增长。此外，上述所有 SELD 方法都依赖于<strong>特定于方法的特征</strong>，因此并<strong>不独立于输入阵列结构</strong>。</p>
<p>与现有的 SELD 方法相比，本文在两个主要方面提出了创新：<strong>所提出的 SELD 方法本身，以及所进行的全面评估研究</strong>。</p>
<p><strong>所提出的 SELD 方法的创新点如下：</strong></p>
<ol>
<li>这是<strong>首个能够同时定位和识别两个以上重叠声事件，并随时间跟踪其活动</strong>的方法。</li>
<li>该方法能够**在任意方位角（azimuth）和俯仰角（elevation）**下进行声源定位，并且对**未知空间位置、混响和环境噪声**具有鲁棒性。</li>
<li>该方法具有<strong>通用性</strong>，能够适用于<strong>任意输入阵列结构</strong>进行 SELD 任务的学习。</li>
<li>具体而言，我们提出使用<strong>复调 SED 输出</strong>[39]作为置信度度量，以选择通过回归方式估计的 DOA。这种方法不仅扩展了<strong>最先进的复调 SED 性能</strong>[39]到<strong>复调 SELD</strong>，同时也<strong>解决了由于复调性导致的 SELD 任务中的数据关联问题</strong>[13]。</li>
</ol>
<p><strong>第二个主要创新点在于，我们对该方法的各类设计选择进行了全面的性能评估，包括：</strong></p>
<ol>
<li>DNN 架构</li>
<li>输入特征</li>
<li>DOA 输出格式</li>
</ol>
<p>此外，我们还提供了该方法的<strong>综合实验结果</strong>，并与<strong>六个基线方法</strong>（两个 SED、三个 DOA 估计和一个 SELD 基线）进行对比。这些实验在<strong>七个不同数据集</strong>上进行评估，这些数据集涵盖了：</p>
<ul>
<li><strong>不同声学条件</strong>（自由声场和混响场景，模拟和真实脉冲响应）</li>
<li><strong>不同阵列配置</strong>（Ambisonic 阵列和环形阵列）</li>
<li><strong>不同重叠声事件数量</strong></li>
</ul>
<p>为了<strong>促进研究的可复现性</strong>，我们已<strong>公开发布所提出的方法和所有使用的数据集</strong>³。此外，用于模拟数据集的<strong>真实脉冲响应</strong>也已发布，以便用户使用自定义声事件进行实验。</p>
<p>本文其余部分的组织结构如下：</p>
<ul>
<li><strong>第二部分</strong> 描述了所提出的 SELD 方法和训练过程。</li>
<li><strong>第三部分</strong> 介绍了数据集、基线方法、评估指标以及实验设置。</li>
<li><strong>第四部分</strong> 展示了在评估数据集上的实验结果，并与基线方法进行对比和讨论。</li>
<li><strong>第五部分</strong> 总结了本文研究的结论。</li>
</ul>
<h3 id="II-方法"><a href="#II-方法" class="headerlink" title="II. 方法"></a>II. 方法</h3><p>所提出的 SELD 方法的框图如<strong>图 1a</strong> 所示。该方法的输入为<strong>多通道音频</strong>，并从每个音频通道中提取<strong>相位和幅度谱图</strong>，将它们分别作为特征。该方法以连续谱图帧的特征序列作为输入，并预测每个输入帧中<strong>所有活动的声事件类别及其对应的空间位置</strong>，从而生成<strong>声事件的时间活动性和 DOA 轨迹</strong>。</p>
<p>具体而言，该方法采用**卷积-递归神经网络（CRNN）**来将输入特征序列**并行映射到两个输出**：</p>
<ol>
<li><strong>第一输出：SED 任务</strong>——作为<strong>多标签分类任务</strong>，允许网络<strong>同时估计每个帧中多个声事件的存在情况</strong>。</li>
<li><strong>第二输出：DOA 任务</strong>——在<strong>连续的三维空间</strong>中进行 DOA 估计，作为<strong>多输出回归任务</strong>。其中，每个声事件类别都关联到<strong>三个回归器</strong>，用于估计 DOA 在单位球面上相对于麦克风的<strong>三维笛卡尔坐标（x, y, z）</strong>。</li>
</ol>
<p>网络的<strong>SED 输出值</strong>在**[0,1]的连续范围内<strong>，对应数据集中每个声事件的置信度，该值随后通过阈值处理，转换为二进制决策，以确定声事件的活动状态（如</strong>图 1b** 所示）。最后，针对这些<strong>被检测为活动的声事件类别</strong>，其相应的<strong>DOA 估计</strong>提供其<strong>空间位置</strong>。</p>
<p>关于<strong>特征提取和所提出方法的详细描述</strong>将在后续章节中进行说明。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310212629544.png" alt="image-20250310212629544"></p>
<h4 id="A-特征提取"><a href="#A-特征提取" class="headerlink" title="A. 特征提取"></a>A. 特征提取</h4><p>​	从多声道音频的每个 <strong>$C$</strong> 通道中，使用 <strong>$M$ 点离散傅里叶变换（DFT）</strong> 在<strong>长度为 $M$ 且 50% 重叠的汉明窗</strong>上提取<strong>频谱图</strong>。然后，从频谱图中<strong>提取相位和幅度</strong>，并分别作为<strong>独立特征</strong>。仅使用 <strong>$M&#x2F;2$ 个正频率分量</strong>，不包括<strong>第 0 频率 bin</strong>。</p>
<p>在 <strong>图 1a</strong> 的<strong>特征提取模块</strong>中，输出的是**$T$ 帧的特征序列**，其整体维度为：</p>
<p>$T \times \frac{M}{2} \times 2C$</p>
<p>其中：</p>
<ul>
<li>**$T$**：时间帧数</li>
<li>**$M&#x2F;2$**：正频率分量数（去掉第 0 频率 bin）</li>
<li>**$2C$**：包含 <strong>$C$ 个幅度分量</strong>和 <strong>$C$ 个相位分量</strong></li>
</ul>
<h4 id="B-Neural-network-architecture"><a href="#B-Neural-network-architecture" class="headerlink" title="B. Neural network architecture"></a>B. Neural network architecture</h4><p>特征提取模块的输出被输入到<strong>神经网络</strong>，如<strong>图 1a</strong> 所示。在所提出的架构中，<strong>2D CNN 的多层卷积</strong>用于学习频谱图中的<strong>局部平移不变特征</strong>。</p>
<h5 id="CNN-结构"><a href="#CNN-结构" class="headerlink" title="CNN 结构"></a><strong>CNN 结构</strong></h5><ul>
<li>每个 CNN 层具有 <strong>$P$</strong> 个<strong>滤波器</strong>，其<strong>感受野</strong>的尺寸为 <strong>$3 \times 3 \times 2C$<strong>（与[25]相同），作用于</strong>时间-频率-通道轴</strong>，并使用 <strong>ReLU 激活函数</strong>。</li>
<li><strong>跨所有通道的滤波核</strong>使 CNN 能够学习<strong>跨通道特征</strong>，这对于声源定位至关重要。</li>
<li><strong>时间和频率维度的滤波核</strong>使 CNN 能够学习适用于 <strong>DOA 和 SED 任务的相关通道特征</strong>。</li>
<li>每个 CNN 层的输出经过 <strong>批归一化（Batch Normalization）</strong>[54] 进行标准化，并在<strong>频率轴</strong>上通过**最大池化（Max-Pooling, $M_Pi$）**降低维度，从而保持**时间序列长度 $T$ 不变**。</li>
</ul>
<h6 id="最终-CNN-层的输出"><a href="#最终-CNN-层的输出" class="headerlink" title="最终 CNN 层的输出"></a><strong>最终 CNN 层的输出</strong></h6><p>经过所有 CNN 层后，最终的输出维度为：</p>
<p>$T \times 2 \times P$</p>
<p>其中：</p>
<ul>
<li>**$T$**：时间帧数（保持不变）。</li>
<li><strong>$2$<strong>：频率维度在多个 CNN 层的最大池化后被减少到 2（详见</strong>第四部分 - IV-1</strong>）。</li>
<li>**$P$**：最终 CNN 层的滤波器数量。</li>
</ul>
<p>​	CNN 的输出激活进一步被<strong>重塑</strong>为$T$ 帧序列，每帧包含 $2P$ 维特征向量<strong>，并输入到</strong>双向 RNN 层，用于从 CNN 的输出中学习<strong>时间上下文信息</strong>。</p>
<h5 id="RNN-结构"><a href="#RNN-结构" class="headerlink" title="RNN 结构"></a><strong>RNN 结构</strong></h5><ul>
<li>每层 RNN 采用 <strong>$Q$ 个门控循环单元（GRU）</strong>，激活函数为 <strong>tanh</strong>。</li>
<li>RNN 的输出随后被分为<strong>两个并行的全连接（FC）层分支</strong>，分别用于 <strong>SED 和 DOA 估计</strong>。</li>
<li><strong>所有 FC 层在时间步长上共享权重</strong>。</li>
</ul>
<h5 id="全连接（FC）层的结构"><a href="#全连接（FC）层的结构" class="headerlink" title="全连接（FC）层的结构"></a><strong>全连接（FC）层的结构</strong></h5><ol>
<li><h6 id="第一层-FC（通用）："><a href="#第一层-FC（通用）：" class="headerlink" title="第一层 FC（通用）："></a><strong>第一层 FC（通用）</strong>：</h6><ul>
<li><strong>两条分支的第一层 FC 均包含 $R$ 个节点</strong>，激活函数为<strong>线性</strong>。</li>
</ul>
</li>
<li><h6 id="SED-分支（声事件检测）："><a href="#SED-分支（声事件检测）：" class="headerlink" title="SED 分支（声事件检测）："></a><strong>SED 分支（声事件检测）</strong>：</h6><ul>
<li>最后一层 FC 具有 <strong>$N$ 个节点</strong>（对应 <strong>$N$ 个声事件类别</strong>）。</li>
<li><strong>激活函数：sigmoid</strong>，允许<strong>多个类别同时处于活动状态</strong>。</li>
<li>输出范围：**[0, 1]**（每个类别的连续置信度值）。</li>
</ul>
</li>
<li><h6 id="DOA-分支（方向估计）："><a href="#DOA-分支（方向估计）：" class="headerlink" title="DOA 分支（方向估计）："></a><strong>DOA 分支（方向估计）</strong>：</h6><ul>
<li>最后一层 FC 具有 <strong>$3N$ 个节点</strong>，其中<strong>每个声事件类别由 3 个节点表示其在 $x, y, z$ 轴上的位置</strong>。</li>
<li><strong>激活函数：tanh</strong>，用于保持输出范围在 **[-1, 1]**。</li>
<li>由于 DOA 估计基于<strong>以原点为中心的单位球面</strong>，因此 DOA 预测的 $x, y, z$ 位置的值也应保持在 **[-1, 1]**。</li>
</ul>
</li>
</ol>
<h5 id="SELDnet-及决策机制"><a href="#SELDnet-及决策机制" class="headerlink" title="SELDnet 及决策机制"></a><strong>SELDnet 及决策机制</strong></h5><ul>
<li>该架构被称为 <strong>SELDnet</strong>。</li>
<li><strong>SED 输出</strong>范围为 **[0, 1]**，表示每个类别的置信度。</li>
<li><strong>DOA 输出</strong>范围为 **[-1, 1]**，表示每个声事件类别的三维位置。</li>
<li>当 <strong>SED 输出值超过 0.5</strong> 时，声事件被判定为<strong>活动状态</strong>，并<strong>选取相应的 DOA 估计</strong>（如图 1b 所示）。</li>
<li><strong>网络超参数</strong>基于<strong>交叉验证</strong>进行优化，具体细节详见 <strong>第三部分 - III-D1</strong>。</li>
</ul>
<h4 id="C-训练流程"><a href="#C-训练流程" class="headerlink" title="C. 训练流程"></a>C. 训练流程</h4><h5 id="目标值设定"><a href="#目标值设定" class="headerlink" title="目标值设定"></a><strong>目标值设定</strong></h5><p>在每个时间帧中，SELDnet 的目标值设定如下：</p>
<ul>
<li><strong>SED 分支</strong>：对于<strong>活动的声事件</strong>，目标值设为 <strong>1</strong>，对于<strong>非活动的事件</strong>，目标值设为 <strong>0</strong>。</li>
<li><strong>DOA 分支</strong>：对于<strong>活动的声事件</strong>，目标值为<strong>参考 DOA 的 $x, y, z$ 坐标</strong>；对于<strong>非活动的事件</strong>，目标值设为 **$x &#x3D; 0, y &#x3D; 0, z &#x3D; 0$**。</li>
</ul>
<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a><strong>损失函数</strong></h5><ul>
<li><strong>SED 预测</strong>使用<strong>二元交叉熵损失（Binary Cross-Entropy Loss）</strong>，用于衡量 SELDnet 预测的 SED 输出与真实声事件活动之间的差异。</li>
<li><strong>DOA 预测</strong>使用<strong>均方误差（MSE）损失</strong>，用于衡量 SELDnet 预测的 DOA 估计与参考 DOA 之间的差异。</li>
</ul>
<h5 id="为什么使用-MSE-作为-DOA-估计的损失函数？"><a href="#为什么使用-MSE-作为-DOA-估计的损失函数？" class="headerlink" title="为什么使用 MSE 作为 DOA 估计的损失函数？"></a><strong>为什么使用 MSE 作为 DOA 估计的损失函数？</strong></h5><ul>
<li>在 <strong>三维笛卡尔坐标系</strong>中，两个点 $(x_1, y_1, z_1)$ 和 $(x_2, y_2, z_2)$ 之间的<strong>欧几里得距离</strong>为： $\sqrt{\text{SE}}, \quad \text{其中 } \text{SE} &#x3D; (x_1 - x_2)^2 + (y_1 - y_2)^2 + (z_1 - z_2)^2$</li>
<li>对应的<strong>MSE 损失</strong>为： $\text{MSE} &#x3D; \frac{\text{SE}}{3}$</li>
<li>这表明，MSE 只是<strong>三维空间中欧几里得距离的缩放版本</strong>，因此最小化 MSE 就等同于<strong>减少 DOA 估计与真实 DOA 之间的距离</strong>。</li>
</ul>
<h5 id="笛卡尔坐标-vs-方位角-俯仰角"><a href="#笛卡尔坐标-vs-方位角-俯仰角" class="headerlink" title="笛卡尔坐标 vs. 方位角 &amp; 俯仰角"></a><strong>笛卡尔坐标 vs. 方位角 &amp; 俯仰角</strong></h5><ul>
<li>在 **完整方位角（full azimuth）和&#x2F;或完整俯仰角（full elevation）**预测任务中，使用**笛卡尔坐标比角度坐标（azimuth, elevation）更优**。</li>
<li>角度表示的连续性问题<ul>
<li>方位角在 $-180^\circ, 180^\circ$ 处是不连续的（存在边界问题）。</li>
<li>笛卡尔坐标是连续的，能够更好地被神经网络学习。</li>
</ul>
</li>
</ul>
<p>这一理论优势的实验验证将在 <strong>第三部分 - III-D</strong> 进一步讨论。</p>
<h5 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a><strong>训练过程</strong></h5><ul>
<li><p><strong>损失函数</strong>：SELDnet 采用 <strong>MSE（DOA）+ 二元交叉熵（SED）</strong> 的<strong>加权组合损失</strong>。</p>
</li>
<li><p><strong>优化器</strong>：使用 <strong>Adam</strong>（采用默认参数，参考[55]）。</p>
</li>
<li><p>训练轮数</p>
<p>：训练 </p>
<p>1000 轮（epochs）</p>
<p>，使用</p>
<p>提前停止（Early Stopping）</p>
<p> 避免过拟合：</p>
<ul>
<li>如果<strong>测试集的 SELD 评分（详见第三部分 - III-C）在 100 轮内没有提升</strong>，则停止训练。</li>
</ul>
</li>
</ul>
<h5 id="实现"><a href="#实现" class="headerlink" title="实现"></a><strong>实现</strong></h5><ul>
<li>该网络使用 <strong>Keras</strong> 库[56]，后端采用 <strong>TensorFlow</strong>[57]。</li>
</ul>
<h3 id="III-评估"><a href="#III-评估" class="headerlink" title="III. 评估"></a>III. 评估</h3><h4 id="A-SELDnet-评估数据集"><a href="#A-SELDnet-评估数据集" class="headerlink" title="A.SELDnet 评估数据集"></a>A.<strong>SELDnet 评估数据集</strong></h4><p>所提出的 SELDnet 在 <strong>七个数据集</strong> 上进行了评估，这些数据集的概述见 <strong>表 II</strong>。</p>
<ul>
<li><strong>四个数据集</strong> 使用 <strong>人工脉冲响应（IR）</strong> 进行合成，涵盖<strong>自由声场（无混响）**和**混响环境**，并采用</strong>环形阵列**或 <strong>Ambisonic 格式</strong> 进行虚拟录制。</li>
<li><strong>三个数据集</strong> 使用 <strong>真实环境中的脉冲响应</strong> 进行合成，采用<strong>球形阵列</strong>录制，并编码为 <strong>Ambisonic 格式</strong>。</li>
<li>所有数据集的声源均为<strong>静态点声源</strong>，并且<strong>每个声源都关联一个空间坐标</strong>。</li>
<li><strong>数据合成过程</strong>：在所有数据集中，<strong>将分离的声事件样本混合在不同的空间位置</strong>，以便生成<strong>参考事件的位置和活动时间</strong>，用于评估和训练方法。</li>
</ul>
<hr>
<h5 id="1-TUT-Sound-Events-2018-Ambisonic-Anechoic-and-Synthetic-Impulse-Response-ANSYN-数据集"><a href="#1-TUT-Sound-Events-2018-Ambisonic-Anechoic-and-Synthetic-Impulse-Response-ANSYN-数据集" class="headerlink" title="1) TUT Sound Events 2018 - Ambisonic, Anechoic and Synthetic Impulse Response (ANSYN) 数据集"></a><strong>1) TUT Sound Events 2018 - Ambisonic, Anechoic and Synthetic Impulse Response (ANSYN) 数据集</strong></h5><ul>
<li><p><strong>无混响环境（自由声场）</strong>，使用<strong>人工脉冲响应（IRs）</strong> 进行合成。</p>
</li>
<li><p>三个子集：</p>
<ul>
<li><strong>O1</strong>：无时间重叠声源</li>
<li><strong>O2</strong>：最多两个重叠声源</li>
<li><strong>O3</strong>：最多三个重叠声源</li>
</ul>
</li>
<li><p>三折交叉验证</p>
<p>（cross-validation），每折包含：</p>
<ul>
<li><strong>240 个训练样本</strong></li>
<li><strong>60 个测试样本</strong></li>
<li><strong>录音时长 30 秒</strong>，<strong>采样率 44100 Hz</strong></li>
<li><strong>FOA 格式</strong> 录音</li>
</ul>
</li>
<li><p><strong>数据来源</strong>：DCASE 2016 任务 2 数据集[58]，包含 <strong>11 类独立声事件</strong>（如<strong>语音、咳嗽、关门、翻书、电话铃声、键盘打字</strong>等）。</p>
</li>
<li><p>数据合成方式：</p>
<ul>
<li><strong>随机放置</strong>在空间网格（方位角和俯仰角分辨率为 <strong>10°</strong>）。</li>
<li><strong>两个重叠的声事件</strong> 至少相隔 <strong>10°</strong>。</li>
<li>俯仰角范围为 **[-60°, 60°)**。</li>
<li>声事件的<strong>距离范围为 1-10 m，步长 0.5 m</strong>。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="2-TUT-Sound-Events-2018-Ambisonic-Reverberant-and-Synthetic-Impulse-Response-RESYN-数据集"><a href="#2-TUT-Sound-Events-2018-Ambisonic-Reverberant-and-Synthetic-Impulse-Response-RESYN-数据集" class="headerlink" title="2) TUT Sound Events 2018 - Ambisonic, Reverberant and Synthetic Impulse Response (RESYN) 数据集"></a><strong>2) TUT Sound Events 2018 - Ambisonic, Reverberant and Synthetic Impulse Response (RESYN) 数据集</strong></h5><ul>
<li><strong>与 ANSYN 数据集相同</strong>，但声事件是<strong>在混响环境中合成</strong>的，使用<strong>图像源法（Image Source Method）[59]</strong> 计算房间中的声传播。</li>
<li>房间环境：<ul>
<li>训练房间（Room 1）尺寸：<strong>10 × 8 × 4 m</strong>。</li>
<li><strong>混响时间（RT60）</strong>：1.0s, 0.8s, 0.7s, 0.6s, 0.5s, 0.4s。</li>
</ul>
</li>
<li>泛化能力测试：<ul>
<li>额外创建两个测试房间：<ul>
<li><strong>Room 2</strong>：体积为 <strong>Room 1 的 80%<strong>（</strong>8 × 8 × 4 m</strong>）。</li>
<li><strong>Room 3</strong>：体积为 <strong>Room 1 的 125%<strong>（</strong>10 × 10 × 4 m</strong>）。</li>
</ul>
</li>
<li>Room 2 和 Room 3 的测试集：<ul>
<li>声事件及其空间位置<strong>与 Room 1 的测试集保持一致</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h5 id="3-TUT-Sound-Events-2018-Ambisonic-Reverberant-and-Real-life-Impulse-Response-REAL-数据集"><a href="#3-TUT-Sound-Events-2018-Ambisonic-Reverberant-and-Real-life-Impulse-Response-REAL-数据集" class="headerlink" title="3) TUT Sound Events 2018 - Ambisonic, Reverberant and Real-life Impulse Response (REAL) 数据集"></a><strong>3) TUT Sound Events 2018 - Ambisonic, Reverberant and Real-life Impulse Response (REAL) 数据集</strong></h5><ul>
<li><strong>目标</strong>：研究 SELDnet 在<strong>真实场景</strong>中的性能。</li>
<li>脉冲响应（IR）采集：<ul>
<li>采用 <strong>Eigenmike⁴ 球形麦克风阵列</strong> 采集真实环境的脉冲响应。</li>
<li>测量方法：<ul>
<li>使用 <strong>Genelec G Two 扬声器⁵</strong> 沿<strong>圆形轨迹</strong>连续播放<strong>最大长度序列（Maximum Length Sequence）</strong>。</li>
<li><strong>播放音量</strong>设定为<strong>高于环境噪声 30 dB</strong>。</li>
<li><strong>测量环境</strong>：大学走廊（如图 2 所示）。</li>
</ul>
</li>
</ul>
</li>
<li>数据合成：<ul>
<li>从 <strong>urbansound8k 数据集</strong>[62] 选取 <strong>10 类真实声事件</strong>（如汽车喇叭、狗叫、钻孔、警笛等）。</li>
<li>最终数据集：<ul>
<li><strong>300 个 30 秒的录音</strong>（240 个训练，60 个测试）。</li>
<li>采用 <strong>FOA 格式</strong> 进行转换。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h5 id="4-TUT-Sound-Events-2018-Ambisonic-Reverberant-and-Real-life-Impulse-Response-big-REALBIG-数据集"><a href="#4-TUT-Sound-Events-2018-Ambisonic-Reverberant-and-Real-life-Impulse-Response-big-REALBIG-数据集" class="headerlink" title="4) TUT Sound Events 2018 - Ambisonic, Reverberant and Real-life Impulse Response big (REALBIG) 数据集"></a><strong>4) TUT Sound Events 2018 - Ambisonic, Reverberant and Real-life Impulse Response big (REALBIG) 数据集</strong></h5><ul>
<li>研究数据集大小对 SELDnet 的影响。</li>
<li>在每个 REAL子集的基础上，扩展为：<ul>
<li><strong>750 份 30 秒录音</strong>（600 训练，150 测试）。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="5-TUT-Sound-Events-2018-Ambisonic-Reverberant-Real-life-Impulse-Response-and-Ambiance-big-REALBIGAMB-数据集"><a href="#5-TUT-Sound-Events-2018-Ambisonic-Reverberant-Real-life-Impulse-Response-and-Ambiance-big-REALBIGAMB-数据集" class="headerlink" title="5) TUT Sound Events 2018 - Ambisonic, Reverberant, Real-life Impulse Response and Ambiance big (REALBIGAMB) 数据集"></a><strong>5) TUT Sound Events 2018 - Ambisonic, Reverberant, Real-life Impulse Response and Ambiance big (REALBIGAMB) 数据集</strong></h5><ul>
<li><strong>模拟真实声场</strong>：录制 <strong>30 分钟环境声音</strong> 作为背景噪声。</li>
<li>不同信噪比（SNR）：<ul>
<li><strong>0, 10, 20 dB</strong> 级别的噪声，混合到 <strong>REALBIG</strong> 数据集中。</li>
</ul>
</li>
<li><strong>训练集与测试集的环境噪声分离</strong>。</li>
</ul>
<hr>
<h5 id="6-TUT-Sound-Events-2018-Circular-array-Anechoic-and-Synthetic-Impulse-Response-CANSYN-数据集"><a href="#6-TUT-Sound-Events-2018-Circular-array-Anechoic-and-Synthetic-Impulse-Response-CANSYN-数据集" class="headerlink" title="6) TUT Sound Events 2018 - Circular array, Anechoic and Synthetic Impulse Response (CANSYN) 数据集"></a><strong>6) TUT Sound Events 2018 - Circular array, Anechoic and Synthetic Impulse Response (CANSYN) 数据集</strong></h5><ul>
<li><p>研究 SELDnet 在<strong>通用麦克风阵列配置</strong>上的性能。</p>
</li>
<li><p>环形阵列</p>
<p>（直径 5 cm，8 个全向麦克风）：</p>
<ul>
<li>麦克风角度：<strong>0°, 45°, 90°, 135°, 180°, 225°, 270°, 315°</strong>。</li>
<li><strong>阵列平面</strong>与地面<strong>平行</strong>。</li>
</ul>
</li>
<li><p><strong>CANSYN 是 ANSYN 数据集的复制版本</strong>，唯一区别是麦克风阵列结构不同。</p>
</li>
</ul>
<hr>
<h5 id="7-TUT-Sound-Events-2018-Circular-array-Reverberant-and-Synthetic-Impulse-Response-CRESYN-数据集"><a href="#7-TUT-Sound-Events-2018-Circular-array-Reverberant-and-Synthetic-Impulse-Response-CRESYN-数据集" class="headerlink" title="7) TUT Sound Events 2018 - Circular array, Reverberant and Synthetic Impulse Response (CRESYN) 数据集"></a><strong>7) TUT Sound Events 2018 - Circular array, Reverberant and Synthetic Impulse Response (CRESYN) 数据集</strong></h5><ul>
<li><strong>CANSYN 数据集的混响版本</strong>，对应 <strong>RESYN（房间 1）</strong>。</li>
<li><strong>环形麦克风阵列</strong>位于房间<strong>中心</strong>，阵列平面与地面<strong>平行</strong>。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310214239203.png" alt="image-20250310214239203"></p>
<h4 id="B-Baseline-方法"><a href="#B-Baseline-方法" class="headerlink" title="B. Baseline 方法"></a>B. Baseline 方法</h4><p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310215351215.png" alt="image-20250310215351215"></p>
<h4 id="SELDnet-评估基线方法"><a href="#SELDnet-评估基线方法" class="headerlink" title="SELDnet 评估基线方法"></a><strong>SELDnet 评估基线方法</strong></h4><p>SELDnet 的性能与 <strong>六种不同的基线方法</strong> 进行了对比，如 <strong>表 III</strong> 所示：</p>
<ul>
<li><strong>2 种 SED 基线方法</strong>（单通道和多通道）</li>
<li><strong>3 种 DOA 估计基线方法</strong>（参数化方法和 DNN 方法）</li>
<li><strong>1 种 SELD 基线方法</strong></li>
</ul>
<hr>
<h5 id="1-SED-基线方法"><a href="#1-SED-基线方法" class="headerlink" title="1) SED 基线方法"></a><strong>1) SED 基线方法</strong></h5><p>SELDnet 的 <strong>SED 能力</strong> 与当前最先进的<strong>多通道 SED 方法</strong>[39] 进行了比较，该方法称为 <strong>MSEDnet</strong>：</p>
<ul>
<li><p>MSEDnet</p>
<p> 适用于 任意输入通道数，并在 DCASE 2017 真实场景 SED 任务[64]中获胜[38]：</p>
<ul>
<li><strong>单通道模式（SEDnet）</strong> 获得第一名。</li>
<li><strong>多通道模式</strong> 获得第二名（仅略逊于单通道模式）。</li>
</ul>
</li>
<li><p>输入：</p>
<ul>
<li>MSEDnet 原始实现[39] 采用 <strong>40 维对数 mel 频带能量</strong> 作为输入，并输出相同长度的<strong>声事件活动序列</strong>。</li>
<li><strong>在本论文中，为了与 SELDnet 进行一对一比较，MSEDnet 采用 STFT 频谱的相位和幅度分量作为输入</strong>。</li>
</ul>
</li>
<li><p>训练过程：</p>
<ul>
<li>训练 <strong>500 轮（epochs）</strong>。</li>
<li><strong>提前停止（Early Stopping）</strong>：如果 <strong>SED 评分（详见 III-C）在 100 轮内没有提升</strong>，则停止训练。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="2-DOA-估计基线方法"><a href="#2-DOA-估计基线方法" class="headerlink" title="2) DOA 估计基线方法"></a><strong>2) DOA 估计基线方法</strong></h5><p>SELDnet 的 <strong>DOA 估计能力</strong> 与 <strong>三种基线方法</strong> 进行了比较：</p>
<h6 id="i-MUSIC（参数化基线）"><a href="#i-MUSIC（参数化基线）" class="headerlink" title="i) MUSIC（参数化基线）"></a><strong>i) MUSIC（参数化基线）</strong></h6><ul>
<li><strong>MUSIC</strong>[46] 是一种<strong>高分辨率子空间方法</strong>，可以<strong>检测多个窄带声源的 DOA</strong>，适用于<strong>通用阵列设置</strong>。</li>
<li>方法：<ul>
<li>基于<strong>多通道频谱的空间协方差矩阵的子空间分解</strong>。</li>
<li><strong>宽带 DOA 估计</strong>：将 <strong>50 Hz - 8000 Hz 频段</strong> 的窄带空间协方差矩阵进行组合。</li>
<li>转向矢量（Steering Vector）：<ul>
<li><strong>CANSYN 和 CRESYN 数据集</strong>：使用<strong>均匀环形阵列（UCA）的转向矢量</strong>。</li>
<li><strong>其余 Ambisonic 数据集</strong>：使用<strong>真实球谐（SH）矢量</strong>。</li>
</ul>
</li>
</ul>
</li>
<li>关键限制：<ul>
<li>MUSIC 需要<strong>准确估计活动声源的数量</strong>。</li>
<li>本论文中，我们使用 <strong>数据集的真实参考声源数量</strong>，使 MUSIC 估计的 DOA 结果达到<strong>最优基准</strong>。</li>
</ul>
</li>
</ul>
<h6 id="ii-DOAnet（DNN-基线）"><a href="#ii-DOAnet（DNN-基线）" class="headerlink" title="ii) DOAnet（DNN 基线）"></a><strong>ii) DOAnet（DNN 基线）</strong></h6><ul>
<li><p><strong>DOAnet</strong>[25] 是当前唯一尝试在 <strong>3D 空间中估计多个重叠声源 DOA</strong> 的 DNN 方法，因此选为对比基线。</p>
</li>
<li><p>方法：</p>
<ul>
<li><p>采用 <strong>类似的 CRNN 架构</strong>，输入为<strong>多通道相位和幅度谱图</strong>。</p>
</li>
<li><p>DOA 估计采用 </p>
<p>多标签分类：</p>
<ul>
<li><strong>方向采样分辨率 10°</strong>（方位角和俯仰角）。</li>
<li><strong>输出为每个角度的声源活动概率</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h6 id="iii-AZInet（DNN-基线，仅估计方位角）"><a href="#iii-AZInet（DNN-基线，仅估计方位角）" class="headerlink" title="iii) AZInet（DNN 基线，仅估计方位角）"></a><strong>iii) AZInet（DNN 基线，仅估计方位角）</strong></h6><ul>
<li><strong>AZInet</strong>[18] 是另一种 DOA 估计方法，仅估计<strong>方位角（azimuth）</strong>。</li>
<li>方法：<ul>
<li>采用 CNN 结构，输入为 <strong>各通道的相位频谱</strong>。</li>
<li><strong>方位角范围 0° 至 180°，分辨率 5°，作为多标签分类任务</strong>。</li>
<li>仅使用 <strong>相位谱图</strong>，适用于 <strong>远场全向麦克风阵列</strong>。</li>
</ul>
</li>
<li>调整以匹配 SELDnet：<ul>
<li>为了与 SELDnet 直接比较，我们扩展 AZInet：<ul>
<li><strong>扩展到完整方位角（full azimuth, 10° 分辨率）</strong>。</li>
<li><strong>SELDnet 仅估计 x 和 y 坐标（称为 SELDnetazi）</strong>。</li>
</ul>
</li>
<li><strong>数据集选择</strong>：使用 <strong>CANSYN 和 CRESYN</strong>（均为环形阵列 + 全向麦克风）。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="3-SELD-基线方法（HIRnet）"><a href="#3-SELD-基线方法（HIRnet）" class="headerlink" title="3) SELD 基线方法（HIRnet）"></a><strong>3) SELD 基线方法（HIRnet）</strong></h5><p>SELDnet 的 <strong>联合 SED 和 DOA 估计能力</strong> 与 <strong>Hirvonen 方法（HIRnet）[20]</strong> 进行了比较：</p>
<ul>
<li><strong>HIRnet 采用 CNN 结构</strong>，输入为 <strong>各通道的对数频谱功率</strong>。</li>
<li>方法：<ul>
<li>仅适用于 <strong>环形阵列的全向麦克风</strong>，因此仅在 <strong>CANSYN 和 CRESYN 数据集</strong> 上进行比较。</li>
<li>输出：<ul>
<li>将每个声事件类别映射到 <strong>完整方位角的 8 个固定方向</strong>。</li>
<li>采用 <strong>多标签分类</strong>（仅适用于 <strong>语音和音乐</strong> 两类）。</li>
</ul>
</li>
</ul>
</li>
<li>调整以匹配 SELDnetazi：<ul>
<li><strong>扩展 HIRnet</strong>，使其在 <strong>10° 分辨率下估计 DOA</strong>，以适配我们的测试数据集。</li>
</ul>
</li>
</ul>
<h4 id="C-评估指标"><a href="#C-评估指标" class="headerlink" title="C. 评估指标"></a>C. 评估指标</h4><p>​	拟议的 SELDnet 使用 SED 和 DOA 估计的单个指标进行评估。对于 SED，我们使用标准的复音 SED 指标、Error Rate （ER） 和 F Score，以一秒为单位计算，如 [67， 68] 中建议的那样，没有重叠。分段结果是从分类器的帧级预测中获得的，如果声音事件在分段内的任何帧中处于活动状态，则认为声音事件在整个分段中处于活动状态。同样，我们从其逐帧注释中获得一秒参考片段的标签，并计算分段 ER 和 Fscores。从数学上讲，F 分数的计算方式如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250310220220465.png" alt="image-20250310220220465"></p>
<p>在第 $k$ 个 1 秒片段中，各个指标的定义如下：</p>
<ul>
<li><strong>真正例（True Positives, $TP(k)$）</strong>：指在<strong>参考数据</strong>和<strong>预测结果</strong>中都<strong>处于活动状态</strong>的声事件类别的数量。</li>
<li><strong>假正例（False Positives, $FP(k)$）</strong>：指在<strong>预测结果</strong>中处于<strong>活动状态</strong>，但在<strong>参考数据</strong>中<strong>未激活</strong>的声事件类别的数量。</li>
<li><strong>假负例（False Negatives, $FN(k)$）</strong>：指在<strong>预测结果</strong>中<strong>未激活</strong>，但在<strong>参考数据</strong>中处于<strong>活动状态</strong>的声事件类别的数量。</li>
</ul>
<p>Error Rate(ER) 指标计算为：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250311210441040.png" alt="image-20250311210441040"></p>
<p>在每个 1 秒片段 $k$ 中，各个统计量的定义如下：</p>
<ul>
<li><strong>$N(k)$<strong>：参考数据中</strong>活动的声事件类别总数</strong>。</li>
<li><strong>替换（Substitution, $S(k)$）</strong>：指<strong>检测到声事件但分配了错误的类别</strong>，计算方式是<strong>合并假负例（$FN(k)$）和假正例（$FP(k)$）</strong>，但不逐一匹配假正例与假负例的对应关系。</li>
<li><strong>插入（Insertion, $I(k)$）</strong>：如果仍然存在<strong>额外的假正例（FP）</strong>，则这些未匹配的假正例被计为插入错误。</li>
<li><strong>删除（Deletion, $D(k)$）</strong>：如果仍然存在<strong>额外的假负例（FN）</strong>，则这些未匹配的假负例被计为删除错误。</li>
</ul>
<p>这些统计量的数学表达式如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250311210558086.png" alt="image-20250311210558086"></p>
<p>​	<strong>声事件检测（SED）方法</strong> 的评估通常采用 <strong>F-score</strong> 和 <strong>错误率（ER）</strong> 这两个指标进行联合评估：</p>
<ul>
<li><strong>F-score</strong> 衡量<strong>预测结果与真实值的匹配程度</strong>，理想情况下 <strong>F-score &#x3D; 1</strong>（即 100%）。</li>
<li><strong>错误率（ER）</strong> 衡量<strong>预测错误的比例</strong>，理想情况下 <strong>ER &#x3D; 0</strong>。</li>
</ul>
<p>这些指标的计算细节可参考文献 **[67, 68]**。在 <strong>表格中，F-score 以百分比表示</strong>，而 <strong>ER 以数值表示</strong>。</p>
<p>预测的 <strong>DOA 估计</strong> $(x_E, y_E, z_E)$ 与数据集的 <strong>参考 DOA</strong> $(x_G, y_G, z_G)$ 进行比较，使用<strong>中心角</strong> $\sigma$ 作为评估指标。</p>
<p>中心角 $\sigma$ 的取值范围为 <strong>$0°, 180°$<strong>，其定义为</strong>在原点处</strong>由<strong>预测 DOA 向量</strong> 和 <strong>参考 DOA 向量</strong> 形成的夹角，计算公式如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250311211801258.png" alt="image-20250311211801258"></p>
<p>其中，$\Delta x &#x3D; x_G - x_E$，$\Delta y &#x3D; y_G - y_E$，$\Delta z &#x3D; z_G - z_E$。然后，整个数据集的 DOA 误差计算如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250311211907032.png" alt="image-20250311211907032"></p>
<p>其中，$D$ 是整个数据集中 DOA 估计的总数，$\sigma((x_G^d, y_G^d, z_G^d), (x_E^d, y_E^d, z_E^d))$ 是第 $d$ 个预测 DOA 与参考 DOA 之间的夹角。</p>
<p>此外，为了考虑预测的 DOA 数量与参考 DOA 数量不相等的时间帧，我们报告 <strong>帧召回率（frame recall）</strong>，计算方式如下：</p>
<p>$\frac{TP}{TP + FN} \times 100%$</p>
<p>其中：</p>
<ul>
<li><strong>真正例（TP）</strong>：预测 DOA 数量与参考 DOA 数量<strong>相等</strong>的时间帧总数。</li>
<li><strong>假负例（FN）</strong>：预测 DOA 数量与参考 DOA 数量<strong>不相等</strong>的时间帧总数。</li>
</ul>
<p>DOA 估计算法的评估同时使用 <strong>DOA 误差</strong> 和 <strong>帧召回率</strong> 进行联合测量，理想情况下：</p>
<ul>
<li><strong>帧召回率 &#x3D; 1</strong>（表格中以百分比表示）。</li>
<li><strong>DOA 误差 &#x3D; 0</strong>。</li>
</ul>
<p>在 SELDnet 训练过程中，我们基于 <strong>综合 SELD 评分（SELD score）</strong> 进行 <strong>提前停止（early stopping）</strong>，计算方式如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250311212311394.png" alt="image-20250311212311394"></p>
<p>理想的 SELD 方法应具有 <strong>SELD 评分（SELD score）&#x3D; 0</strong>。</p>
<p>在所提出的方法中，<strong>定位性能依赖于检测性能</strong>，这一关系通过 <strong>DOA 估计的帧召回率（frame recall）</strong> 进行量化。因此，<strong>SELD 评分不仅包含 SED 评估指标，还包括帧召回率</strong>，这使得 <strong>SELD 评分在衡量整体性能时对 SED 的权重高于 DOA</strong>。</p>
<h4 id="D-实验"><a href="#D-实验" class="headerlink" title="D. 实验"></a>D. 实验</h4><h5 id="SELDnet-评估维度"><a href="#SELDnet-评估维度" class="headerlink" title="SELDnet 评估维度"></a><strong>SELDnet 评估维度</strong></h5><p>为了全面理解 SELDnet 的潜力和局限性，我们在不同维度上对其进行了评估，并在不同数据集上进行了实验。具体实验如下：</p>
<hr>
<h6 id="1-SELDnet-结构和模型参数调整"><a href="#1-SELDnet-结构和模型参数调整" class="headerlink" title="1) SELDnet 结构和模型参数调整"></a><strong>1) SELDnet 结构和模型参数调整</strong></h6><ul>
<li>在 <strong>ANSYN O2</strong> 数据集上测试不同的 <strong>CNN、RNN 和 FC 层的组合架构</strong>，帧长设定为 $M &#x3D; 1024$（23.2 ms）。</li>
<li>超参数优化：<ul>
<li><strong>CNN、RNN 和 FC 层的数量</strong>：0 至 4 层</li>
<li><strong>每层节点数</strong>：${16, 32, 64, 128, 256, 512}$</li>
<li><strong>输入序列长度</strong>：${32, 64, 128, 256, 512}$</li>
<li><strong>DOA 和 SED 分支损失权重</strong>：${1, 5, 50, 500}$</li>
<li>正则化：<ul>
<li><strong>Dropout</strong>：${0, 0.1, 0.2, 0.3, 0.4, 0.5}$</li>
<li><strong>L1&#x2F;L2 权重衰减</strong>：${0, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}, 10^{-7}}$</li>
</ul>
</li>
<li><strong>CNN 最大池化（max-pooling）</strong>：${2, 4, 6, 8, 16}$</li>
</ul>
</li>
<li><strong>最优参数选择</strong>：基于 <strong>数据集三折交叉验证的 SELD 评分最低值</strong> 进行选择。</li>
<li>输入特征参数 $M$ 的调整：<ul>
<li>取值范围：${512, 1024, 2048}$。</li>
<li><strong>同时调整序列长度</strong>，以保持输入音频长度恒定（1.49 秒）。</li>
<li>进行 <strong>微调（fine-tuning）</strong> 以选择最佳参数，并在其他数据集上重复优化。</li>
</ul>
</li>
</ul>
<hr>
<h6 id="2-SELDnet-输出格式选择"><a href="#2-SELDnet-输出格式选择" class="headerlink" title="2) SELDnet 输出格式选择"></a><strong>2) SELDnet 输出格式选择</strong></h6><ul>
<li>SED 输出格式：<ul>
<li>文献中 <strong>复调 SED（polyphonic SED）</strong> 的标准格式是 <strong>逐帧二进制分类（frame-wise binary classification）</strong> [31–34]。</li>
</ul>
</li>
<li>DOA 输出格式：<ul>
<li>两种可能的 DOA 回归格式：<ol>
<li>直接预测 <strong>方位角（azimuth）和俯仰角（elevation）</strong>。</li>
<li>预测 <strong>DOA 在单位球面上的 $x, y, z$ 笛卡尔坐标</strong>。</li>
</ol>
</li>
<li>在 SELDnet 上测试 <strong>两种格式</strong>，调整 <strong>DOA 分支损失权重 ${1, 5, 50, 500}$</strong> 以选择最佳格式。</li>
</ul>
</li>
<li>默认 DOA 目标值（无声事件时的 DOA）：<ul>
<li><strong>角度格式</strong>：设为 <strong>$180^\circ$（方位角），$60^\circ$（俯仰角）</strong>，这些角度<strong>不在数据集中的真实 DOA 范围内</strong>。</li>
<li><strong>笛卡尔格式</strong>：设为 **$x &#x3D; 0, y &#x3D; 0, z &#x3D; 0$**，确保与所有可能 DOA 位置等距。</li>
</ul>
</li>
</ul>
<hr>
<h6 id="3-连续-DOA-估计及对未见-DOA-的泛化能力"><a href="#3-连续-DOA-估计及对未见-DOA-的泛化能力" class="headerlink" title="3) 连续 DOA 估计及对未见 DOA 的泛化能力"></a><strong>3) 连续 DOA 估计及对未见 DOA 的泛化能力</strong></h6><ul>
<li>回归模型 vs. 分类模型：<ul>
<li><strong>回归模型可在连续空间中预测 DOA</strong>，不局限于固定角度集。</li>
</ul>
</li>
<li>实验设计：<ul>
<li>训练集：<strong>DOA 放置在 10° 角度网格</strong>（方位角 &amp; 俯仰角）。</li>
<li>测试集：<strong>角度网格沿方位角 &amp; 俯仰角偏移 5°</strong>，保证测试数据的 DOA 值<strong>未出现在训练集中</strong>。</li>
<li><strong>如果 SELDnet 能正确预测未见过的 DOA 值，证明其具备泛化能力</strong>。</li>
</ul>
</li>
</ul>
<hr>
<h6 id="4-不匹配混响数据集上的性能"><a href="#4-不匹配混响数据集上的性能" class="headerlink" title="4) 不匹配混响数据集上的性能"></a><strong>4) 不匹配混响数据集上的性能</strong></h6><ul>
<li><p>参数化 DOA 估计（如 MUSIC）对混响敏感</p>
<p> [48]，需要评估 SELDnet 在混响环境下的表现：</p>
<ul>
<li>测试数据集：<ul>
<li><strong>模拟混响数据集</strong>：RESYN</li>
<li><strong>真实混响数据集</strong>：REAL, REALBIG, REALBIGAMB</li>
</ul>
</li>
<li><strong>对比基线</strong>：MUSIC</li>
</ul>
</li>
<li><p>混响条件下的 DNN 泛化能力问题：</p>
<ul>
<li>DNN 在训练数据与测试数据来自不同域时表现较差：<ul>
<li>例如，在无混响数据集上训练的 DNN 在混响测试数据上的表现会很差。</li>
</ul>
</li>
<li>解决方案：<ul>
<li><strong>在类似混响条件的数据集上训练 DNN</strong>（但难以涵盖所有房间尺寸、材料分布和混响时间）。</li>
</ul>
</li>
<li>研究 SELDnet 在中等混响失配情况下的鲁棒性：<ul>
<li>训练集：<strong>RESYN（房间 1）</strong>。</li>
<li>测试集：<strong>RESYN（房间 2 &amp; 3）</strong>（房间尺寸和混响时间不同，详见 III-A2）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h6 id="5-训练数据集大小对-SELDnet-性能的影响"><a href="#5-训练数据集大小对-SELDnet-性能的影响" class="headerlink" title="5) 训练数据集大小对 SELDnet 性能的影响"></a><strong>5) 训练数据集大小对 SELDnet 性能的影响</strong></h6><ul>
<li>选择 REAL 和 REALBIG进行对比：<ul>
<li><strong>两者内容相似，但数据集大小不同</strong>。</li>
<li>研究数据集规模对 SELD 任务的影响。</li>
</ul>
</li>
</ul>
<hr>
<h6 id="6-不同信噪比（SNR）下的-SELDnet-性能"><a href="#6-不同信噪比（SNR）下的-SELDnet-性能" class="headerlink" title="6) 不同信噪比（SNR）下的 SELDnet 性能"></a><strong>6) 不同信噪比（SNR）下的 SELDnet 性能</strong></h6><ul>
<li>采用 <strong>REALBIGAMB 数据集</strong> 研究 <strong>不同 SNR（0, 10, 20 dB）</strong> 对 SELDnet 识别声事件的影响。</li>
</ul>
<hr>
<h6 id="7-SELDnet-在不同阵列结构上的通用性"><a href="#7-SELDnet-在不同阵列结构上的通用性" class="headerlink" title="7) SELDnet 在不同阵列结构上的通用性"></a><strong>7) SELDnet 在不同阵列结构上的通用性</strong></h6><ul>
<li>SELDnet 适用于任意阵列结构，只要训练集和测试集的阵列配置保持一致：<ul>
<li><strong>阵列配置不变</strong>（麦克风响应、麦克风间距、通道数相同）时，SELDnet 仍然有效。</li>
<li><strong>阵列配置变化</strong>时，需要重新训练 SELDnet 以适应新配置。</li>
</ul>
</li>
</ul>
<p>为了证明 <strong>SELDnet 适用于任何阵列配置</strong>，而不仅仅依赖于 <strong>Ambisonics 格式</strong>，我们在 <strong>环形阵列（circular array）</strong> 上对其进行了评估。</p>
<ul>
<li><strong>环形阵列 vs. Ambisonics</strong>：<ul>
<li><strong>麦克风数量不同</strong>。</li>
<li><strong>麦克风位于同一平面</strong>（不像 Ambisonics 具有完整 3D 空间的分布）。</li>
<li><strong>麦克风为全向性（Omnidirectional Response）</strong>。</li>
</ul>
</li>
<li><strong>基线方法对比</strong>：<ul>
<li>选择 适用于环形阵列的数据集的基线方法：<ul>
<li><strong>SEDnet</strong>（单通道 SED）</li>
<li><strong>MSEDnet</strong>（多通道 SED）</li>
<li><strong>HIRnet</strong>（仅估计方位角 DOA）</li>
<li><strong>AZInet</strong>（仅估计方位角 DOA）</li>
</ul>
</li>
<li>由于 <strong>HIRnet 和 AZInet 仅估计方位角（azimuth）</strong>，因此与 <strong>SELDnet-azi</strong> 版本进行比较。</li>
<li>额外报告 <strong>SELDnet 在 $x, y, z$ 轴上的 DOA 估计</strong>（在 <strong>CANSYN 和 CRESYN</strong> 数据集上）。</li>
</ul>
</li>
</ul>
<hr>
<h6 id="训练集与测试集的差异"><a href="#训练集与测试集的差异" class="headerlink" title="训练集与测试集的差异"></a><strong>训练集与测试集的差异</strong></h6><ul>
<li><p><strong>所有实验</strong> 的训练集与测试集的唯一区别是 <strong>声音样本的互斥性</strong>（训练和测试使用不同的音频片段）。</p>
</li>
<li><p>除 <strong>实验 III-D3 之外</strong>：</p>
<ul>
<li><p>训练和测试集中的 空间位置相同（方位角 &amp; 俯仰角 10° 分辨率，共468 个空间位置）。</p>
<ul>
<li>计算方式：$36$ 个方位角 $\times$ $13$ 个俯仰角 &#x3D; <strong>468 个空间点</strong>。</li>
</ul>
</li>
<li><p>声源的距离是一个额外变量：</p>
<ul>
<li><p>自由声场（无混响）场景：</p>
<ul>
<li>声源的距离可在 <strong>1 - 10 m</strong> 之间，以 <strong>0.5 m 步长</strong>变化。</li>
<li>共有 <strong>19 种距离位置</strong>。</li>
</ul>
</li>
<li><p>总空间位置数量：</p>
<p>468×19&#x3D;8892468 \times 19 &#x3D; 8892</p>
<ul>
<li>这些空间位置<strong>被粗略地归为 468 个主要位置</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>实验 III-D3 的特殊性</strong>：</p>
<ul>
<li><strong>测试集的声音样本和空间位置</strong>（方位角 &amp; 俯仰角）<strong>都与训练集不同</strong>。</li>
<li>进一步增加了 SELDnet 泛化能力的测试难度。</li>
</ul>
</li>
</ul>
<h3 id="IV-结果和讨论"><a href="#IV-结果和讨论" class="headerlink" title="IV. 结果和讨论"></a>IV. 结果和讨论</h3><p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250311213151549.png" alt="image-20250311213151549"></p>
<h4 id="1-SELDnet-结构和模型参数调整-1"><a href="#1-SELDnet-结构和模型参数调整-1" class="headerlink" title="1) SELDnet 结构和模型参数调整"></a><strong>1) SELDnet 结构和模型参数调整</strong></h4><ul>
<li><strong>不同 CNN、RNN、CRNN 配置的 SELD 评分</strong> 如 <strong>图 3</strong> 所示。</li>
<li>模型结构比较：<ul>
<li><strong>CNN 仅包含 CNN 和 FC 层</strong>（无 RNN）。</li>
<li><strong>RNN 仅包含 RNN 层</strong>（无 CNN）。</li>
<li><strong>CRNN 结合 CNN、RNN 和 FC 层</strong>。</li>
</ul>
</li>
<li><strong>实验数据集</strong>：<strong>ANSYN O2</strong>。</li>
<li>最佳架构：<ul>
<li><strong>CNN</strong>：3 层，每层 <strong>64 个节点</strong>（$P$ in 图 1a）。</li>
<li><strong>GRU</strong>：2 层，每层 <strong>128 个节点</strong>（$Q$ in 图 1a）。</li>
<li><strong>FC 层</strong>：1 层 <strong>128 个节点</strong>（$R$ in 图 1a）。</li>
<li><strong>CNN 频率方向最大池化</strong>：$(8,8,2)$。</li>
<li><strong>总参数数目</strong>：约 <strong>513,000</strong>。</li>
</ul>
</li>
<li>正则化影响：<ul>
<li><strong>最佳性能时不使用正则化</strong>（无 Dropout、L1 或 L2 正则化）。</li>
</ul>
</li>
<li>最佳帧长度与序列长度：<ul>
<li>帧长 $M &#x3D; 512$，序列长度 <strong>256</strong> 帧（图 4）。</li>
<li>固定帧长 $M &#x3D; 512$，最佳序列长度 <strong>512</strong> 帧（2.97 s）。</li>
<li><strong>序列长度过长受硬件限制</strong>。</li>
</ul>
</li>
<li>最佳输出权重：<ul>
<li><strong>DOA 输出权重 50 倍于 SED 输出</strong>（图 5）。</li>
</ul>
</li>
<li>不同数据集上的微调：<ul>
<li><strong>RESYN 数据集</strong>：最佳参数调整为 <strong>序列长度 256</strong>（总参数数不变）。</li>
<li><strong>CANSYN、CRESYN</strong>：最佳参数与 <strong>ANSYN</strong> 相同。</li>
<li><strong>REAL、REALBIG、REALBIGAMB</strong>：最佳参数与 <strong>ANSYN</strong> 相同。</li>
</ul>
</li>
</ul>
<hr>
<h4 id="2-SELDnet-输出格式选择-1"><a href="#2-SELDnet-输出格式选择-1" class="headerlink" title="2) SELDnet 输出格式选择"></a><strong>2) SELDnet 输出格式选择</strong></h4><ul>
<li>DOA 输出格式对比（图 6）：<ul>
<li><strong>笛卡尔坐标（$x, y, z$）</strong> 比 <strong>方位角&#x2F;俯仰角</strong> 表现更佳。</li>
<li><strong>角度制 DOA 由于边界不连续（例如 $-180^\circ, 180^\circ$）降低了性能</strong>。</li>
</ul>
</li>
</ul>
<hr>
<h4 id="3-连续-DOA-估计及对未见-DOA-的泛化能力-1"><a href="#3-连续-DOA-估计及对未见-DOA-的泛化能力-1" class="headerlink" title="3) 连续 DOA 估计及对未见 DOA 的泛化能力"></a><strong>3) 连续 DOA 估计及对未见 DOA 的泛化能力</strong></h4><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250311213519407.png" alt="image-20250311213519407"  />

<ul>
<li><p>输入 &amp; 输出可视化（图 7）：</p>
<ul>
<li><strong>训练数据集</strong>：ANSYN O1 &amp; O2。</li>
<li><strong>测试数据集</strong>：含 <strong>1000 帧</strong> 的测试序列。</li>
</ul>
</li>
<li><p>测试情境：</p>
<ul>
<li><p>ANSYN O1：</p>
<ul>
<li>SED &amp; DOA 预测几乎完美。</li>
<li>未见 DOA（标记为“×”）仍能准确预测，证明回归模型可学习<strong>连续 DOA</strong>。</li>
</ul>
</li>
<li><p>ANSYN O2：</p>
<ul>
<li><strong>SED 预测准确</strong>。</li>
<li>DOA 预测围绕参考均值波动：<ul>
<li>声事件具有固有的振幅变化 &amp; 短暂静音，导致 DOA 估计波动（图 7b）。</li>
</ul>
</li>
<li><strong>整体来看，SELDnet 能够同时识别、定位和跟踪多个重叠声事件</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="4-不匹配混响数据集上的性能-1"><a href="#4-不匹配混响数据集上的性能-1" class="headerlink" title="4) 不匹配混响数据集上的性能"></a><strong>4) 不匹配混响数据集上的性能</strong></h4><p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250311213619140.png" alt="image-20250311213619140"></p>
<ul>
<li><strong>RESYN（房间 1）实验结果（表 IV）</strong>：<ul>
<li><strong>MUSIC 在混响环境下表现较差</strong>。</li>
<li><strong>SELDnet 的帧召回率远高于 DOAnet</strong>。</li>
<li><strong>DOAnet 的 DOA 误差低于 SELDnet</strong>。</li>
<li><strong>SELDnet 的 SED 评分可与 MSEDnet 相媲美</strong>。</li>
</ul>
</li>
<li><strong>跨房间泛化能力测试</strong>：<ul>
<li><strong>训练：RESYN 房间 1</strong>。</li>
<li>测试：RESYN 房间 2 &amp; 3（体积 &amp; 混响时间不同）：<ul>
<li><strong>SELDnet 依然保持较高帧召回率</strong>。</li>
<li><strong>DOAnet 仍然具有较低 DOA 误差</strong>。</li>
<li><strong>MUSIC 依然表现最差</strong>。</li>
<li><strong>SELDnet 的 SED 评分仍可与 MSEDnet 相比</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><strong>结论</strong>：<ul>
<li><strong>SELDnet 对混响更具鲁棒性</strong>。</li>
<li><strong>能够适应中等程度的混响失配房间</strong>。</li>
</ul>
</li>
<li><strong>混淆矩阵可视化（图 8）</strong>：<ul>
<li>例如，图 8c：<ul>
<li>SELDnet <strong>76% 的时间帧能正确估计 2 个声源</strong>（TP%）。</li>
</ul>
</li>
<li><strong>帧召回率随着声源数量的增加而下降</strong>，在混响条件下降幅更大。</li>
<li><strong>但 SELDnet 的帧召回率仍明显优于 DOAnet</strong>，特别是在<strong>高重叠声事件 &amp; 混响环境</strong>中。</li>
<li><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250311213658113.png" alt="image-20250311213658113"></li>
</ul>
</li>
</ul>
<hr>
<h4 id="5-训练数据集大小对-SELDnet-性能的影响-1"><a href="#5-训练数据集大小对-SELDnet-性能的影响-1" class="headerlink" title="5) 训练数据集大小对 SELDnet 性能的影响"></a><strong>5) 训练数据集大小对 SELDnet 性能的影响</strong></h4><ul>
<li>REAL 数据集（表 V）：<ul>
<li><strong>SELDnet 在 REAL 上的整体性能下降</strong>（相较于 ANSYN &amp; RESYN）。</li>
<li><strong>MSEDnet 的 SED 评分高于 SELDnet</strong>（符合其他 SED 研究[37]）。</li>
<li>DOA 指标：<ul>
<li><strong>SELDnet 帧召回率远高于 DOAnet</strong>。</li>
<li><strong>DOAnet 的 DOA 误差低于 SELDnet</strong>。</li>
<li><strong>MUSIC 依然表现最差</strong>。</li>
</ul>
</li>
</ul>
</li>
<li>REALBIG 数据集：<ul>
<li><strong>SELDnet 性能有所提升</strong>。</li>
<li><strong>REALBIG &gt; REAL</strong>，表明<strong>真实 IR 数据集比合成 IR 数据集更复杂</strong>。</li>
<li><strong>更大的真实数据集有助于网络学习</strong>。</li>
</ul>
</li>
</ul>
<hr>
<h4 id="6-不同信噪比（SNR）下的-SELDnet-性能-1"><a href="#6-不同信噪比（SNR）下的-SELDnet-性能-1" class="headerlink" title="6) 不同信噪比（SNR）下的 SELDnet 性能"></a><strong>6) 不同信噪比（SNR）下的 SELDnet 性能</strong></h4><p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250311213717712.png" alt="image-20250311213717712"></p>
<ul>
<li>REALBIGAMB 数据集（表 V）：<ul>
<li>10 &amp; 20 dB SNR：<ul>
<li><strong>SELDnet 在 O1 子集的 SED 评分与 REALBIG（无背景噪声）相当</strong>。</li>
<li><strong>O2 &amp; O3 子集的性能略有下降</strong>。</li>
</ul>
</li>
<li>0 dB SNR：<ul>
<li><strong>性能下降明显</strong>。</li>
</ul>
</li>
<li>DOA 指标：<ul>
<li><strong>SELDnet 的 DOA 误差低于 MUSIC，但高于 DOAnet</strong>。</li>
<li><strong>SELDnet 的帧召回率远高于 DOAnet</strong>。</li>
</ul>
</li>
</ul>
</li>
<li>结论：<ul>
<li><strong>更复杂的声学场景需要更大的数据集以提高 SELDnet 适应性</strong>。</li>
</ul>
</li>
</ul>
<hr>
<h4 id="7-SELDnet-在不同阵列结构上的通用性-1"><a href="#7-SELDnet-在不同阵列结构上的通用性-1" class="headerlink" title="7) SELDnet 在不同阵列结构上的通用性"></a><strong>7) SELDnet 在不同阵列结构上的通用性</strong></h4><p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250311213740635.png" alt="image-20250311213740635"></p>
<ul>
<li>环形阵列实验结果（表 VI）：<ul>
<li><strong>SELDnet-azi 在 CRESYN 的 SED 评分优于 MSEDnet</strong>（所有子集）。</li>
<li><strong>MSEDnet 在 CANSYN O2 &amp; O3 子集上表现更佳</strong>。</li>
<li>DOA 指标：<ul>
<li><strong>SELDnet-azi 的帧召回率高于 AZInet（除 CANSYN O1 以外）</strong>。</li>
<li><strong>AZInet 的 DOA 误差低于 SELDnet-azi</strong>。</li>
</ul>
</li>
<li>SELDnet vs. SELDnet-azi：<ul>
<li><strong>两者帧召回率相近</strong>。</li>
<li><strong>SELDnet-azi 的 DOA 误差低于 SELDnet</strong>。</li>
<li><strong>使用环形阵列估计 3D DOA（x, y, z）更具挑战性</strong>。</li>
</ul>
</li>
</ul>
</li>
<li>结论：<ul>
<li><strong>SELDnet 适用于不同阵列结构（Ambisonic &amp; 环形阵列）</strong>。</li>
<li><strong>利用 SED 输出作为 DOA 置信度度量，提高帧召回率</strong>。</li>
<li><strong>回归 DOA 方法的 DOA 误差高于分类 DOA 方法</strong>[18, 25]。</li>
<li><strong>未来工作计划研究 DOA 估计的输入特征与连续 DOA 映射关系</strong>。</li>
<li><strong>分类方法 vs. 回归方法的选择取决于帧召回率、DOA 误差、DOA 分辨率、训练数据量及泛化能力</strong>。</li>
</ul>
</li>
</ul>
<h3 id="V-结论"><a href="#V-结论" class="headerlink" title="V. 结论"></a>V. 结论</h3><p>在本文中，我们提出了一种 <strong>卷积-递归神经网络（SELDnet）</strong>，用于<strong>同时识别、定位和跟踪声事件的时间活动性</strong>。</p>
<ul>
<li><strong>定位任务</strong> 通过 <strong>在麦克风周围的单位球面上估计到达方向（DOA）</strong> 来实现，并采用 <strong>3D 笛卡尔坐标（$x, y, z$）</strong> 进行表示。</li>
<li>在 <strong>SELDnet 中，每个声事件类别的输出与三个回归器绑定</strong>，用于估计相应的笛卡尔坐标。</li>
<li>回归方法的优势：<ul>
<li>使 <strong>DOA 估计在连续空间中进行</strong>。</li>
<li><strong>能够准确预测未见过的 DOA 角度</strong>。</li>
</ul>
</li>
<li>局限性：<ul>
<li>由于<strong>每个声事件类别仅估计一个 DOA</strong>，因此<strong>无法识别同类别的多个重叠实例</strong>。</li>
<li><strong>未来工作</strong> 将专注于解决此问题。</li>
</ul>
</li>
</ul>
<p>此外，<strong>使用 SED 输出作为 DOA 估计的置信度度量</strong>，使得 SELD 任务能<strong>扩展最先进的 SED 性能</strong>，从而 <strong>提高 DOA 召回率</strong>。</p>
<p>在 <strong>DOA 误差方面</strong>：</p>
<ul>
<li><strong>基于分类的 DOA 估计方法</strong> 召回率较低，但<strong>DOA 误差较小</strong>。</li>
<li><strong>基于回归的 SELDnet</strong> 召回率更高，但 <strong>DOA 误差略高</strong>。</li>
</ul>
<p><strong>输入特征选择</strong>：</p>
<ul>
<li>SELDnet <strong>使用相位和幅度谱图作为输入特征</strong>。</li>
<li>由于该特征<strong>不依赖于特定方法或阵列结构</strong>，使得 SELDnet <strong>具有通用性</strong>，可以轻松扩展到不同的阵列结构。</li>
<li>实验验证：<ul>
<li>SELDnet 在 <strong>Ambisonic 阵列</strong> 和 <strong>环形阵列</strong> 数据集上均表现良好。</li>
</ul>
</li>
</ul>
<p><strong>系统鲁棒性</strong>：</p>
<ul>
<li><strong>SELDnet 对混响、低信噪比（SNR）场景以及未见房间（具有相似房间大小）的泛化能力较强</strong>。</li>
</ul>
<p><strong>未来改进方向</strong>：</p>
<ul>
<li>在 <strong>基于真实环境脉冲响应（IR）合成的数据集</strong> 上，SELDnet 的整体性能较 <strong>人工 IR 数据集</strong> 略有下降。</li>
<li>这表明：<ul>
<li><strong>需要更大规模的真实场景训练数据</strong> 以提高模型泛化能力。</li>
<li><strong>更强的分类器</strong> 可能有助于提升性能，这也是未来研究的重点方向。</li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Gavin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/03/03/%E6%AF%94%E8%B5%9B%E7%9B%B8%E5%85%B3%EF%BC%9ADCASE%202019%20Task3%20SELD/">http://example.com/2025/03/03/%E6%AF%94%E8%B5%9B%E7%9B%B8%E5%85%B3%EF%BC%9ADCASE%202019%20Task3%20SELD/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Gavin</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/DCASE/">DCASE</a></div><div class="post_share"><div class="social-share" data-image="https://i.ibb.co/ZRtszJys/image.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/12/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AA%20Hybrid%20DSP:Deep%20Learning%20Approach%20to%20%20Real-Time%20Full-Band%20Speech%20Enhancement/" title="文献阅读: A Hybrid DSP/Deep Learning Approach to  Real-Time Full-Band Speech Enhancement"><img class="cover" src="https://i.ibb.co/VYX8yFsD/image.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">文献阅读: A Hybrid DSP/Deep Learning Approach to  Real-Time Full-Band Speech Enhancement</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/05/06/%20%20%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-%20Real-time%20implementation%20and%20explainable%20AI%20analysis%20of%20delayless%20CNN-based%20selective%20fixed-filter%20active%20noise%20control/" title="文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control"><img class="cover" src="https://i.ibb.co/rk6B9g0/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control</div></div></a></div><div><a href="/2024/05/06/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Transferable%20Latent%20of%20CNN-Based%20Selective%20Fixed-Filter%20Active%20Noise%20Control/" title="文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control"><img class="cover" src="https://i.ibb.co/xSTFTsh/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control</div></div></a></div><div><a href="/2024/04/23/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AA%20Hybrid%20SFANC-FxNLMS%20Algorithm%20for%20Active%20Noise%20Control%20based%20on%20Deep%20Learning/" title="文献阅读: A Hybrid SFANC-FxNLMS Algorithm for Active Noise Control based on Deep Learning"><img class="cover" src="https://i.ibb.co/5GmwJBS/image.png	" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-23</div><div class="title">文献阅读: A Hybrid SFANC-FxNLMS Algorithm for Active Noise Control based on Deep Learning</div></div></a></div><div><a href="/2024/05/05/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADEEP%20GENERATIVE%20FIXED-FILTER%20ACTIVE%20NOISE%20CONTROL/" title="文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL"><img class="cover" src="https://i.ibb.co/LSLjfkw/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-05</div><div class="title">文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL</div></div></a></div><div><a href="/2024/08/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADeep%20root%20music%20algorithm%20for%20data-driven%20DoA%20estimation/" title="文献阅读: SubspaceNet: Deep root music algorithm for data-driven DoA estimation"><img class="cover" src="https://i.ibb.co/8xsnM1g/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-15</div><div class="title">文献阅读: SubspaceNet: Deep root music algorithm for data-driven DoA estimation</div></div></a></div><div><a href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners"><img class="cover" src="https://i.ibb.co/gD7cJGh/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-17</div><div class="title">文献阅读: Masked Autoencoders Are Scalable Vision Learners</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.ibb.co/0fP2mb0/image.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Gavin</div><div class="author-info__description">我的过去常常追赶着我</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/alexandergwm"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/alexandergwm" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:wenmiaogao@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">May you stay young forever, do the thing in your own zone.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%AF%94%E8%B5%9B%E7%9B%B8%E5%85%B3"><span class="toc-number">1.</span> <span class="toc-text">1. 比赛相关</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%8F%8F%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 描述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E9%9F%B3%E9%A2%91%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 音频数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E8%AE%B0%E5%BD%95%E8%BF%87%E7%A8%8B"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 记录过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E5%BD%95%E5%88%B6%E6%A0%BC%E5%BC%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E8%A7%84%E8%8C%83"><span class="toc-number">1.4.</span> <span class="toc-text">1.4 录制格式和数据集规范</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-%E5%8F%82%E8%80%83labels"><span class="toc-number">1.5.</span> <span class="toc-text">1.5 参考labels</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-%E4%BB%BB%E5%8A%A1%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.6.</span> <span class="toc-text">1.6 任务设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-7-%E8%AE%AD%E7%BB%83%E9%9B%86"><span class="toc-number">1.7.</span> <span class="toc-text">1.7 训练集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-8-%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="toc-number">1.8.</span> <span class="toc-text">1.8 验证集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-9-%E4%BB%BB%E5%8A%A1%E8%A7%84%E5%88%99"><span class="toc-number">1.9.</span> <span class="toc-text">1.9 任务规则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-10-%E6%8F%90%E4%BA%A4"><span class="toc-number">1.10.</span> <span class="toc-text">1.10 提交</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-11-%E8%AF%84%E4%BC%B0"><span class="toc-number">1.11.</span> <span class="toc-text">1.11 评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-12-Baseline"><span class="toc-number">1.12.</span> <span class="toc-text">1.12 Baseline</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Baseline-Sound-Event-Localization-and-Detection-of-Overlapping-Sources-Using-Convolutional-Recurrent-Neural-Networks"><span class="toc-number">2.</span> <span class="toc-text">2. Baseline: Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#I-%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.1.</span> <span class="toc-text">I. 介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#A-Sound-event-detection"><span class="toc-number">2.1.1.</span> <span class="toc-text">A. Sound event detection</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#B-Sound-source-localization"><span class="toc-number">2.1.2.</span> <span class="toc-text">B. Sound source localization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#C-Joint-localization-and-detection"><span class="toc-number">2.1.3.</span> <span class="toc-text">C. Joint localization and detection</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#D-%E6%9C%AC%E7%AF%87%E8%AE%BA%E6%96%87%E7%9A%84%E8%B4%A1%E7%8C%AE"><span class="toc-number">2.1.4.</span> <span class="toc-text">D. 本篇论文的贡献</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#II-%E6%96%B9%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text">II. 方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#A-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-number">2.2.1.</span> <span class="toc-text">A. 特征提取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#B-Neural-network-architecture"><span class="toc-number">2.2.2.</span> <span class="toc-text">B. Neural network architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#CNN-%E7%BB%93%E6%9E%84"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">CNN 结构</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%9C%80%E7%BB%88-CNN-%E5%B1%82%E7%9A%84%E8%BE%93%E5%87%BA"><span class="toc-number">2.2.2.1.1.</span> <span class="toc-text">最终 CNN 层的输出</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#RNN-%E7%BB%93%E6%9E%84"><span class="toc-number">2.2.2.2.</span> <span class="toc-text">RNN 结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%EF%BC%88FC%EF%BC%89%E5%B1%82%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-number">2.2.2.3.</span> <span class="toc-text">全连接（FC）层的结构</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E5%B1%82-FC%EF%BC%88%E9%80%9A%E7%94%A8%EF%BC%89%EF%BC%9A"><span class="toc-number">2.2.2.3.1.</span> <span class="toc-text">第一层 FC（通用）：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#SED-%E5%88%86%E6%94%AF%EF%BC%88%E5%A3%B0%E4%BA%8B%E4%BB%B6%E6%A3%80%E6%B5%8B%EF%BC%89%EF%BC%9A"><span class="toc-number">2.2.2.3.2.</span> <span class="toc-text">SED 分支（声事件检测）：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#DOA-%E5%88%86%E6%94%AF%EF%BC%88%E6%96%B9%E5%90%91%E4%BC%B0%E8%AE%A1%EF%BC%89%EF%BC%9A"><span class="toc-number">2.2.2.3.3.</span> <span class="toc-text">DOA 分支（方向估计）：</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#SELDnet-%E5%8F%8A%E5%86%B3%E7%AD%96%E6%9C%BA%E5%88%B6"><span class="toc-number">2.2.2.4.</span> <span class="toc-text">SELDnet 及决策机制</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#C-%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="toc-number">2.2.3.</span> <span class="toc-text">C. 训练流程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E5%80%BC%E8%AE%BE%E5%AE%9A"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">目标值设定</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.3.2.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8-MSE-%E4%BD%9C%E4%B8%BA-DOA-%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-number">2.2.3.3.</span> <span class="toc-text">为什么使用 MSE 作为 DOA 估计的损失函数？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%AC%9B%E5%8D%A1%E5%B0%94%E5%9D%90%E6%A0%87-vs-%E6%96%B9%E4%BD%8D%E8%A7%92-%E4%BF%AF%E4%BB%B0%E8%A7%92"><span class="toc-number">2.2.3.4.</span> <span class="toc-text">笛卡尔坐标 vs. 方位角 &amp; 俯仰角</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">2.2.3.5.</span> <span class="toc-text">训练过程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.2.3.6.</span> <span class="toc-text">实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#III-%E8%AF%84%E4%BC%B0"><span class="toc-number">2.3.</span> <span class="toc-text">III. 评估</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#A-SELDnet-%E8%AF%84%E4%BC%B0%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.3.1.</span> <span class="toc-text">A.SELDnet 评估数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-TUT-Sound-Events-2018-Ambisonic-Anechoic-and-Synthetic-Impulse-Response-ANSYN-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.3.1.1.</span> <span class="toc-text">1) TUT Sound Events 2018 - Ambisonic, Anechoic and Synthetic Impulse Response (ANSYN) 数据集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-TUT-Sound-Events-2018-Ambisonic-Reverberant-and-Synthetic-Impulse-Response-RESYN-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.3.1.2.</span> <span class="toc-text">2) TUT Sound Events 2018 - Ambisonic, Reverberant and Synthetic Impulse Response (RESYN) 数据集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-TUT-Sound-Events-2018-Ambisonic-Reverberant-and-Real-life-Impulse-Response-REAL-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.3.1.3.</span> <span class="toc-text">3) TUT Sound Events 2018 - Ambisonic, Reverberant and Real-life Impulse Response (REAL) 数据集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-TUT-Sound-Events-2018-Ambisonic-Reverberant-and-Real-life-Impulse-Response-big-REALBIG-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.3.1.4.</span> <span class="toc-text">4) TUT Sound Events 2018 - Ambisonic, Reverberant and Real-life Impulse Response big (REALBIG) 数据集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-TUT-Sound-Events-2018-Ambisonic-Reverberant-Real-life-Impulse-Response-and-Ambiance-big-REALBIGAMB-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.3.1.5.</span> <span class="toc-text">5) TUT Sound Events 2018 - Ambisonic, Reverberant, Real-life Impulse Response and Ambiance big (REALBIGAMB) 数据集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#6-TUT-Sound-Events-2018-Circular-array-Anechoic-and-Synthetic-Impulse-Response-CANSYN-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.3.1.6.</span> <span class="toc-text">6) TUT Sound Events 2018 - Circular array, Anechoic and Synthetic Impulse Response (CANSYN) 数据集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#7-TUT-Sound-Events-2018-Circular-array-Reverberant-and-Synthetic-Impulse-Response-CRESYN-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.3.1.7.</span> <span class="toc-text">7) TUT Sound Events 2018 - Circular array, Reverberant and Synthetic Impulse Response (CRESYN) 数据集</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#B-Baseline-%E6%96%B9%E6%B3%95"><span class="toc-number">2.3.2.</span> <span class="toc-text">B. Baseline 方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SELDnet-%E8%AF%84%E4%BC%B0%E5%9F%BA%E7%BA%BF%E6%96%B9%E6%B3%95"><span class="toc-number">2.3.3.</span> <span class="toc-text">SELDnet 评估基线方法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-SED-%E5%9F%BA%E7%BA%BF%E6%96%B9%E6%B3%95"><span class="toc-number">2.3.3.1.</span> <span class="toc-text">1) SED 基线方法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-DOA-%E4%BC%B0%E8%AE%A1%E5%9F%BA%E7%BA%BF%E6%96%B9%E6%B3%95"><span class="toc-number">2.3.3.2.</span> <span class="toc-text">2) DOA 估计基线方法</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#i-MUSIC%EF%BC%88%E5%8F%82%E6%95%B0%E5%8C%96%E5%9F%BA%E7%BA%BF%EF%BC%89"><span class="toc-number">2.3.3.2.1.</span> <span class="toc-text">i) MUSIC（参数化基线）</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#ii-DOAnet%EF%BC%88DNN-%E5%9F%BA%E7%BA%BF%EF%BC%89"><span class="toc-number">2.3.3.2.2.</span> <span class="toc-text">ii) DOAnet（DNN 基线）</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#iii-AZInet%EF%BC%88DNN-%E5%9F%BA%E7%BA%BF%EF%BC%8C%E4%BB%85%E4%BC%B0%E8%AE%A1%E6%96%B9%E4%BD%8D%E8%A7%92%EF%BC%89"><span class="toc-number">2.3.3.2.3.</span> <span class="toc-text">iii) AZInet（DNN 基线，仅估计方位角）</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-SELD-%E5%9F%BA%E7%BA%BF%E6%96%B9%E6%B3%95%EF%BC%88HIRnet%EF%BC%89"><span class="toc-number">2.3.3.3.</span> <span class="toc-text">3) SELD 基线方法（HIRnet）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#C-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">2.3.4.</span> <span class="toc-text">C. 评估指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#D-%E5%AE%9E%E9%AA%8C"><span class="toc-number">2.3.5.</span> <span class="toc-text">D. 实验</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#SELDnet-%E8%AF%84%E4%BC%B0%E7%BB%B4%E5%BA%A6"><span class="toc-number">2.3.5.1.</span> <span class="toc-text">SELDnet 评估维度</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1-SELDnet-%E7%BB%93%E6%9E%84%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4"><span class="toc-number">2.3.5.1.1.</span> <span class="toc-text">1) SELDnet 结构和模型参数调整</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-SELDnet-%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F%E9%80%89%E6%8B%A9"><span class="toc-number">2.3.5.1.2.</span> <span class="toc-text">2) SELDnet 输出格式选择</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-%E8%BF%9E%E7%BB%AD-DOA-%E4%BC%B0%E8%AE%A1%E5%8F%8A%E5%AF%B9%E6%9C%AA%E8%A7%81-DOA-%E7%9A%84%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B"><span class="toc-number">2.3.5.1.3.</span> <span class="toc-text">3) 连续 DOA 估计及对未见 DOA 的泛化能力</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#4-%E4%B8%8D%E5%8C%B9%E9%85%8D%E6%B7%B7%E5%93%8D%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%E6%80%A7%E8%83%BD"><span class="toc-number">2.3.5.1.4.</span> <span class="toc-text">4) 不匹配混响数据集上的性能</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#5-%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%A7%E5%B0%8F%E5%AF%B9-SELDnet-%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">2.3.5.1.5.</span> <span class="toc-text">5) 训练数据集大小对 SELDnet 性能的影响</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#6-%E4%B8%8D%E5%90%8C%E4%BF%A1%E5%99%AA%E6%AF%94%EF%BC%88SNR%EF%BC%89%E4%B8%8B%E7%9A%84-SELDnet-%E6%80%A7%E8%83%BD"><span class="toc-number">2.3.5.1.6.</span> <span class="toc-text">6) 不同信噪比（SNR）下的 SELDnet 性能</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#7-SELDnet-%E5%9C%A8%E4%B8%8D%E5%90%8C%E9%98%B5%E5%88%97%E7%BB%93%E6%9E%84%E4%B8%8A%E7%9A%84%E9%80%9A%E7%94%A8%E6%80%A7"><span class="toc-number">2.3.5.1.7.</span> <span class="toc-text">7) SELDnet 在不同阵列结构上的通用性</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86%E4%B8%8E%E6%B5%8B%E8%AF%95%E9%9B%86%E7%9A%84%E5%B7%AE%E5%BC%82"><span class="toc-number">2.3.5.1.8.</span> <span class="toc-text">训练集与测试集的差异</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#IV-%E7%BB%93%E6%9E%9C%E5%92%8C%E8%AE%A8%E8%AE%BA"><span class="toc-number">2.4.</span> <span class="toc-text">IV. 结果和讨论</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-SELDnet-%E7%BB%93%E6%9E%84%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4-1"><span class="toc-number">2.4.1.</span> <span class="toc-text">1) SELDnet 结构和模型参数调整</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-SELDnet-%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F%E9%80%89%E6%8B%A9-1"><span class="toc-number">2.4.2.</span> <span class="toc-text">2) SELDnet 输出格式选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E8%BF%9E%E7%BB%AD-DOA-%E4%BC%B0%E8%AE%A1%E5%8F%8A%E5%AF%B9%E6%9C%AA%E8%A7%81-DOA-%E7%9A%84%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B-1"><span class="toc-number">2.4.3.</span> <span class="toc-text">3) 连续 DOA 估计及对未见 DOA 的泛化能力</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E4%B8%8D%E5%8C%B9%E9%85%8D%E6%B7%B7%E5%93%8D%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%E6%80%A7%E8%83%BD-1"><span class="toc-number">2.4.4.</span> <span class="toc-text">4) 不匹配混响数据集上的性能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%A7%E5%B0%8F%E5%AF%B9-SELDnet-%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D-1"><span class="toc-number">2.4.5.</span> <span class="toc-text">5) 训练数据集大小对 SELDnet 性能的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E4%B8%8D%E5%90%8C%E4%BF%A1%E5%99%AA%E6%AF%94%EF%BC%88SNR%EF%BC%89%E4%B8%8B%E7%9A%84-SELDnet-%E6%80%A7%E8%83%BD-1"><span class="toc-number">2.4.6.</span> <span class="toc-text">6) 不同信噪比（SNR）下的 SELDnet 性能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-SELDnet-%E5%9C%A8%E4%B8%8D%E5%90%8C%E9%98%B5%E5%88%97%E7%BB%93%E6%9E%84%E4%B8%8A%E7%9A%84%E9%80%9A%E7%94%A8%E6%80%A7-1"><span class="toc-number">2.4.7.</span> <span class="toc-text">7) SELDnet 在不同阵列结构上的通用性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#V-%E7%BB%93%E8%AE%BA"><span class="toc-number">2.5.</span> <span class="toc-text">V. 结论</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/03/%E6%AF%94%E8%B5%9B%E7%9B%B8%E5%85%B3%EF%BC%9ADCASE%202019%20Task3%20SELD/" title="比赛相关：DCASE 2019 Task3 SELD"><img src="https://i.ibb.co/ZRtszJys/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="比赛相关：DCASE 2019 Task3 SELD"/></a><div class="content"><a class="title" href="/2025/03/03/%E6%AF%94%E8%B5%9B%E7%9B%B8%E5%85%B3%EF%BC%9ADCASE%202019%20Task3%20SELD/" title="比赛相关：DCASE 2019 Task3 SELD">比赛相关：DCASE 2019 Task3 SELD</a><time datetime="2025-03-03T14:26:06.037Z" title="发表于 2025-03-03 22:26:06">2025-03-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AA%20Hybrid%20DSP:Deep%20Learning%20Approach%20to%20%20Real-Time%20Full-Band%20Speech%20Enhancement/" title="文献阅读: A Hybrid DSP/Deep Learning Approach to  Real-Time Full-Band Speech Enhancement"><img src="https://i.ibb.co/VYX8yFsD/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: A Hybrid DSP/Deep Learning Approach to  Real-Time Full-Band Speech Enhancement"/></a><div class="content"><a class="title" href="/2024/12/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AA%20Hybrid%20DSP:Deep%20Learning%20Approach%20to%20%20Real-Time%20Full-Band%20Speech%20Enhancement/" title="文献阅读: A Hybrid DSP/Deep Learning Approach to  Real-Time Full-Band Speech Enhancement">文献阅读: A Hybrid DSP/Deep Learning Approach to  Real-Time Full-Band Speech Enhancement</a><time datetime="2024-12-15T04:12:53.312Z" title="发表于 2024-12-15 12:12:53">2024-12-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/" title="文献阅读: Masked Autoencoders that Listen"><img src="https://i.ibb.co/9HZr35S/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Masked Autoencoders that Listen"/></a><div class="content"><a class="title" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/" title="文献阅读: Masked Autoencoders that Listen">文献阅读: Masked Autoencoders that Listen</a><time datetime="2024-11-19T13:34:57.816Z" title="发表于 2024-11-19 21:34:57">2024-11-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners"><img src="https://i.ibb.co/gD7cJGh/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Masked Autoencoders Are Scalable Vision Learners"/></a><div class="content"><a class="title" href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners">文献阅读: Masked Autoencoders Are Scalable Vision Learners</a><time datetime="2024-11-17T12:30:01.804Z" title="发表于 2024-11-17 20:30:01">2024-11-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%20-%20Robustness%20and%20Regularization%20of%20Personal%20Audio%20Systems/" title="文献阅读: Robustness and Regularization of Personal Audio Systems"><img src="https://i.ibb.co/c1YpYsm/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Robustness and Regularization of Personal Audio Systems"/></a><div class="content"><a class="title" href="/2024/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%20-%20Robustness%20and%20Regularization%20of%20Personal%20Audio%20Systems/" title="文献阅读: Robustness and Regularization of Personal Audio Systems">文献阅读: Robustness and Regularization of Personal Audio Systems</a><time datetime="2024-10-27T13:47:25.740Z" title="发表于 2024-10-27 21:47:25">2024-10-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Gavin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Have a nice day!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>