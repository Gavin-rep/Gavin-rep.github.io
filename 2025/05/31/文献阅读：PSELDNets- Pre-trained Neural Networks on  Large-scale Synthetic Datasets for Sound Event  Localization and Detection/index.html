<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>文献阅读: PSELDNets: Pre-trained Neural Networks on  Large-scale Synthetic Datasets for Sound Event  Localization and Detection | Gavin</title><meta name="author" content="Gavin"><meta name="copyright" content="Gavin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Arxiv  10 Nov 2024 摘要​	基于学习的声音事件定位与检测（SELD）技术已取得显著进展。此类系统通常在特定数据集上从头训练，但展现出较强的泛化能力。近年来，基于大规模数据集训练的深度神经网络在声音事件分类（SEC）领域取得显著成功，这引发了一个开放性问题：能否将此类进展拓展至通用型SELD模型的开发？本文利用预训练SEC模型的优势，提出在大规模合成数据集上预训练的SELD网络（P">
<meta property="og:type" content="article">
<meta property="og:title" content="文献阅读: PSELDNets: Pre-trained Neural Networks on  Large-scale Synthetic Datasets for Sound Event  Localization and Detection">
<meta property="og:url" content="http://example.com/2025/05/31/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9APSELDNets-%20Pre-trained%20Neural%20Networks%20on%20%20Large-scale%20Synthetic%20Datasets%20for%20Sound%20Event%20%20Localization%20and%20Detection/index.html">
<meta property="og:site_name" content="Gavin">
<meta property="og:description" content="Arxiv  10 Nov 2024 摘要​	基于学习的声音事件定位与检测（SELD）技术已取得显著进展。此类系统通常在特定数据集上从头训练，但展现出较强的泛化能力。近年来，基于大规模数据集训练的深度神经网络在声音事件分类（SEC）领域取得显著成功，这引发了一个开放性问题：能否将此类进展拓展至通用型SELD模型的开发？本文利用预训练SEC模型的优势，提出在大规模合成数据集上预训练的SELD网络（P">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.ibb.co/7tWdCyHB/image.png">
<meta property="article:published_time" content="2025-05-31T06:55:43.250Z">
<meta property="article:modified_time" content="2025-05-31T08:19:16.372Z">
<meta property="article:author" content="Gavin">
<meta property="article:tag" content="SSL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.ibb.co/7tWdCyHB/image.png"><link rel="shortcut icon" href="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"><link rel="canonical" href="http://example.com/2025/05/31/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9APSELDNets-%20Pre-trained%20Neural%20Networks%20on%20%20Large-scale%20Synthetic%20Datasets%20for%20Sound%20Event%20%20Localization%20and%20Detection/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '文献阅读: PSELDNets: Pre-trained Neural Networks on  Large-scale Synthetic Datasets for Sound Event  Localization and Detection',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-05-31 16:19:16'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.ibb.co/0fP2mb0/image.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">31</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.ibb.co/7tWdCyHB/image.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Gavin"><img class="site-icon" src="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"/><span class="site-name">Gavin</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">文献阅读: PSELDNets: Pre-trained Neural Networks on  Large-scale Synthetic Datasets for Sound Event  Localization and Detection</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-31T06:55:43.250Z" title="发表于 2025-05-31 14:55:43">2025-05-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-31T08:19:16.372Z" title="更新于 2025-05-31 16:19:16">2025-05-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="文献阅读: PSELDNets: Pre-trained Neural Networks on  Large-scale Synthetic Datasets for Sound Event  Localization and Detection"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Arxiv  10 Nov 2024</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​	基于学习的声音事件定位与检测（SELD）技术已取得显著进展。此类系统通常在特定数据集上从头训练，但展现出较强的泛化能力。近年来，基于大规模数据集训练的深度神经网络在声音事件分类（SEC）领域取得显著成功，这引发了一个开放性问题：能否将此类进展拓展至通用型SELD模型的开发？本文利用预训练SEC模型的优势，提出在大规模合成数据集上预训练的SELD网络（PSELDNets）。该合成数据集通过将声音事件与模拟空间房间脉冲响应（SRIRs）进行卷积生成，包含1,167小时的音频片段，涵盖170种声音类别。这些PSELDNets可迁移至下游SELD任务中。为适应特定场景（尤其是低资源数据情况），我们提出一种数据高效的微调方法AdapterBit。PSELDNets在基于TAU空间房间脉冲响应数据库（TAU-SRIR DB）采集的SRIRs构建的合成测试集上表现优异。我们进一步通过实验验证了PSELDNets在三个公开数据集及自采录音数据上的可迁移性。结果表明，PSELDNets在所有公开数据集上均超越现有最优系统。尽管SELD通常依赖充足的多通道音频片段以实现声达方向估计，但引入AdapterBit后，PSELDNets仅需极少多通道甚至单声道音频即可高效适应不同任务，其表现优于传统微调方法。</p>
<p><strong>索引术语</strong>——声音事件定位与检测（SELD）、预训练SELD网络、数据高效微调。</p>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><p>​	声音事件定位与检测（SELD）将声音事件检测（SED）与声达方向（DOA）估计相结合，旨在识别各类声源的类别、起始点、结束点及DOA信息。SELD框架在空间域与时域上表征声源，使其适用于机器人听觉、音频监控及智能家居环境等应用场景。</p>
<h3 id="A-现有的基于学习的SELD方法"><a href="#A-现有的基于学习的SELD方法" class="headerlink" title="A. 现有的基于学习的SELD方法"></a>A. 现有的基于学习的SELD方法</h3><p>​	近年来，基于学习的声音事件定位与检测（SELD）方法取得了显著进展。Adavanne等人[1]提出了SELDnet，这是一种专为同步声音事件检测和DOA估计设计的端到端网络。然而，SELDnet在识别来自不同位置的同类重叠声音事件方面仍面临挑战。为解决这类同源重叠问题，研究提出了事件独立网络V2（EINV2）[2]-[4]。EINV2采用轨道式输出格式和排列不变训练方法，可为每条轨道预测单个声音事件及其对应DOA。与SELDnet和EINV2不同，活动耦合笛卡尔DOA（ACCDOA）将SED和DOA任务合并为单一输出，并将活动信息嵌入笛卡尔DOA向量中[5]。多ACCDOA（mACCDOA）[6]通过引入轨道式输出格式和辅助重复排列不变训练来扩展ACCDOA，从而解决同源重叠问题。</p>
<p>​	另一方面，大量基于学习的SELD研究[2]-[11]主要使用了来自SELD挑战赛的合成数据集[12]-[17]，这些系统在模拟和真实空间环境中都表现出良好性能。然而，这些系统存在两个局限性：首先，系统预测的目标声音事件类别必须在训练前预先指定，这带来了挑战，因为每个应用场景可能需要不同的目标类别；其次，当遇到训练中未出现的声学环境时，基于学习的SELD方法可能会出现性能下降，这种现象被称为环境偏移[18]。</p>
<p>​	解决未知声音事件类别和未见声学环境问题的有效方法之一是获取大量特定场景数据进行训练。然而，创建空间声音事件信号是一项复杂的任务，涉及大量数据收集和计算生成。这一过程需要将干声源信号与测量的空间房间脉冲响应（SRIRs）进行卷积。此外，人工收集和标注真实世界空间声音事件记录的成本非常高，而公开可用的真实场景SELD数据十分有限[19] [20]。为应对这些挑战，零样本和少样本SELD系统[21]以及环境自适应Meta-SELD[18] [22]利用预训练模型在有限数据条件下有效工作。尽管有这些进展，SELD领域仍然明显缺乏基础模型。相比之下，声音事件分类（SEC）领域近期已开发出多个基础模型[23]-[26]，这些模型与SELD任务高度相关。然而，将SEC基础模型应用于SELD系统的潜在效益仍不明确。</p>
<h3 id="B-SEC的基础模型"><a href="#B-SEC的基础模型" class="headerlink" title="B. SEC的基础模型"></a>B. SEC的基础模型</h3><p>​	深度神经网络在声音事件分类（SEC）研究中取得了重大进展[23]-[26]。一个关键里程碑是AudioSet数据集的推出[27]，该数据集包含超过200万条人工标注的10秒音频片段，涵盖527种声音类别，被广泛用于通用声音事件识别。以预训练音频神经网络（PANNs）[23]为代表的卷积神经网络（CNN），通过从音频频谱图中提取局部特征，并优化网络的深度和宽度来提升性能。</p>
<p>​	近年来，在序列建模中表现优异的Transformer架构[28]被引入计算机视觉领域，其通过将图像分割为小块进行处理[29][30]。受此启发，音频谱图Transformer（AST）[24]、Patchout快速谱图Transformer（PaSST）[25]和分层令牌语义音频Transformer（HTS-AT）[26]等研究将纯注意力模型应用于音频频谱图，以捕捉长程全局上下文信息。AST[24]利用自注意力机制，通过重叠音频频谱图块并使用计算机视觉预训练参数，构建了首个无卷积的SEC模型。PaSST[25]受SpecAugment[31]和Dropout[32]启发，在训练时省略Transformer输入序列的部分片段，实现了AST的高效实现。相比之下，HTS-AT[26]采用具有窗口移位注意力的Swin Transformer模块[30]，通过将自注意力计算限制在局部非重叠窗口内，同时允许跨窗口连接来提高效率。这些模型在AudioSet上均实现了最先进的SEC性能。</p>
<p>​	这些基于大规模数据集预训练的模型展现出向其他SEC任务迁移的潜力[23]-[26]。然而，如何有效将这些预训练模型迁移至不同音频下游任务仍具挑战性。常见的全参数微调方法虽然可以调整所有预训练参数，但需要大量计算资源和内存容量。此外，这种方法可能导致模型泛化能力下降，这可能是由于任务间的灾难性干扰所致[33]。</p>
<h3 id="C-参数有效的微调"><a href="#C-参数有效的微调" class="headerlink" title="C. 参数有效的微调"></a>C. 参数有效的微调</h3><p>​	为应对高效迁移面临的挑战，参数高效微调（PEFT）方法在自然语言处理[34]-[37]和计算机视觉[33] [38]-[40]领域得到广泛研究。该方法仅需微调少量（额外）参数即可获得强劲性能。典型的PEFT方法包括低秩适应（LoRA）[34]、适配器调优[33] [37] [38]、提示调优[40]等。这些PEFT方法的核心原理是冻结全部或部分预训练参数，同时引入少量可训练参数进行微调。</p>
<p>​	基于这些PEFT技术，音频领域的研究者将特定模型适配器（Adapter）集成到框架中[41]-[44]。适配器作为一种即插即用模块，专为基于注意力的网络设计，通过在Transformer层中嵌入轻量级瓶颈网络实现。这些方法既能保持预训练模型的通用性，又可节省计算资源、降低数据需求，同时获得可比甚至更优的性能表现。</p>
<h3 id="D-我们的贡献"><a href="#D-我们的贡献" class="headerlink" title="D.我们的贡献"></a>D.我们的贡献</h3><p>​	本研究致力于开发适用于多样化现实场景的通用型SELD模型。我们提出了基于大规模合成数据集预训练的SELD网络（PSELDNets）。该数据集通过将FSD50K[45]中的声音事件片段与模拟SRIRs进行卷积生成，包含约1,167小时的音频片段，涵盖170种声音类别。PSELDNets继承了在SEC领域取得最先进（SOTA）性能的预训练模型架构，包括PANNs[23]、PaSST[25]和HTS-AT[26]，能够从多通道频谱图中提取空间和全局特征。我们在采用TAU空间房间脉冲响应数据库（TAU-SRIR DB）[46]实测SRIRs构建的合成测试集上评估PSELDNets，获得了满意性能。</p>
<p>​	我们将PSELDNets迁移至多个公开下游数据集，包括DCASE 2021挑战赛任务3[14]、L3DAS22挑战赛任务2[16]、STARSS23数据集[20]以及自采录音数据。实验结果表明，PSELDNets在所有公开数据集上均超越现有SOTA基准[4] [10] [47]-[49]，展现出卓越的迁移能力。受PEFT技术[33] [36] [38]启发，我们提出数据高效微调方法AdapterBit，该方法可利用极少量多通道甚至单声道片段进行高效调优。相比传统全参数微调，采用AdapterBit进行低资源数据迁移学习时，PSELDNets表现出更优性能。值得注意的是，当使用单声道片段时，我们通过麦克风阵列理论响应生成伪多通道片段以确保与PSELDNets输入的兼容性。</p>
<p>本研究的贡献包括：</p>
<ol>
<li><p>构建包含大量声音事件实例和多样声学环境的大规模SELD合成数据集</p>
</li>
<li><p>基于该数据集训练PSELDNets以开发通用模型</p>
</li>
<li><p>将PSELDNets迁移至多个下游SELD任务并实现SOTA性能</p>
</li>
<li><p>提出数据高效微调技术AdapterBit，支持有限资源下的目标任务适配</p>
</li>
<li><p>公开PSELDNets源代码、预训练参数及大规模合成SELD数据集</p>
</li>
</ol>
<h2 id="2-数据合成"><a href="#2-数据合成" class="headerlink" title="2. 数据合成"></a>2. 数据合成</h2><p>​	SELD片段的合成通过将FSD50K[45]中的纯净声音事件片段与模拟SRIRs进行卷积实现。准确模拟空间声音事件记录的关键在于获取高质量的声音事件片段和SRIRs。</p>
<h3 id="A-声音事件片段"><a href="#A-声音事件片段" class="headerlink" title="A. 声音事件片段"></a>A. 声音事件片段</h3><p>​	在声音事件分类（SEC）数据集开发方面已有诸多研究[27] [45] [50]-[55]。本研究基于AudioSet本体论[27]筛选声音事件片段，重点关注强标注、单源片段和高标签质量三个维度。</p>
<ol>
<li><strong>AudioSet本体论</strong>：为构建通用型SELD模型，所选声音类别需覆盖日常声音的广泛范围，并在数据和词汇量上具备可扩展性。我们采用包含632个声音类别、具有6层层次结构的AudioSet本体论2进行数据组织。该本体论已被AudioSet[27]、FSDnoisy18K[50]、FSDKaggle2019[51]和FSD50K[45]等数据集采用。</li>
<li><strong>强数据标注</strong>：与声音事件检测（SED）类似，SELD任务需要预测声音事件的精确起止时间（强标注），这对运动声源的轨迹预测至关重要。然而，具备此类精细标注的SED数据集[54] [56]十分稀缺，多数数据集仅提供片段级弱标注[27] [45] [50]-[53] [55]。</li>
<li><strong>单源片段</strong>：SELD要求对空间分离的独立声源进行区分。而典型SEC数据集通常包含多标签标注的音频片段，可能涉及重叠声音事件或具有层次传播标签的单个事件[27] [45]。因此合成SELD片段时，必须确保每个片段仅包含单一时刻的声源。</li>
<li><strong>标签质量</strong>：数据集质量直接影响模型性能。早期音频研究依赖小规模精细标注数据集[52] [54] [55]，而随着AudioSet[27]等大规模数据集出现，标签噪声问题日益显著[45]。虽然部分研究关注噪声标签下的学习[50] [51]，但本工作优先保证标签准确性以避免性能退化[57]。</li>
</ol>
<p>​	基于此，我们选用FSD50K[45]的单源片段进行合成。该数据集包含51,197个音频片段（总计108小时），人工标注200个AudioSet类别，虽采用弱标注但具有高标签密度[58]（有效录音时长占比），可将整段片段视为有效事件。值得注意的是，AudioSet的强标注数据[56]因存在类别不平衡和缺失问题未被采用。</p>
<h3 id="B-空间房间脉冲响应"><a href="#B-空间房间脉冲响应" class="headerlink" title="B. 空间房间脉冲响应"></a>B. 空间房间脉冲响应</h3><p>​	声音事件定位与检测（SELD）通常需要多通道音频输入以实现有效的声源定位。作为阵列无关格式的一阶Ambisonics（FOA）被广泛应用于各类SELD数据集[12]-[17] [19] [20] [59]。许多先进方法采用FOA信号（而非原始麦克风阵列格式信号）取得了最先进（SOTA）成果[10] [11] [48] [60]。此外，部分研究[61] [62]探索了任意麦克风阵列的Ambisonics编码方案，同样展现出优越性能。因此，本研究采用FOA格式的SRIRs合成SELD片段。</p>
<p>​	Ambisonics是一种基于球谐函数正交基分解声场的数据格式，通常通过球面麦克风阵列信号转换获得[63]。FOA信号包含四个通道（W, Y, Z, X），其中W表示全向麦克风，(Y, Z, X)分别对应笛卡尔坐标系三个轴向的指向性麦克风。FOA的理论空间响应可表示为[12] [63] [64]：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531152328007.png" alt="image-20250531152328007" style="zoom:50%;" />

<p>其中$\theta$ 和$\phi$ 表示俯仰角和方位角。</p>
<p>​	FOA格式SRIRs的计算生成方法包含两个步骤：麦克风阵列RIRs模拟和Ambisonics格式转换。麦克风阵列RIRs主要通过镜像源法[65]生成，而将麦克风阵列信号转换为FOA信号的具体流程可参考文献[18] [63] [64] [66] [67]。</p>
<h2 id="3-SELD系统"><a href="#3-SELD系统" class="headerlink" title="3. SELD系统"></a>3. SELD系统</h2><h3 id="A-相关的SELD方法"><a href="#A-相关的SELD方法" class="headerlink" title="A. 相关的SELD方法"></a>A. 相关的SELD方法</h3><p>本文介绍两种现有的基于学习的SELD方法：事件独立网络V2（EINV2）[2]-[4]和活动耦合笛卡尔DOA（ACCDOA）[5] [6]。</p>
<ol>
<li>EINV2方法：<br>EINV2[3]采用双分支结构（SED分支和DOA估计分支），通过软参数共享策略（如多组可学习参数）实现分支关联。每个分支包含多个事件独立轨道，形成轨道对。每个轨道对仅能预测一个声音事件及其对应DOA。该方法采用排列不变训练解决预测结果与真实标签间的轨道错位问题，网络架构如图1所示。对于第$i$个轨道，$y^i_{SED}$表示集合$S$中$M$个声音事件类别的one-hot编码，$y^i_{DOA}$表示笛卡尔DOA输出。轨道数量取决于最大重叠事件数。</li>
<li>ACCDOA方法：<br>ACCDOA通过笛卡尔DOA向量的长度表征声音事件存在性。与EINV2不同，ACCDOA将两个分支合并，既避免了平衡SED与DOA分支损失的需求，又防止了模型参数膨胀。</li>
</ol>
<p>​	但该方法无法检测同类型事件的多位置实例。为此提出的mACCDOA表示法[6]融合了类级和轨道级输出格式（如图2所示），并引入辅助复制排列不变训练（ADPIT）来解决轨道错位和稀疏目标输出问题。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531152737911.png" alt="image-20250531152737911" style="zoom:50%;" />

<h3 id="B-网络结构"><a href="#B-网络结构" class="headerlink" title="B. 网络结构"></a>B. 网络结构</h3><p>​	基于深度神经网络[23]-[26]和大规模数据集[27] [45]的发展，声音事件分类（SEC）领域取得显著进展。利用在SEC中表现优异的预训练模型（如PANNs[23]、AST[24]、PaSST[25]和HTS-AT[26]）可能提升SELD任务性能。因此，PSELDNets的结构设计与这些预训练SEC模型保持一致以实现有效迁移学习。PSELDNets以对数梅尔频谱与强度向量的拼接作为输入，根据第III-A节所述SELD输出格式，预测每个时间戳的活跃声音事件及其对应DOA向量。</p>
<ol>
<li><p><strong>PANNs架构</strong>：<br>PANNs[23]是基于卷积的模型，在AudioSet上从头训练。当前SELD技术主要采用CNN-注意力混合模型（如ResNet-Conformer[10] [11]和CNN-Conformer[4] [9]）。</p>
<p>我们采用CNN14-Conformer结构，即在CNN14模块[23]主体后接Conformer块[68]。CNN14包含6个类VGG[69]的CNN块，而Conformer由两个前馈层（含残差连接）夹着多头自注意力与卷积模块组成。CNN块提取局部细粒度特征，Conformer块则捕捉音频序列的局部与全局上下文依赖。</p>
</li>
<li><p><strong>PaSST架构</strong>：<br>PaSST[25]是AST[24]的高效改进版，整体结构如图3所示，注意力块细节见图4(a)。其将二维音频频谱分割为重叠块序列，经展平和线性投影后形成一维块嵌入，由标准Transformer编码器[28]处理。借鉴Dropout[32]和SpecAugment[31]，PaSST采用非结构化与结构化Patchout技术：前者随机丢弃任意位置块，后者选择特定频段或时间帧并移除对应行列块。这两种方法提升泛化能力并降低计算复杂度。</p>
<p>我们采用频域结构化Patchout[25]确保每帧有效输出。PaSST使用分类与蒸馏令牌机制进行标签预测，但需通过线性头将Transformer层输出的时序嵌入投影为SELD格式，以实现精确时间戳预测。</p>
</li>
<li><p><strong>HTS-AT架构</strong>：<br>HTS-AT[26]结合了Swin Transformer[30]与令牌语义模块[70]，其结构见图3与图4(b)。Swin Transformer在固定大小的非重叠局部窗口内计算自注意力，并通过层间窗口移位机制建立相邻窗口连接。深层通过逐步合并相邻块构建分层特征图以缩减序列尺寸。令牌语义模块[70]使用卷积头将最终Swin Transformer块输出的特征图转换为时间戳级的激活图。Swin Transformer块的具体架构与图4(a)标准Transformer编码器相同。</p>
</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531153034790.png" alt="image-20250531153034790" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531153051114.png" alt="image-20250531153051114" style="zoom:50%;" />

<h3 id="C-数据增强"><a href="#C-数据增强" class="headerlink" title="C. 数据增强"></a>C. 数据增强</h3><p>数据增强是提升系统泛化能力的有效技术。基于我们先前提出的数据增强链方法[4] [9]在L3DAS22任务2[16]和DCASE 2022任务3[19]中的成功应用，本研究采用该技术以增加数据多样性。每条数据增强链由多个随机选择并串联的增强操作组成。参照文献[4] [9]的方法，我们随机采样k&#x3D;3条增强链，并选取以下增强操作：</p>
<ol>
<li>Mixup[71]：混合样本增强</li>
<li>Cutout[72]：区域遮挡增强</li>
<li>SpecAugment[31]：频谱掩蔽增强</li>
<li>频移变换[8]：频率维度位移增强</li>
</ol>
<p>此外，本研究独立采用FOA信号旋转[10] [73]作为空间增强操作（不包含在增强链中），该操作通过随机旋转声场空间坐标来提升模型对方向变化的鲁棒性。</p>
<h2 id="4-数据有效的微调"><a href="#4-数据有效的微调" class="headerlink" title="4. 数据有效的微调"></a>4. 数据有效的微调</h2><p>​	微调涉及将预训练模型部署到新任务中，其中所有参数都从预训练模型初始化，头部层可能除外。传统的全微调方法可能导致模型泛化能力下降，这可能是由于任务间的灾难性干扰[33]。受PEFT技术[33] [36]-[38]的启发，我们提出了一种数据高效的微调策略AdapterBit。</p>
<p>​	SELD通常需要多通道音频输入进行声源定位。通过使用AdapterBit，PSELDNets可以更高效地适应使用有限数据的下游SELD任务，特别强调单声道声音事件片段。具体来说，当使用单声道信号进行微调时，我们基于所用麦克风阵列的理论响应生成伪FOA信号，以与PSELDNets的输入对齐。</p>
<h3 id="A-AdapterBit"><a href="#A-AdapterBit" class="headerlink" title="A. AdapterBit"></a>A. AdapterBit</h3><p>​	我们设计的AdapterBit结构如图5所示。AdapterBit包含偏置项调优和多层感知器（MLP）适配器。在偏置项调优中，仅微调预训练模型中的偏置项，例如线性层和卷积层中的偏置项。适配器由两个线性层之间的高斯误差线性单元（GELU）非线性激活函数构成，随后通过缩放因子$s$ 集成到标准Transformer编码器层中。该缩放因子用于平衡冻结分支产生的任务无关特征和可训练分支产生的任务特定特征。对于给定的输入特征$x_l$，适配器生成适应后的特征$\tilde{x}_l$如下：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531153728243.png" alt="image-20250531153728243" style="zoom:50%;" />

<p>其中$W_1$、$W_2$、$b_1$和$b_2$表示线性层的权重和偏置。参数$W_1$和$b_1$被随机初始化，而$W_2$和$b_2$被设为零。零初始化的基本原理是通过利用适配器的并行设计和残差连接，确保适应模型在微调开始时保持与预训练模型相同的状态。在微调阶段，我们专注于仅优化新添加的参数和PSELDNets的偏置项，同时保持其他参数冻结。具体而言，PSELDNets从预训练检查点初始化其权重，并保持除偏置项外的所有参数处于冻结状态。适配器和适应模型的偏置项在特定数据域上进行更新。在推理阶段，我们重新加载所有预训练参数，包括之前保持冻结的参数，以及新插入和微调过的参数。</p>
<h3 id="B-Pseudo-FOA-信号"><a href="#B-Pseudo-FOA-信号" class="headerlink" title="B. Pseudo-FOA 信号"></a>B. Pseudo-FOA 信号</h3><p>​	SELD通常需要多通道音频输入以实现有效的DOA估计，例如PSELDNET的四通道FOA信号输入。为满足此输入要求，可利用等式(1)描述的FOA理论响应从单声道信号生成伪FOA信号。此外，单声道信号必须包含非重叠声音事件以满足SEC中所述的单源片段要求（参见第II-A3节）。</p>
<p>​	pseudo-FOA 信号如下表示：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531154004315.png" alt="image-20250531154004315" style="zoom:50%;" />

<p>其中$S(t, f)$表示单声道信号频谱图。方位角$φ$和仰角$θ$可从目标分布中随机采样。生成的伪FOA信号可视为对输入单声道信号的一种正则化形式，因其在尽可能保留声音事件相关信息的同时，能够缓解通道间关联性的损失。</p>
<h2 id="5-实验设置"><a href="#5-实验设置" class="headerlink" title="5. 实验设置"></a>5. 实验设置</h2><h4 id="A-数据集"><a href="#A-数据集" class="headerlink" title="A. 数据集"></a>A. 数据集</h4><p>​	根据第II-A节的标准从FSD50K[45]中选择音频片段。我们选择单一声源的声音事件片段，并过滤掉少于30个片段的类别和那些存在识别挑战的类别。最终共选择170个类别。所选音频片段包含31,444个训练样本，总计43.4小时，以及另外3,701个测试样本，总计5.3小时。SRIRs主要通过模拟生成[74]。我们模拟了多种鞋盒形房间，采用频率相关的吸声系数。这种方法避免了从混响时间分布中采样和估计吸声系数值的要求，从而防止了不现实的场景，例如小房间中的长混响时间。来自典型声学材料数据库3,4的吸声材料被随机分配到每个模拟房间的墙壁、天花板和地面。此外，我们使用从TAU-SRIR DB[46]收集的SRIRs合成了额外的空间音频片段，以评估模拟的SRIRs和各种网络架构。我们采用了公开可用的数据合成代码5。</p>
<p>​	总共合成了67,000个1分钟片段，总计约1,117小时用于训练，其中每个片段使用独特的房间配置模拟，称为synthetic-training-set。另外合成了3,060个1分钟片段，总计约51小时用于测试，记为synthetic-test-set。合成数据集中最大复音数1、2和3的分布比例约为10:5:2。</p>
<h3 id="B-超参数"><a href="#B-超参数" class="headerlink" title="B. 超参数"></a>B. 超参数</h3><p>​	采样率为24 kHz。我们使用1024点的Hanning window和240的hopsize，从FOA信号中提取64维对数梅尔频谱图。每个音频片段被分割为固定的10秒时长用于训练和推理。PSELDNets的所有超参数与预训练SEC模型[23] [25] [26]保持一致。EINV2和mACCDOA中事件独立轨道的数量设置为3。训练采用批量大小为32和AdamW[75]优化器，学习率在前20个周期设为10−4，随后5个周期降至10−5。</p>
<h3 id="C-评估指标"><a href="#C-评估指标" class="headerlink" title="C. 评估指标"></a>C. 评估指标</h3><p>​	我们采用定位与检测的联合评价指标[76][77]：两个位置相关检测指标——F值(F≤20◦)和错误率(ER≤20◦)，以及两个类别相关定位指标——定位召回率($LR_{CD}$)和定位误差($LE_{CD}$)。$F_{\leq 20°}$和$ER_{\leq 20°}$考虑预测位置与真实位置相距空间阈值20◦范围内的真阳性。$LE_{CD}$和$LR_{CD}$计算在声事件类别被正确预测时的平均角度误差和真阳性率。需要注意的是，$LR_{CD}$也可以解释为无阈值召回率。我们使用综合SELD指标进行方法比较和超参数选择：	</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531154737339-20250531160154745.png" alt="image-20250531154737339" style="zoom:50%;" />

<p>​	为保持不同任务设置间的可比性和一致性，在STARSS23和synthetic-test-set中采用各类别指标的宏平均（macro-average），包括$F_{\leq 20°}$、$LR_{CD}$、$LE_{CD}$和$\epsilon_{SELD}$；在其他数据集中则采用各实例指标的微平均（micro-average）。一个有效的系统应表现为：低$ER_{\leq 20°}$、高$F_{\leq 20°}$、低$LE_{CD}$、高$LR_{CD}$和低$\epsilon_{SELD}$。</p>
<h2 id="6-实验"><a href="#6-实验" class="headerlink" title="6. 实验"></a>6. 实验</h2><p>​	首先，在synthetic-test-set上评估PSELDNets的性能，研究不同网络和SELD输出格式。其次，将PSELDNets迁移到多个公开可用的数据集。随后，在低资源数据上验证数据高效微调方法的效率。最后，使用我们自行收集的音频记录（称为Indoor Recordings）测试PSELDNets和数据高效微调方法的有效性。</p>
<h3 id="A-在合成数据集上的结果"><a href="#A-在合成数据集上的结果" class="headerlink" title="A. 在合成数据集上的结果"></a>A. 在合成数据集上的结果</h3><ol>
<li><p>网络架构：我们评估了CNN14-Conformer、PaSST和HTS-AT的性能，并选择mACCDOA[6]表示作为主要SELD输出格式。每个网络采用其各自的预训练检查点[23] [25] [26]，不包括随机初始化的额外Conformer[68]模块。表I展示了各种网络的比较结果。我们观察到，PSELDNets在所有类别的宏平均上实现了超过32%的$LR_{CD}$和约17◦的$LE_{CD}$。尽管CNN14-Conformer具有大量参数，但它在三个网络中表现最差，这可能是由于优化如此大型模型的挑战所致。与PaSST相比，HTS-AT以更少的参数实现了相似的性能。因此，我们选择HTS-AT作为进一步研究的基线模型。</p>
</li>
<li><p>SELD方法：我们评估了采用HTS-AT的三种SELD方法的性能：ACCDOA表示[5]、mACCDOA表示[6]和EINV2[3]。在EINV2方法中，我们使用了两个相对独立的SED和DOA分支，这两个分支具有相同的架构和预训练检查点。如图4(b)所示，这些分支通过几组可训练参数[3]连接，每组参数跟随一组Swin Transformers[30]。各种SELD方法的结果如表II所示。在这三种方法中，EINV2方法表现最差，尤其是在检测方面。与基于ACCDOA的方法相比，EINV2方法的$LR_{CD}$明显较差，但$LE_{CD}$显著更好。这种差异的一个可能解释是，EINV2使用多个轨道，每个轨道仅预测170个事件类别中的一个，导致SED分支的输出稀疏。相比之下，mACCDOA表示使用辅助复制排列不变训练（ADPIT）进行训练，使每个轨道能够学习与ACCDOA格式相同的目标。这种训练机制使mACCDOA产生密集输出，从而实现了与ACCDOA相似的性能。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531155241014-20250531160143967.png" alt="image-20250531155241014" style="zoom:50%;" /></li>
</ol>
<h3 id="B-迁移至下游任务"><a href="#B-迁移至下游任务" class="headerlink" title="B. 迁移至下游任务"></a>B. 迁移至下游任务</h3><p>​	本节探讨PSELDNets的一个应用案例。我们采用HTS-AT与mACCDOA进行迁移学习，并通过全参数微调方法应用于多个下游SELD任务。需特别说明，部分系统仅报告经过后处理的集成模型或单一模型结果。为公平比较，我们采用包含移动平均(MA)和动态阈值(DT)的后处理方法。推理阶段将测试样本分割为10秒片段（跳跃长度0.5秒），每个0.5秒片段的结果通过所有时间重叠片段取平均（MA方法）。与传统统一阈值方法不同，DT采用类别特定阈值。针对每个任务实施以下策略：1.使用AudioSet训练检查点微调（Scratch方法） 2.使用Synthetic-dataset训练检查点微调（Fine-tune方法）</p>
<ol>
<li><p><strong>L3DAS22任务2</strong>：<br>该任务[16]研究大型办公室环境中双FOA麦克风的3D声事件定位检测。数据集采用实测SRIRs合成，包含900段30秒音频（14类事件）。表III显示PSELDNets与我们先前提出的冠军系统ConvBlock-Conformer[4]在测试集的性能对比。我们将后者输出格式改为预测DOA而非3D笛卡尔坐标，且仅使用中心FOA麦克风。实验表明：Scratch方法与ConvBlock-Conformer性能相当，而Fine-tune方法显著优于后者（但数据增强未带来提升，可能源于合成数据集与目标数据集的高度相似性）。经后处理的Fine-tune方法在所有指标上达到最优。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531160107869-20250531160129655.png" style="zoom:50%;" />
</li>
<li><p><strong>DCASE 2021任务3</strong>：<br>该数据集[14]包含800段1分钟音频，含移动声源和类外定向干扰。表IV对比PSELDNets与排名前二的系统[47][48][78]（均报告集成模型后处理结果）。实验显示：微调PSELDNets和数据增强均显著提升性能。就综合指标$\epsilon_{SELD}$而言，经后处理的单模型PSELDNets甚至优于Shimada等[48]采用集成模型的SOTA系统。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531160229826.png" style="zoom:50%;" />
</li>
<li><p><strong>STARSS23</strong>：<br>该真实场景数据集[20]是STARSS22[19]的扩展版，包含7.5小时开发集录音。由于测试集标签未公开，表V展示验证集上PSELDNets与两大SOTA系统[11] [49] [80]的对比（均报告单模型后处理结果）。这些系统合成大量任务特定数据集（Wang-set&#x2F;Xue-set），我们相应构建Synth-set。经后处理的Fine-tune方法在ESELD（特别是$LR_{CD}$和$ER_{\leq 20°}$）上优于Wang等[11]结合声分离技术的SOTA系统。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531160329722.png" style="zoom:50%;" />
</li>
<li><p><strong>讨论</strong>：<br>我们通过三项任务评估数据增强与后处理的影响，并分析PSELDNets的局限性：</p>
</li>
</ol>
<p><strong>数据增强影响</strong>：如表IV&#x2F;V所示，即使采用预训练检查点，数据增强仍能显著提升性能。PSELDNets提供通用先验知识，数据增强则有效挖掘任务特定数据潜力。</p>
<p><strong>后处理影响</strong>：图6显示后处理在三项任务中的提升效果——MA显著改善ER20◦&#x2F;F20◦&#x2F;$LE_{CD}$，DT大幅提升$LR_{CD}$。图7可视化DCASE 2021任务3测试集某片段的预测结果：MA平滑事件轨迹（如婴儿哭声路径更接近真值），DT减少漏检（如10-15秒的敲门声）。</p>
<p><strong>PSELDNets局限</strong>：在DCASE 2021任务3和STARSS23中，Fine-tune方法的$LE_{CD}$比SOTA系统[11][48]高1◦以上，这源于HTS-AT的0.3秒时间分辨率（其他系统为0.1秒[4] [11] [48]）。该差异对静态声源（如L3DAS22任务2）影响较小，但会导致移动声源系统误差。值得注意的是，PSELDNets在检测指标的提升幅度普遍大于定位指标，这与文献[4][9]-[11] [48] [60] [78][80]的结论一致。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531161428231.png" alt="image-20250531161428231" style="zoom:50%;" />

<h3 id="C-数据有效的微调结果"><a href="#C-数据有效的微调结果" class="headerlink" title="C. 数据有效的微调结果"></a>C. 数据有效的微调结果</h3><p>​	本节探讨PSELDNets的另一应用场景：数据高效微调。具体而言，我们在低资源数据（仅含模拟SRIRs的小型合成数据集，无数据增强）和富资源数据（含增强处理的真实场景或实测SRIRs合成的大规模样本）两种场景下，采用集成AdapterBit的PSELDNets。所有结果均在DCASE 2021任务3与L3DAS22任务2的测试集及STARSS23验证集上评估。参照第VI-B节，我们对比以下方法：1.Scratch方法：从零训练  2.Fine-tune方法：全参数微调PSELDNets  3.AdapterBit方法：仅微调Adapter模块与偏置项。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531160901028.png" alt="image-20250531160901028"></p>
<ol>
<li><strong>AdapterBit效果验证</strong>：<br>使用L3DAS22任务2的ov1子集（250段30秒非重叠事件录音）进行消融实验。对单声道输入，从FOA信号提取首通道生成伪FOA信号。表VI顶部区块的1-channel部分显示：Adapter（不含偏置调优）的ESELD为0.449，接近全微调的0.456；加入偏置项后性能进一步提升。与真实FOA信号（4-channel）对比发现：单声道输入的所有方法均显著优于使用FOA信号的Scratch方法（ESELD&#x3D;0.523）。定位性能差异主要源于LECD（如4-ch Fine-tune的12.7° vs 1-ch Fine-tune的25.1°），而LRCD相近（65.6% vs 60.4%），反映真实FOA信号包含更丰富的声学环境与阵列信息。</li>
<li><strong>低资源数据结果</strong>：<br>针对STARSS23和DCASE 2021任务3（片段无复音标注），我们构建各任务专用的120段1分钟FOA训练集（模拟SRIRs生成）。表VI-VIII底部区块显示：无论多通道或单声道输入，AdapterBit均优于全微调。值得注意的是，单声道AdapterBit微调性能接近合成FOA信号，可显著降低目标场景适配成本。</li>
<li><strong>富资源数据结果</strong>：<br>图8展示AdapterBit在不同比例下游训练集（不同房间数&#x2F;数据分块）上的表现。对于STARSS23，额外使用包含全部事件类别的小型合成数据集（因单个房间录音仅含部分类别）。实验表明：</li>
</ol>
<ul>
<li>两种微调方法均优于Scratch方法</li>
<li>在有限数据下（如L3DAS22前两分块、DCASE 2021前两房间、STARSS23前五房间），AdapterBit数据利用率更高</li>
<li>数据量充足时，AdapterBit因可调参数有限，难以学习模拟环境与真实场景的分布差异，性能落后于全微调</li>
</ul>
<h3 id="D-室内记录的结果"><a href="#D-室内记录的结果" class="headerlink" title="D. 室内记录的结果"></a>D. 室内记录的结果</h3><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531161623141.png" alt="image-20250531161623141" style="zoom:50%;" />

<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20250531161636787.png" alt="image-20250531161636787"></p>
<p>​	本节评估PSELDNets在自建室内录音数据集（Indoor Recordings）上的迁移能力与数据高效微调效果。实验设置如图9所示，采用半径0.12米的四通道无障板球面麦克风阵列（四面体构型），在消声室和会议室两种环境中录制扬声器声源。会议室参数：混响时间T60&#x3D;900 ms，信噪比SNR&#x3D;6 dB。阵列位于正方形中心，声源布置在三个垂直高度与八个水平位置（正方形顶点或边中点），正方形边长分别为4米和2.4米，共48个声源位置。每个位置录制1分钟单声源片段（最大复音数1），另随机选取位置组合录制12段双声源片段（最大复音数2）。补充测量会议室8个位置的RIRs用于高效微调。声音事件片段来自NIGENS数据集[54]，其训练子集用于数据合成，测试子集通过扬声器播放。每个环境最终获得60段1分钟评估集录音。</p>
<p>我们合成四个训练数据集：</p>
<ol>
<li>Sim240：模拟SRIRs生成，240段，最大复音数2</li>
<li>Sim120：模拟SRIRs生成，120段，最大复音数2</li>
<li>Sim120_ov1：模拟SRIRs生成，120段，最大复音数1</li>
<li>Co120：实测会议室RIRs生成，120段，最大复音数2<br>（所有合成数据集的麦克风阵列构型与实际录音一致）</li>
</ol>
<p><strong>迁移性分析</strong>（表IX顶部区块）：<br>在增强合成数据集上微调PSELDNets后，于两种环境录音上测试发现：</p>
<ul>
<li>两环境的LRCD相近，但定位性能差异显著（源于声学环境差异）</li>
<li>使用实测RIRs合成的Co120相比Sim120能提升定位与检测性能</li>
<li>消声室的LECD约12°（阵列大半径与无障板设计导致高频段FOA转换误差[63]）</li>
<li>源域与目标域的分布差异（如麦克风特性）可能削弱预训练知识有效性[82]-[85]</li>
</ul>
<p><strong>数据高效微调效果</strong>（表IX底部区块）：</p>
<ol>
<li>输入信号对比：</li>
</ol>
<ul>
<li>FOA信号微调效果显著优于伪FOA信号（源于合成训练集与本实验阵列构型差异）</li>
</ul>
<ol>
<li>微调方法对比：</li>
</ol>
<ul>
<li>无论多通道或单声道输入，AdapterBit均优于全微调</li>
<li>单声道场景下AdapterBit优势尤为显著（可能机制：避免任务适应时的灾难性干扰[33]）</li>
</ul>
<h2 id="7-结论"><a href="#7-结论" class="headerlink" title="7.结论"></a>7.结论</h2><p>​	本文通过在大规模合成数据集上训练预训练SELD网络(PSELDNets)，构建了一个通用声学事件定位与检测(SELD)模型。合成数据集包含170种声音类别、总计1,167小时的音频样本。为提升PSELDNets在低资源场景下的适应能力，我们提出了数据高效微调技术AdapterBit。在synthetic-test-set上评估PSELDNets获得满意性能。我们将PSELDNets迁移至多个公开可用数据集以及自建的Indoor Recordings，实验结果优于先前state-of-the-art系统。此外，AdapterBit的引入提升了PSELDNets对低资源数据（包括有限的多通道和单声道音频片段）的迁移效率。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Gavin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/05/31/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9APSELDNets-%20Pre-trained%20Neural%20Networks%20on%20%20Large-scale%20Synthetic%20Datasets%20for%20Sound%20Event%20%20Localization%20and%20Detection/">http://example.com/2025/05/31/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9APSELDNets-%20Pre-trained%20Neural%20Networks%20on%20%20Large-scale%20Synthetic%20Datasets%20for%20Sound%20Event%20%20Localization%20and%20Detection/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Gavin</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/SSL/">SSL</a></div><div class="post_share"><div class="social-share" data-image="https://i.ibb.co/7tWdCyHB/image.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/" title="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization"><img class="cover" src="https://i.ibb.co/S7ZQZGLX/image.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFN-SSL%EF%BC%9AFull-Band%20and%20Narrow-Band%20Fusion%20for%20Sound%20Source%20Localization/" title="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization"><img class="cover" src="https://i.ibb.co/Q7BGbDBZ/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-20</div><div class="title">文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization</div></div></a></div><div><a href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/" title="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization"><img class="cover" src="https://i.ibb.co/S7ZQZGLX/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-20</div><div class="title">文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.ibb.co/0fP2mb0/image.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Gavin</div><div class="author-info__description">我的过去常常追赶着我</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">31</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/alexandergwm"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/alexandergwm" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:wenmiaogao@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">May you stay young forever, do the thing in your own zone.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text">1. 介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E7%8E%B0%E6%9C%89%E7%9A%84%E5%9F%BA%E4%BA%8E%E5%AD%A6%E4%B9%A0%E7%9A%84SELD%E6%96%B9%E6%B3%95"><span class="toc-number">2.1.</span> <span class="toc-text">A. 现有的基于学习的SELD方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-SEC%E7%9A%84%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.2.</span> <span class="toc-text">B. SEC的基础模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-%E5%8F%82%E6%95%B0%E6%9C%89%E6%95%88%E7%9A%84%E5%BE%AE%E8%B0%83"><span class="toc-number">2.3.</span> <span class="toc-text">C. 参数有效的微调</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#D-%E6%88%91%E4%BB%AC%E7%9A%84%E8%B4%A1%E7%8C%AE"><span class="toc-number">2.4.</span> <span class="toc-text">D.我们的贡献</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E5%90%88%E6%88%90"><span class="toc-number">3.</span> <span class="toc-text">2. 数据合成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E5%A3%B0%E9%9F%B3%E4%BA%8B%E4%BB%B6%E7%89%87%E6%AE%B5"><span class="toc-number">3.1.</span> <span class="toc-text">A. 声音事件片段</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-%E7%A9%BA%E9%97%B4%E6%88%BF%E9%97%B4%E8%84%89%E5%86%B2%E5%93%8D%E5%BA%94"><span class="toc-number">3.2.</span> <span class="toc-text">B. 空间房间脉冲响应</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-SELD%E7%B3%BB%E7%BB%9F"><span class="toc-number">4.</span> <span class="toc-text">3. SELD系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E7%9B%B8%E5%85%B3%E7%9A%84SELD%E6%96%B9%E6%B3%95"><span class="toc-number">4.1.</span> <span class="toc-text">A. 相关的SELD方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">4.2.</span> <span class="toc-text">B. 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">4.3.</span> <span class="toc-text">C. 数据增强</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%95%B0%E6%8D%AE%E6%9C%89%E6%95%88%E7%9A%84%E5%BE%AE%E8%B0%83"><span class="toc-number">5.</span> <span class="toc-text">4. 数据有效的微调</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-AdapterBit"><span class="toc-number">5.1.</span> <span class="toc-text">A. AdapterBit</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-Pseudo-FOA-%E4%BF%A1%E5%8F%B7"><span class="toc-number">5.2.</span> <span class="toc-text">B. Pseudo-FOA 信号</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="toc-number">6.</span> <span class="toc-text">5. 实验设置</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#A-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">6.0.1.</span> <span class="toc-text">A. 数据集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-number">6.1.</span> <span class="toc-text">B. 超参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">6.2.</span> <span class="toc-text">C. 评估指标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%AE%9E%E9%AA%8C"><span class="toc-number">7.</span> <span class="toc-text">6. 实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E5%9C%A8%E5%90%88%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%E7%BB%93%E6%9E%9C"><span class="toc-number">7.1.</span> <span class="toc-text">A. 在合成数据集上的结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-%E8%BF%81%E7%A7%BB%E8%87%B3%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1"><span class="toc-number">7.2.</span> <span class="toc-text">B. 迁移至下游任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-%E6%95%B0%E6%8D%AE%E6%9C%89%E6%95%88%E7%9A%84%E5%BE%AE%E8%B0%83%E7%BB%93%E6%9E%9C"><span class="toc-number">7.3.</span> <span class="toc-text">C. 数据有效的微调结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#D-%E5%AE%A4%E5%86%85%E8%AE%B0%E5%BD%95%E7%9A%84%E7%BB%93%E6%9E%9C"><span class="toc-number">7.4.</span> <span class="toc-text">D. 室内记录的结果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E7%BB%93%E8%AE%BA"><span class="toc-number">8.</span> <span class="toc-text">7.结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/05/31/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9APSELDNets-%20Pre-trained%20Neural%20Networks%20on%20%20Large-scale%20Synthetic%20Datasets%20for%20Sound%20Event%20%20Localization%20and%20Detection/" title="文献阅读: PSELDNets: Pre-trained Neural Networks on  Large-scale Synthetic Datasets for Sound Event  Localization and Detection"><img src="https://i.ibb.co/7tWdCyHB/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: PSELDNets: Pre-trained Neural Networks on  Large-scale Synthetic Datasets for Sound Event  Localization and Detection"/></a><div class="content"><a class="title" href="/2025/05/31/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9APSELDNets-%20Pre-trained%20Neural%20Networks%20on%20%20Large-scale%20Synthetic%20Datasets%20for%20Sound%20Event%20%20Localization%20and%20Detection/" title="文献阅读: PSELDNets: Pre-trained Neural Networks on  Large-scale Synthetic Datasets for Sound Event  Localization and Detection">文献阅读: PSELDNets: Pre-trained Neural Networks on  Large-scale Synthetic Datasets for Sound Event  Localization and Detection</a><time datetime="2025-05-31T06:55:43.250Z" title="发表于 2025-05-31 14:55:43">2025-05-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/" title="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization"><img src="https://i.ibb.co/S7ZQZGLX/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization"/></a><div class="content"><a class="title" href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AIPDnet-%20A%20Universal%20Direct-Path%20IPD%20Estimation%20%20Network%20for%20Sound%20Source%20Localization/" title="文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization">文献阅读: IPDnet: A Universal Direct-Path IPD Estimation  Network for Sound Source Localization</a><time datetime="2025-03-19T22:40:18.592Z" title="发表于 2025-03-20 06:40:18">2025-03-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFN-SSL%EF%BC%9AFull-Band%20and%20Narrow-Band%20Fusion%20for%20Sound%20Source%20Localization/" title="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization"><img src="https://i.ibb.co/Q7BGbDBZ/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization"/></a><div class="content"><a class="title" href="/2025/03/20/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFN-SSL%EF%BC%9AFull-Band%20and%20Narrow-Band%20Fusion%20for%20Sound%20Source%20Localization/" title="文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization">文献阅读: FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization</a><time datetime="2025-03-19T22:39:09.062Z" title="发表于 2025-03-20 06:39:09">2025-03-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/" title="文献阅读: Masked Autoencoders that Listen"><img src="https://i.ibb.co/9HZr35S/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Masked Autoencoders that Listen"/></a><div class="content"><a class="title" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/" title="文献阅读: Masked Autoencoders that Listen">文献阅读: Masked Autoencoders that Listen</a><time datetime="2024-11-19T13:34:57.816Z" title="发表于 2024-11-19 21:34:57">2024-11-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners"><img src="https://i.ibb.co/gD7cJGh/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Masked Autoencoders Are Scalable Vision Learners"/></a><div class="content"><a class="title" href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners">文献阅读: Masked Autoencoders Are Scalable Vision Learners</a><time datetime="2024-11-17T12:30:01.804Z" title="发表于 2024-11-17 20:30:01">2024-11-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Gavin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Have a nice day!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>