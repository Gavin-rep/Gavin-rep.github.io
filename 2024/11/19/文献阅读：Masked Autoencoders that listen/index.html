<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>文献阅读: Masked Autoencoders that Listen | Gavin</title><meta name="author" content="Gavin"><meta name="copyright" content="Gavin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Arxiv  12 Jan 2023.  META AI 摘要 ​	本文研究了基于图像的Masked Autoencoders (MAE) [1] 在音频谱图的自监督表示学习中的简单扩展。遵循MAE中的Transformer编码器-解码器设计，我们的Audio-MAE首先对音频谱图块进行编码，并采用较高的掩蔽比率，只将未被掩蔽的标记通过编码器层。然后，解码器重新排序并解码带有掩蔽标记的编码上下文，">
<meta property="og:type" content="article">
<meta property="og:title" content="文献阅读: Masked Autoencoders that Listen">
<meta property="og:url" content="http://example.com/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/index.html">
<meta property="og:site_name" content="Gavin">
<meta property="og:description" content="Arxiv  12 Jan 2023.  META AI 摘要 ​	本文研究了基于图像的Masked Autoencoders (MAE) [1] 在音频谱图的自监督表示学习中的简单扩展。遵循MAE中的Transformer编码器-解码器设计，我们的Audio-MAE首先对音频谱图块进行编码，并采用较高的掩蔽比率，只将未被掩蔽的标记通过编码器层。然后，解码器重新排序并解码带有掩蔽标记的编码上下文，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.ibb.co/9HZr35S/image.png">
<meta property="article:published_time" content="2024-11-19T13:34:57.816Z">
<meta property="article:modified_time" content="2024-11-19T14:09:39.416Z">
<meta property="article:author" content="Gavin">
<meta property="article:tag" content="MAE">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.ibb.co/9HZr35S/image.png"><link rel="shortcut icon" href="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"><link rel="canonical" href="http://example.com/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '文献阅读: Masked Autoencoders that Listen',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-19 22:09:39'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.ibb.co/0fP2mb0/image.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.ibb.co/9HZr35S/image.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Gavin"><img class="site-icon" src="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"/><span class="site-name">Gavin</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">文献阅读: Masked Autoencoders that Listen</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-19T13:34:57.816Z" title="发表于 2024-11-19 21:34:57">2024-11-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-19T14:09:39.416Z" title="更新于 2024-11-19 22:09:39">2024-11-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="文献阅读: Masked Autoencoders that Listen"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Arxiv  12 Jan 2023.  META AI</p>
<h2 id="摘要">摘要</h2>
<p>​	本文研究了基于图像的Masked Autoencoders (MAE) [1] 在音频谱图的自监督表示学习中的简单扩展。遵循MAE中的Transformer编码器-解码器设计，我们的Audio-MAE首先对音频谱图块进行编码，并采用较高的掩蔽比率，只将未被掩蔽的标记通过编码器层。然后，解码器重新排序并解码带有掩蔽标记的编码上下文，以重建输入的音频谱图。我们发现，在解码器中加入局部窗口注意力是有益的，因为音频谱图在时间和频率带上具有很高的局部相关性。接着，我们在目标数据集上使用较低的掩蔽比率微调编码器。实证结果表明，Audio-MAE在六个音频和语音分类任务中设立了新的最先进的性能，超越了其他使用外部监督预训练的近期模型。我们的代码和模型可在 <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/AudioMAE">https://github.com/facebookresearch/AudioMAE</a> 获取。</p>
<h2 id="1-介绍">1. 介绍</h2>
<p>​	Transformer [2] 和自监督学习 [3, 4, 5, 6, 7, 1] 正在主导计算机视觉（CV）和自然语言处理（NLP）研究。这场革命首先在NLP中开始，源于Transformer架构和自注意力 [8] 的发明。基于BERT的掩蔽自编码 [3] 通过在大规模语言语料库上进行自监督预训练，在各种NLP任务中设置了新的最先进的性能。同样，在CV领域，Vision Transformers (ViT) [9] 已经在CV任务中获得了广泛的应用，而在自监督图像表示学习方面，Masked Autoencoders (MAE) [1] 将CV社区推向了BERT在NLP中的成功。同样的，除了现有的能阅读（BERT）或看见（MAE）的掩蔽自编码器，在这项工作中，我们研究了能够“听”的掩蔽自编码器。</p>
<p>​	基于Transformer的模型最近在音频理解任务中刷新了排行榜。例如，AST [10] 和MBT [11] 提高了AudioSet [12]、事件声音分类 [13] 等数据集上的音频分类性能。背后的关键技术是通过膨胀音频模型权重并插值位置嵌入，使用ImageNet预训练的监督模型（例如，DeiT [14]）来初始化音频模型权重，从而对音频谱图进行编码。然而，利用ImageNet预训练模型可能不是最优的。与使用图像模型初始化视频模型（例如，I3D [15] 或 3D-ResNets [16] 的初始权重是从ImageNet预训练的图像模型膨胀而来的）不同，音频谱图与自然图像在表示音频内容和视觉物体形状方面存在显著差异。当前尚不清楚为何这种异质的图像到音频的迁移是有用的，除了在低层次语义（如谱图的形状和视觉物体的形状）上可能的相似性。此外，任何标签偏差都会不可避免地转移到音频模型中。</p>
<p>​	针对这些问题，自监督音频表示学习最近吸引了大量研究关注。基于BEiT [17]，它学习重构图像块或学习的块标记，SS-AST [18] 扩展到音频领域，利用类似1通道二维图像的音频谱图，并使用对比和重建目标作为自监督。在不使用任何标签的情况下，有效的自监督表示学习的关键在于大规模预训练数据。在这项工作中，我们使用AudioSet [12] 进行预训练，这是一个包含约200万个音频录音的常用数据集。使用Transformer架构进行大规模训练具有挑战性，因为Transformer中的自注意力在输入序列的长度上具有二次复杂度。</p>
<p>​	为了应对这一计算负担，采用了不同的解决方案。一种流行的方法是减少自注意力中的序列长度。为了解决图像和视频理解中的问题，开发了多种基于ViT的架构。例如，Swin-Transformer [19] 仅在窗口内执行局部注意力，并且这些窗口在各层之间平移。MViT [20] 采用池化注意力来构建一个Transformer层次结构，其中序列长度被下采样。对于自监督学习，MAE [1] 高效地编码了仅25%的视觉块，而大多数块则被丢弃。MAE的简洁性和可扩展性使其成为大规模自监督学习的一个有前景的框架。</p>
<p>​	在这项工作中，我们研究了MAE在声音识别中的应用以及音频领域的独特挑战。我们提出了Audio-MAE（图1），作为一个统一且可扩展的框架，用于学习自监督的音频表示。与MAE类似，它由一个Transformer编码器和解码器对组成。首先将声音转换并嵌入到谱图块中。在将它们送入Transformer编码器之前，我们使用较高的掩蔽比率并丢弃大部分块，仅将少量未掩蔽的嵌入送入编码器进行高效编码。编码后的块经过填充，使用可学习的嵌入来表示掩蔽块，然后恢复这些块在频率和时间上的顺序，并通过Transformer解码器传播它们，以重建音频谱图。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241119214244243.png" alt="image-20241119214244243"></p>
<p>​	与图像块不同，音频谱图块在局部上有较强的相关性。例如，形式音（formants），即声道共振，通常在谱图中是局部连续和分组的。频率和时间中的位置嵌入了决定谱图块语义的关键信息。因此，我们进一步研究了在Transformer解码器中使用局部化注意力和混合架构，以便正确解码并进行重建。这个简单而有效的升级提高了Audio-MAE的性能。</p>
<p>​	与MAE在图像中的做法类似，我们最小化了块归一化的均方误差。在微调阶段，我们丢弃解码器，并使用块掩蔽微调编码器。实验证明，Audio-MAE在六个音频和语音分类任务中设置了新的最先进的性能。它是第一个仅使用音频的自监督模型，在AudioSet-2M上实现了最先进的mAP，超越了其他最近的具有外部监督的模型。我们进一步提供了可视化和可听示例，以定性地展示Audio-MAE解码器的有效性。</p>
<h2 id="2-相关工作">2. 相关工作</h2>
<h3 id="视觉掩蔽预训练">视觉掩蔽预训练</h3>
<p>掩蔽/去噪自编码器 [21, 22, 3] 是一种通过从掩蔽或损坏的输入重构源数据的通用表示学习方法。在计算机视觉（CV）领域，视觉掩蔽预训练已经取得了最近的进展 [23, 24, 1, 20]。基于ViT [9]，它将Transformer应用于图像块，BEiT [17] 和MAE [1] 提出了掩蔽图像建模框架。BEiT [17] 学习预测由VAE [25] 生成的离散视觉标记，这些标记出现在掩蔽的图像块中。MAE [1] 通过随机掩蔽大部分图像块并仅对非掩蔽块进行编码来减少序列长度，从而重建像素颜色信息。MaskFeat [20] 研究了用于掩蔽预训练的特征，发现梯度方向直方图（HoG）[26]（与谱图特征相关）在图像和视频分类模型中表现强劲。我们的工作将MAE框架扩展到音频谱图的表示学习。</p>
<h3 id="跨领域音频预训练">跨领域音频预训练</h3>
<p>将ImageNet上预训练的监督式ViT [9] 或 ResNet [27] 迁移到音频模型已经成为一种流行做法 [10, 28, 11, 29, 30, 31]。这些模型在预训练后，通过在ViT中的预训练图像块嵌入中将3通道（RGB）降维到1通道（谱图），并在其上应用Transformer块来操作音频谱图。例如，HTS-AT [29] 使用来自Swin Transformer [19] 的层次Transformer对谱图进行编码。MBT [11] 使用ImageNet-21K上预训练的ViT；AST [10] 和 PaSST [28] 则采用DeiT [14] 作为Transformer骨干。我们提出的Audio-MAE则专注于音频仅自监督预训练，从零开始进行训练，且不使用跨领域（非音频）数据。</p>
<h3 id="音频领域内预训练">音频领域内预训练</h3>
<p>现有的领域内（即音频专用）自监督方法可以根据输入信号类型（如原始波形 [32, 33, 34]、帧级特征 [35, 36, 37] 或谱图块 [18, 38]）以及用于自监督的目标（如对比学习 [39, 33, 40, 41, 35] 或预测/重建 [18, 34, 37, 36]）进行大致分类。例如，wav2vec 2.0 [33] 采用原始波形作为输入，并利用对比学习来区分不同时间段中的上下文表示。Mockingjay [42] 提出了一个掩蔽的声学模型预任务，用于重建掩蔽时间帧的Mel特征。SS-AST [18] 是与Audio-MAE最为接近的工作，也是我们的主要基准。受到BERT [3] 成功的启发，SS-AST 提出了一个自监督学习方法，该方法在谱图块上进行操作，并在掩蔽的块上采用联合对比和重建目标。与这些以前的方法不同，Audio-MAE仅对非掩蔽的谱图块进行编码。</p>
<p>我们的工作与 [38, 43, 44] 相关方法独立且同时进行。我们还在实验中将我们的模型与这些并行的工作进行对比，展示了Audio-MAE的优越性。</p>
<h2 id="3-Audio掩码自编码器-Audio-MAE">3. Audio掩码自编码器 (Audio-MAE)</h2>
<h3 id="Audio-MAE概述">Audio-MAE概述</h3>
<p>Audio-MAE是MAE的一个概念扩展，旨在从音频谱图中学习自监督表示。其框架简单，具有较强的表达能力。图1展示了Audio-MAE的整体框架，具体组件的细节如下：</p>
<h3 id="谱图块嵌入">谱图块嵌入</h3>
<p>遵循[10, 18]，我们将音频录音转化为梅尔频率谱图（Mel-spectrograms），并将其划分为不重叠的规则网格块。这些块随后被展平并通过线性投影进行嵌入。类似于MAE [1]，我们向嵌入的块中添加固定的正弦位置嵌入。</p>
<h3 id="掩蔽策略">掩蔽策略</h3>
<p>Audio-MAE对大量的谱图块进行掩蔽。由于谱图可以视为声音的时间和频率成分的二维表示，探索在掩蔽时如何区分时间和频率是合理的。在本研究中，我们探索了无结构的掩蔽（即不进行任何预先设计的随机掩蔽）和结构化的掩蔽（即随机掩蔽谱图中的一部分时间、频率或时间+频率）策略，分别在预训练和微调阶段应用。图2展示了掩蔽的示例区域（黑色覆盖层）。掩蔽机制，如MAE [1]中所介绍的，是高效自监督学习的关键。对于输入的块序列，这可以看作一个伯努利过程，每个块以概率p（掩蔽比率）被掩蔽或丢弃。掩蔽减少了输入块序列的长度，并鼓励从有限的“可见”块中学习全局上下文表示。我们观察到，类似于图像，较大的掩蔽比率（在我们实验中，音频谱图块为80%，类似于MAE在图像中的75%）在学习自监督音频表示时是可行的。与BERT [3]在NLP中使用15%的掩蔽比率不同，由于这些模态中的高冗余性，音频谱图和图像的许多块/标记可以被丢弃。除了自监督预训练外，我们还进一步探索了在监督微调阶段掩蔽的有效性。实验证明，对于预训练，较高比率的无结构掩蔽和较低比率的结构化掩蔽（时间+频率掩蔽）能够提供最佳的准确性（具体消融实验见§4.4）。</p>
<h3 id="编码器">编码器</h3>
<p>Audio-MAE使用标准Transformer堆栈[2]作为其编码器。编码器仅处理（20%）非掩蔽块，以减少计算开销，因为自注意力的计算复杂度与输入序列长度的平方成正比。我们使用12层的ViT-Base (ViT-B) [9] Transformer作为默认设置。</p>
<h3 id="带局局部注意力的解码器">带局局部注意力的解码器</h3>
<p>解码器也由标准Transformer块组成。来自编码器的编码块被用可训练的掩蔽标记进行填充。在恢复音频谱图的原始时间-频率顺序后，我们将解码器的（固定的正弦）位置嵌入添加到恢复后的序列中，并将其输入到解码器。在解码器堆栈的顶部，我们添加了一个线性层，用于预测并重建输入的音频谱图。</p>
<h3 id="处理音频谱图的解码器增强">处理音频谱图的解码器增强</h3>
<p>为了处理音频谱图的独特特点，我们研究了对MAE解码器的增强。基于图像的MAE在Transformer解码器中使用全局自注意力，这对于视觉上下文是合适的，因为视觉对象通常在平移或缩放下是不变的，其精确位置可能不影响图像的语义。相比之下，音频谱图特征的位置、尺度和平移直接影响音频录音的声音或语义。因此，当谱图特征在时间-频率组件上主要局部相关时，全局自注意力对于音频谱图来说并非最优。例如，我们更倾向于使用元音低频带的谐波（如图2a所示）来预测谱图块在较高频带中的垂直位置，而不是在时间域中水平预测。同样，辅音的摩擦音通常只与辅音的其他部分相关，而与音频录音中的其他静音段无关。与图像相比，音频谱图块更类似于语音或文本标记，其顺序和位置更加相关。</p>
<p>为了解决音频谱图的这一特点，除了在标准MAE解码器中使用全局自注意力外，我们还结合了局部注意力机制，它在解码过程中将谱图块分组并在自注意力中为局部窗口进行解码。我们研究了两种类型的局部注意力：（1）移位窗口位置：受到Swin Transformer [19]中移位窗口的启发，我们在连续的Transformer解码器层之间以50%的比例进行窗口注意力的移位。为了填充移位时的边缘，我们将谱图周期性地向左上角移位。图3展示了通过移位窗口实现的局部化解码器注意力；（2）混合窗口注意力（全局+局部注意力）：受到[45]的启发，为了增加窗口之间的连接，我们设计了一种简单的混合（全局+局部）注意力，它在倒数几层之前在窗口内计算局部注意力。通过这种方式，最终重建层的输入特征图也包含了全局信息。为简便起见，我们不使用池化或层次结构。不同注意力类型的解码器比较见§4.4。</p>
<h3 id="目标">目标</h3>
<p>Audio-MAE解码器通过预测谱图块或其每个块归一化后的值来学习重建输入的音频谱图。目标是预测值与输入音频谱图之间的均方误差（MSE），并在未知块上取平均。实验证明，仅使用重建损失就足够了，而包含额外的对比目标（例如，InfoNCE损失 [46]）并没有提高Audio-MAE的效果。</p>
<h3 id="下游任务的微调">下游任务的微调</h3>
<p>在微调阶段，我们仅保留并微调Audio-MAE编码器，丢弃解码器。与原始MAE不同，并受到[47, 28]的启发，我们还探索在微调阶段使用掩蔽，去除部分块，以进一步正则化从有限视角的谱图输入中学习，这也间接减少了微调时的计算量。与SpecAug [48]（通过将掩蔽部分设置为零作为数据增强）不同，Audio-MAE仅看到未被掩蔽的真实输入块，并对这些非掩蔽块进行编码，并在上面应用平均池化层，然后为分类任务添加线性层进行微调。</p>
<h2 id="4-实验">4. 实验</h2>
<h3 id="4-1-数据集和任务">4.1 数据集和任务</h3>
<p>我们在六个任务上进行了广泛的评估，包括在AudioSet (AS-2M, AS-20K) 和环境声音分类（ESC-50），以及语音分类任务上，如Speech Commands (SPC-1 和 SPC-2) 和 VoxCeleb (SID)。我们使用AudioSet进行消融研究。</p>
<ul>
<li>
<p><strong>AudioSet</strong> [12]（AS-2M, AS-20K）包含约200万段10秒钟的YouTube音频片段，用于音频分类。每段音频片段有527种音频事件类型，采用弱标注[49, 50, 51]。每个片段可能包含多个事件。完整的训练集有两个子集：一个是类平衡的（22,176片段），另一个是类不平衡的（2,042,985片段）。评估集包含20,383片段。我们下载并处理了约1.96M的不平衡训练集、21K平衡训练集和19K评估集。对于AS-2M实验，我们使用不平衡和平衡训练音频的联合集进行预训练和微调；对于AS-20K实验，我们使用AS-2M进行预训练，并使用20K平衡集进行微调。我们报告在19K评估集上测试的mAP，这是AST [10]使用的评估集。</p>
</li>
<li>
<p><strong>环境声音分类（ESC-50）</strong> [13] 是一个音频分类数据集，包含2,000个5秒钟的环境声音录音，共有50个类别。我们报告了使用5折交叉验证的准确性，采用与[10]相同的分割。</p>
</li>
<li>
<p><strong>Speech Commands (SPC-2, SPC-1)</strong> [52] 是两个关键词检测任务。在SPC-2中，有35个语音命令。训练集/验证集/测试集分别包含84,843/9,981/11,005个1秒钟的录音。在SPC-1中，有10个关键词类，1个静音类，以及1个未知类，包含其余20个常见语音命令。我们使用SUPERB [53]基准提供的数据和分割，报告测试准确性。</p>
</li>
<li>
<p><strong>VoxCeleb (SID)</strong> [54] 包含来自1,251名说话者的150K语音片段。说话人识别任务（SID）是将语音片段分类以识别其原始说话者。我们使用V1标准的训练集（138,361）、验证集（6,904）和测试集（8,251），并报告测试准确性。</p>
</li>
</ul>
<h3 id="4-2-实现细节">4.2 实现细节</h3>
<p>我们默认使用一个12层的ViT-B作为Transformer编码器。对于解码器，我们使用一个16层的Transformer，并采用移位局部注意力机制。我们调查了原始（全局注意力）和混合（全局+局部注意力）解码器变体（见表1c）。</p>
<p>遵循[10, 11]的方法，我们将原始波形（以16,000采样率处理为单声道）转换为128个与Kaldi [55]兼容的梅尔频率带，使用25毫秒的汉宁窗，并每10毫秒移动一次。对于AudioSet中的10秒录音，生成的谱图为1×1024×128维度。</p>
<p>对于块嵌入，我们使用大小为（16, 16）的卷积核，分别在时间和频率方向上进行步幅处理（因此，块之间不重叠），以避免在自监督学习中通过重叠产生捷径（尽管在高掩蔽比率下，这种捷径较不严重）。默认情况下，我们对预训练使用0.8的掩蔽比率（无结构的随机掩蔽）。在微调阶段，我们使用较低的掩蔽比率（时间方向和频率方向各为0.3）。这些设计选择的消融实验见§4.4。</p>
<h3 id="4-3-预训练和微调">4.3 预训练和微调</h3>
<p>我们使用AudioSet-2M进行预训练，并随机遍历所有音频录音。我们训练32个周期，批量大小为512，学习率为0.0002。训练负载分布在64个V100 GPU上，总训练时间约为36小时。对于每个音频，我们随机抽取开始时间，周期性地提取10秒音频，并随机调整其幅度，最多±6dB。我们仅使用自然音频谱图，并且不应用增强（例如[48, 56, 57]），因为我们没有发现这些强增强方法在预训练阶段有显著帮助。</p>
<p>在微调阶段，我们去除解码器，仅微调编码器。对于AudioSet-2M上的监督微调，由于训练样本在各类之间的不均衡（不平衡），我们遵循常见做法，在训练过程中使用加权抽样来平衡各类。在每个周期中，我们无替换地抽取200K实例（约占AudioSet-2M的10%）。我们微调100个周期，相当于AudioSet-2M的约10个完整周期。实例抽样的概率与其类别在数据集中的出现频率成反比。微调使用64个GPU，耗时约12小时。对于较小的平衡AudioSet-20K，我们在4个GPU上微调60个周期，并且不使用加权抽样。其他数据集的详细信息请见补充材料。</p>
<h3 id="4-4-消融实验和模型特性">4.4 消融实验和模型特性</h3>
<h4 id="预训练和微调中的掩蔽策略">预训练和微调中的掩蔽策略</h4>
<p>在图4中，我们比较了不同的预训练和微调掩蔽策略。</p>
<ul>
<li>
<p><strong>图4a</strong>探讨了预训练掩蔽比率。我们观察到，类似于MAE在图像中的表现 [1]，高的预训练掩蔽比率（在我们实验中为80%）对于音频谱图是最优的。这是因为音频谱图和图像都是具有显著冗余的连续信号。此外，我们发现无结构的随机掩蔽在自监督预训练中表现最好，相比之下，结构化掩蔽（如时间+频率掩蔽）效果较差。<br>
与图像的MAE不同，音频谱图的预训练掩蔽策略之间存在显著的性能差异。我们比较了图6a到6e与6d到6h的重建结果，在相同掩蔽比率下，我们观察到无结构的随机掩蔽比结构化掩蔽（即时间和/或频率）更容易，因为模型可以通过外推附近的上下文（例如，元音中的共鸣峰和辅音中的摩擦音）来推测缺失的成分。我们还发现，对于较高的掩蔽比率，结构化掩蔽的性能下降，可能是因为任务变得过于困难，而随机掩蔽在掩蔽比率达到80%时性能稳步提升。这个结果表明，为有效进行自监督音频表示学习，设计合适的任务难度非常重要。因此，我们选择80%的掩蔽比率作为预训练的默认设置。</p>
</li>
<li>
<p><strong>图4b</strong>研究了微调阶段掩蔽的效果。我们发现，微调阶段使用结构化掩蔽（时间+频率）比时间掩蔽或频率掩蔽更有益，这些掩蔽策略比无结构掩蔽效果更好。总体而言，微调阶段的最佳掩蔽比率低于预训练阶段，因此我们在微调阶段使用0.3作为默认掩蔽比率。</p>
</li>
</ul>
<h4 id="块大小和步幅的影响">块大小和步幅的影响</h4>
<p>我们在表1a中比较了使用不同块大小和步幅训练的Audio-MAE的性能。我们观察到，当块之间存在非零重叠（即步幅&lt;块大小）时，会增加块的数量，并且浮点计算（FLOPs）呈二次增长。大多数先前的工作（如AST [10]）使用重叠块（块=16，步幅=10）来提升任务性能。表1a显示，我们没有观察到重叠块能提高Audio-MAE的性能（两者均为47.3 mAP），可能是因为重叠会导致块嵌入信息泄漏到掩蔽的块中。非重叠的16×16块在计算和性能之间取得了良好的平衡。默认情况下，我们在实验中使用这一设置。</p>
<h4 id="编码器-2">编码器</h4>
<p>我们研究了Audio-MAE中编码器和解码器架构的设计选择。表1b展示了编码器模型大小与性能之间的权衡。正如预期的那样，较大的模型能够获得更好的性能，但代价是计算和内存的增加。ViT-L相比ViT-B/S在较小且平衡的AS-20K上的性能提升更为显著。对于ViT-S，在更多领域内数据（AS-20K → AS-2M）的微调下，ViT-B与ViT-S之间的性能差距可以显著缩小（5.0 → 2.3 mAP）。</p>
<h4 id="解码器">解码器</h4>
<p>表1c比较了Audio-MAE中不同的解码器注意力类型。需要注意的是，解码器在预训练后会被丢弃，只有相同大小的ViT-B编码器会在最终任务上进行微调。我们的结果显示，带有移位窗口的局部注意力取得了最佳性能。结合局部和全局注意力（即混合注意力）也改进了原始的全局自注意力。图5展示了定性重建对比。在元音的谱图中，使用局部注意力的解码器能够更好地重建谐波，并恢复谱图中的更多上下文。在辅音的摩擦音中也观察到了类似的现象。</p>
<h4 id="解码器深度的影响">解码器深度的影响</h4>
<p>表1d研究了解码器深度对mAP的影响。较深的16层解码器在性能上优于较浅的变体。需要注意的是，我们的解码器默认使用局部窗口注意力，其中只有一部分标记（4×4局部窗口对比全局注意力中的64×8）会被关注。对于全局注意力，我们发现8层解码器比16层的性能更好。</p>
<h4 id="解码器宽度（嵌入维度）">解码器宽度（嵌入维度）</h4>
<p>表1e比较了解码器宽度（嵌入维度）。512维的解码器在计算和性能之间取得了良好的平衡，因为更宽的解码器并没有带来更好的效果。</p>
<h4 id="预训练数据和设置">预训练数据和设置</h4>
<p>表1f总结了预训练数据集大小的影响。总体来说，使用更多数据进行预训练时，模型性能单调增加。比较使用1%良好标注的AS-20K平衡数据与使用随机采样的20K不平衡数据进行预训练的性能，类似的mAP（39.4 vs 39.6）表明数据类别的分布（平衡与不平衡）对预训练的重要性较小。与此同时，表1g显示，训练更长时间是有益的，但在第24个周期后性能趋于饱和。</p>
<h4 id="跨领域预训练（ImageNet）">跨领域预训练（ImageNet）</h4>
<p>初始化音频模型时使用ImageNet预训练的权重已成为音频分类中的一种流行做法。然而，由于图像和音频模态之间存在显著差异，是否跨领域预训练对音频表示学习有益仍然值得质疑。在表1h中，我们设计了3种方案来调查这一问题：（1）从头开始进行音频专用预训练（AS-SSL）。我们认为这是学习音频表示的理想方案，因为它是一个简单且干净的设置，避免了从其他模态转移的不可控偏差。（2）直接使用自监督的ImageNet MAE模型（IN-SSL）及其微调变体（IN-SL）。(3) 在这些ImageNet权重上进行Audio-MAE自监督预训练。</p>
<p>结果表明，（1）从头开始的音频专用预训练是最好的。对于方案（2）和（3），我们观察到仅使用ImageNet预训练（2）是不足够的（尤其是在下游数据较小的情况下，如AS-20K），而在AudioSet上进行的自监督预训练中，ImageNet初始化（3）并没有带来帮助，反而导致准确度下降。在（3）中，ImageNet监督预训练（IN-SL）似乎有害。因此，结果表明，跨领域预训练（即ImageNet）对Audio-MAE并没有帮助，可能是由于领域偏移的原因。</p>
<h3 id="4-5-与最先进方法的比较">4.5 与最先进方法的比较</h3>
<p>表2将Audio-MAE（带有3次运行误差条）与先前的最先进方法进行了比较。我们将比较分为3组。为了公平比较，我们的主要基准是中间组中自监督预训练于领域内（音频）数据集（AudioSet和LibriSpeech）的模型。为了参考，我们还列出了没有预训练的其他模型（顶部组）以及在ImageNet上进行监督预训练的其他模型（底部组），后者包含了数据集上先前最佳的系统。</p>
<p>在AudioSet预训练下，Audio-MAE在所有任务中相较于其他领域内自监督预训练的模型，取得了最佳表现。在AudioSet-20K上，它的37.1 mAP显著超过了所有其他方法，包括同时进行的工作和使用跨领域预训练的其他模型。在AudioSet-2M和ESC-50上，我们的方法也超越了Conformer [37] 和 SS-AST [18]。特别值得注意的是，与SS-AST和同时进行的MAE-AST [38]不同，后者使用了Librispeech中的1,000小时语音进行训练，而我们仅使用AudioSet进行预训练。</p>
<p>在表2的底部组中，Audio-MAE也超越了先前使用ImageNet监督预训练的最先进模型。需要注意的是，提出的Audio-MAE并不依赖任何跨领域数据和标签，也没有使用从额外CNN模型（如DeiT）中进行的知识蒸馏。与HTS-AT [29] 和 PaSST [28]相比，Audio-MAE是在16K采样率的音频上进行训练的。根据[59]的实验，如果使用32K采样率的音频，Audio-MAE</p>
<p>的性能可能会提高0.4 mAP。</p>
<p>对于语音任务（SPC-1, SPC-2 和 SID），Audio-MAE优于其他没有预训练的模型（ERANN [58]、PANN [59]），以及监督和自监督的模型（SS-AST, MAE-AST）。我们还列出了其他工作（标记为*），以包括最新的结果，这些结果是在SUPERB [53]基准中引入的。但需要注意的是，这些结果不完全可比，因为SUPERB使用了线性评估，其中基础预训练模型并未进行端到端微调。</p>
<p>总之，借助AudioSet的音频专用从头预训练，Audio-MAE在音频和语音分类任务中都表现优异。</p>
<h3 id="4-6-Audio-MAE解码器的可视化和听觉示例">4.6 Audio-MAE解码器的可视化和听觉示例</h3>
<p>为了更好的可视化，我们遵循MAE [1]的做法，使用均方误差（MSE）来作为非归一化谱图的自监督目标。我们使用ViT-L作为Audio-MAE编码器进行可视化。图6展示了从AudioSet-2M评估集采样的重建结果。我们进一步使用Griffin-Lim [61]算法重建.wav文件，并通过匿名链接提供可听示例。</p>
<p>从图中可以看出，在不同的掩蔽策略和不同的声音下，Audio-MAE能够生成合理的重建。它对嘈杂的事件声音（例如，图6c-3中的重建警报器）以及语音和音乐（例如，图6b-3中的重建歌声）都有较好的表现。值得注意的是，与通常在视觉内容中具有尺度/平移/位置不变性 [19] 的情况不同，音频谱图组件的位置和排列对人类理解声音至关重要 [62]。例如，音调的变化会使音频听起来完全不同。此外，语音中的音素序列对于理解语音也至关重要。因此，无结构掩蔽产生的输出更接近真实值（每个子图的顶部行），因为模型可以基于相邻的谱图块做出更好的预测；而结构化掩蔽则更加困难（准确度较差或缺少单词），特别是当掩蔽在时间轴上进行时。在图6e-3中，我们展示了一个失败示例（缺失单词）的重建语音。</p>
<h2 id="5-结论">5. 结论</h2>
<p>我们探索了将MAE [1] 方法简单扩展到音频数据的应用。我们的Audio-MAE学习从音频录音中重建掩蔽的谱图块，并在六个音频和语音分类任务上取得了最先进的性能。我们得出四个有趣的观察结果：</p>
<ol>
<li><strong>简单的MAE方法在音频谱图中表现出色</strong>：我们惊讶地发现，简单的MAE方法在音频谱图中效果非常好。</li>
<li><strong>局部自注意力在解码器中的重要性</strong>：我们发现，通过在解码器中引入局部自注意力，可以学习到更强的表示。</li>
<li><strong>掩蔽策略在预训练和微调中的应用</strong>：我们展示了掩蔽可以应用于预训练和微调，既提高了准确性，又减少了训练计算量。最佳策略取决于数据的性质（音频、图像等）和学习类型（自监督/监督）。</li>
<li><strong>同一模态下的预训练和微调能获得最佳性能</strong>：最佳性能的实现是通过在同一模态下进行预训练和微调，而不依赖于跨模态的迁移学习。</li>
</ol>
<p>在未来的工作中，我们计划探索多模态自监督学习，采用音视频联合的MAE方法，因为这些领域在视频数据中具有自然的对应关系。</p>
<h2 id="附录">附录</h2>
<p>附录组织如下：</p>
<ul>
<li><strong>§A</strong>：我们首先展示了更多的可听可视化，提供匿名的URL链接供查看。</li>
<li><strong>§B</strong>：我们提供了完整的实验细节和每个数据集的预训练和微调的超参数配置。</li>
<li><strong>§C</strong>：我们在ESC-50数据集上进行了额外的实验（§C.1），并在AudioSet上进行了额外的监督预训练，以完成与主论文中表2中标记为†的模型的比较。接着，我们研究了Audio-MAE如何应用于实际的语音生成任务（§C.2）；并分享了我们尝试过但效果不佳的负面结果和见解（§C.3）。</li>
<li><strong>§D</strong>：最后，我们讨论了Audio-MAE的局限性。</li>
</ul>
<h3 id="A-通过Audio-Mae-Decoder获得额外的重建细节和结果">A 通过Audio - Mae Decoder获得额外的重建细节和结果</h3>
<p>​	图7展示了在AudioSet-2M评估集上的额外重建结果。可听示例可以通过点击各自的匿名链接（1、2、3）进行访问。（1是地面真实参考，2是Audio-MAE的掩蔽输入，3是Audio-MAE生成的重建输出。）</p>
<p>我们使用带有ViT-L编码器和16层局部注意力解码器的Audio-MAE模型进行可视化。模型在AudioSet上使用80%的无结构（随机）掩蔽进行训练。我们逆转梅尔谱图并利用Griffin-Lim [61]算法重建波形。由于[61]中的相位估计不完美，可能会出现可感知的伪影。请注意，图7中的默认掩蔽比率为70%，以便更好地可视化。我们还展示了在80%掩蔽比率下的重建结果（图7e-7h），供比较。</p>
<p>比较图7中每个标题下的2和3，即使在70%-80%的掩蔽比率下，Audio-MAE仍能生成合理的重建。由于其相对可预测的谱图模式，音乐和事件声音对于Audio-MAE来说较容易处理。例如，时间域中的重复节奏（例如，图7b和图7l中的音乐）和频率域中的谐波（例如，图7c中的警报声和图7d中的象鸣声）都得到了很好的重建。语音录音则更具挑战性，如图7a和图7e所示。</p>
<p>在大多数情况下，Audio-MAE成功地从掩蔽/损坏的输入中恢复了音频。凭借这些鼓舞人心的结果，我们设想Audio-MAE也可以应用于其他语音生成任务，并在§C.2中进行定性案例研究。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241119215253187.png" alt="image-20241119215253187"></p>
<h2 id="B-实验细节和Hyper参数设置">B 实验细节和Hyper参数设置</h2>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Gavin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/">http://example.com/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Gavin</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/MAE/">MAE</a></div><div class="post_share"><div class="social-share" data-image="https://i.ibb.co/9HZr35S/image.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ALanguage%20Quantized%20AutoEncoders%EF%BC%9ATowards%20Unsupervised%20Text-Image%20Alignment/" title="文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment"><img class="cover" src="https://i.ibb.co/zGtqjf2/image.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment</div></div></a></div><div class="next-post pull-right"><a href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners"><img class="cover" src="https://i.ibb.co/gD7cJGh/image.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">文献阅读: Masked Autoencoders Are Scalable Vision Learners</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.ibb.co/0fP2mb0/image.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Gavin</div><div class="author-info__description">我的过去常常追赶着我</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/alexandergwm"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/alexandergwm" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:wenmiaogao@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">May you stay young forever, do the thing in your own zone.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text">1. 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">3.</span> <span class="toc-text">2. 相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%86%E8%A7%89%E6%8E%A9%E8%94%BD%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">3.1.</span> <span class="toc-text">视觉掩蔽预训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%A8%E9%A2%86%E5%9F%9F%E9%9F%B3%E9%A2%91%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">3.2.</span> <span class="toc-text">跨领域音频预训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9F%B3%E9%A2%91%E9%A2%86%E5%9F%9F%E5%86%85%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">3.3.</span> <span class="toc-text">音频领域内预训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Audio%E6%8E%A9%E7%A0%81%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Audio-MAE"><span class="toc-number">4.</span> <span class="toc-text">3. Audio掩码自编码器 (Audio-MAE)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Audio-MAE%E6%A6%82%E8%BF%B0"><span class="toc-number">4.1.</span> <span class="toc-text">Audio-MAE概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%B1%E5%9B%BE%E5%9D%97%E5%B5%8C%E5%85%A5"><span class="toc-number">4.2.</span> <span class="toc-text">谱图块嵌入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A9%E8%94%BD%E7%AD%96%E7%95%A5"><span class="toc-number">4.3.</span> <span class="toc-text">掩蔽策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">4.4.</span> <span class="toc-text">编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%A6%E5%B1%80%E5%B1%80%E9%83%A8%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">4.5.</span> <span class="toc-text">带局局部注意力的解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E9%9F%B3%E9%A2%91%E8%B0%B1%E5%9B%BE%E7%9A%84%E8%A7%A3%E7%A0%81%E5%99%A8%E5%A2%9E%E5%BC%BA"><span class="toc-number">4.6.</span> <span class="toc-text">处理音频谱图的解码器增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87"><span class="toc-number">4.7.</span> <span class="toc-text">目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%BE%AE%E8%B0%83"><span class="toc-number">4.8.</span> <span class="toc-text">下游任务的微调</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.</span> <span class="toc-text">4. 实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E4%BB%BB%E5%8A%A1"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 数据集和任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 实现细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E9%A2%84%E8%AE%AD%E7%BB%83%E5%92%8C%E5%BE%AE%E8%B0%83"><span class="toc-number">5.3.</span> <span class="toc-text">4.3 预训练和微调</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C%E5%92%8C%E6%A8%A1%E5%9E%8B%E7%89%B9%E6%80%A7"><span class="toc-number">5.4.</span> <span class="toc-text">4.4 消融实验和模型特性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E5%92%8C%E5%BE%AE%E8%B0%83%E4%B8%AD%E7%9A%84%E6%8E%A9%E8%94%BD%E7%AD%96%E7%95%A5"><span class="toc-number">5.4.1.</span> <span class="toc-text">预训练和微调中的掩蔽策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9D%97%E5%A4%A7%E5%B0%8F%E5%92%8C%E6%AD%A5%E5%B9%85%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">5.4.2.</span> <span class="toc-text">块大小和步幅的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-2"><span class="toc-number">5.4.3.</span> <span class="toc-text">编码器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">5.4.4.</span> <span class="toc-text">解码器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8%E6%B7%B1%E5%BA%A6%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">5.4.5.</span> <span class="toc-text">解码器深度的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8%E5%AE%BD%E5%BA%A6%EF%BC%88%E5%B5%8C%E5%85%A5%E7%BB%B4%E5%BA%A6%EF%BC%89"><span class="toc-number">5.4.6.</span> <span class="toc-text">解码器宽度（嵌入维度）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%92%8C%E8%AE%BE%E7%BD%AE"><span class="toc-number">5.4.7.</span> <span class="toc-text">预训练数据和设置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B7%A8%E9%A2%86%E5%9F%9F%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%88ImageNet%EF%BC%89"><span class="toc-number">5.4.8.</span> <span class="toc-text">跨领域预训练（ImageNet）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E4%B8%8E%E6%9C%80%E5%85%88%E8%BF%9B%E6%96%B9%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">5.5.</span> <span class="toc-text">4.5 与最先进方法的比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-Audio-MAE%E8%A7%A3%E7%A0%81%E5%99%A8%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E5%92%8C%E5%90%AC%E8%A7%89%E7%A4%BA%E4%BE%8B"><span class="toc-number">5.6.</span> <span class="toc-text">4.6 Audio-MAE解码器的可视化和听觉示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E7%BB%93%E8%AE%BA"><span class="toc-number">6.</span> <span class="toc-text">5. 结论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-number">7.</span> <span class="toc-text">附录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E9%80%9A%E8%BF%87Audio-Mae-Decoder%E8%8E%B7%E5%BE%97%E9%A2%9D%E5%A4%96%E7%9A%84%E9%87%8D%E5%BB%BA%E7%BB%86%E8%8A%82%E5%92%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">7.1.</span> <span class="toc-text">A 通过Audio - Mae Decoder获得额外的重建细节和结果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#B-%E5%AE%9E%E9%AA%8C%E7%BB%86%E8%8A%82%E5%92%8CHyper%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">8.</span> <span class="toc-text">B 实验细节和Hyper参数设置</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/12/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AA%20Hybrid%20DSP:Deep%20Learning%20Approach%20to%20%20Real-Time%20Full-Band%20Speech%20Enhancement/" title="文献阅读: A Hybrid DSP/Deep Learning Approach to  Real-Time Full-Band Speech Enhancement"><img src="https://i.ibb.co/VYX8yFsD/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: A Hybrid DSP/Deep Learning Approach to  Real-Time Full-Band Speech Enhancement"/></a><div class="content"><a class="title" href="/2024/12/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AA%20Hybrid%20DSP:Deep%20Learning%20Approach%20to%20%20Real-Time%20Full-Band%20Speech%20Enhancement/" title="文献阅读: A Hybrid DSP/Deep Learning Approach to  Real-Time Full-Band Speech Enhancement">文献阅读: A Hybrid DSP/Deep Learning Approach to  Real-Time Full-Band Speech Enhancement</a><time datetime="2024-12-15T04:12:53.312Z" title="发表于 2024-12-15 12:12:53">2024-12-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/01/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ABAT%EF%BC%9ALearning%20to%20reason%20about%20Spatial%20Sounds%20with%20Large%20Language%20Models/" title="文献阅读: BAT: Learning to Reason about Spatial Sounds with Large Language Models"><img src="https://i.ibb.co/bXjJh1J/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: BAT: Learning to Reason about Spatial Sounds with Large Language Models"/></a><div class="content"><a class="title" href="/2024/12/01/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ABAT%EF%BC%9ALearning%20to%20reason%20about%20Spatial%20Sounds%20with%20Large%20Language%20Models/" title="文献阅读: BAT: Learning to Reason about Spatial Sounds with Large Language Models">文献阅读: BAT: Learning to Reason about Spatial Sounds with Large Language Models</a><time datetime="2024-12-01T12:04:59.068Z" title="发表于 2024-12-01 20:04:59">2024-12-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ALanguage%20Quantized%20AutoEncoders%EF%BC%9ATowards%20Unsupervised%20Text-Image%20Alignment/" title="文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment"><img src="https://i.ibb.co/zGtqjf2/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment"/></a><div class="content"><a class="title" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ALanguage%20Quantized%20AutoEncoders%EF%BC%9ATowards%20Unsupervised%20Text-Image%20Alignment/" title="文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment">文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment</a><time datetime="2024-11-19T14:09:10.711Z" title="发表于 2024-11-19 22:09:10">2024-11-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/" title="文献阅读: Masked Autoencoders that Listen"><img src="https://i.ibb.co/9HZr35S/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Masked Autoencoders that Listen"/></a><div class="content"><a class="title" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/" title="文献阅读: Masked Autoencoders that Listen">文献阅读: Masked Autoencoders that Listen</a><time datetime="2024-11-19T13:34:57.816Z" title="发表于 2024-11-19 21:34:57">2024-11-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners"><img src="https://i.ibb.co/gD7cJGh/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Masked Autoencoders Are Scalable Vision Learners"/></a><div class="content"><a class="title" href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners">文献阅读: Masked Autoencoders Are Scalable Vision Learners</a><time datetime="2024-11-17T12:30:01.804Z" title="发表于 2024-11-17 20:30:01">2024-11-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Gavin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Have a nice day!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>