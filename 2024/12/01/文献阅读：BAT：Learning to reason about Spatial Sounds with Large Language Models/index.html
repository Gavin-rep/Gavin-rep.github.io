<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>文献阅读: BAT: Learning to Reason about Spatial Sounds with Large Language Models | Gavin</title><meta name="author" content="Gavin"><meta name="copyright" content="Gavin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Arxiv  25 May 2024.  Texas at Austin 代码：https:&#x2F;&#x2F;github.com&#x2F;zszheng147&#x2F;Spatial-AST 摘要空间声音推理是一项基本的人类技能，使我们能够基于声音导航和理解周围环境。在本文中，我们提出了BAT（Binaural Acoustic Transformer），它结合了双耳声学场景分析模型的空间声音感知能力与大型语言模型（LLM）">
<meta property="og:type" content="article">
<meta property="og:title" content="文献阅读: BAT: Learning to Reason about Spatial Sounds with Large Language Models">
<meta property="og:url" content="http://example.com/2024/12/01/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ABAT%EF%BC%9ALearning%20to%20reason%20about%20Spatial%20Sounds%20with%20Large%20Language%20Models/index.html">
<meta property="og:site_name" content="Gavin">
<meta property="og:description" content="Arxiv  25 May 2024.  Texas at Austin 代码：https:&#x2F;&#x2F;github.com&#x2F;zszheng147&#x2F;Spatial-AST 摘要空间声音推理是一项基本的人类技能，使我们能够基于声音导航和理解周围环境。在本文中，我们提出了BAT（Binaural Acoustic Transformer），它结合了双耳声学场景分析模型的空间声音感知能力与大型语言模型（LLM）">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.ibb.co/bXjJh1J/image.png">
<meta property="article:published_time" content="2024-12-01T12:04:59.068Z">
<meta property="article:modified_time" content="2024-12-01T13:59:41.327Z">
<meta property="article:author" content="Gavin">
<meta property="article:tag" content="声源定位">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.ibb.co/bXjJh1J/image.png"><link rel="shortcut icon" href="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"><link rel="canonical" href="http://example.com/2024/12/01/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ABAT%EF%BC%9ALearning%20to%20reason%20about%20Spatial%20Sounds%20with%20Large%20Language%20Models/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '文献阅读: BAT: Learning to Reason about Spatial Sounds with Large Language Models',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-01 21:59:41'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.ibb.co/0fP2mb0/image.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.ibb.co/bXjJh1J/image.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Gavin"><img class="site-icon" src="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"/><span class="site-name">Gavin</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">文献阅读: BAT: Learning to Reason about Spatial Sounds with Large Language Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-01T12:04:59.068Z" title="发表于 2024-12-01 20:04:59">2024-12-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-01T13:59:41.327Z" title="更新于 2024-12-01 21:59:41">2024-12-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="文献阅读: BAT: Learning to Reason about Spatial Sounds with Large Language Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Arxiv  25 May 2024.  Texas at Austin</p>
<p>代码：<a target="_blank" rel="noopener" href="https://github.com/zszheng147/Spatial-AST">https://github.com/zszheng147/Spatial-AST</a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>空间声音推理是一项基本的人类技能，使我们能够基于声音导航和理解周围环境。在本文中，我们提出了BAT（Binaural Acoustic Transformer），它结合了双耳声学场景分析模型的空间声音感知能力与大型语言模型（LLM）的自然语言推理能力，以复制这一天生的技能。为了解决缺乏真实场景空间声音数据集的问题，我们利用AudioSet和SoundSpaces 2.0合成了一个双耳音频数据集。随后，我们开发了SPATIALSOUNDQA，这是一个基于空间声音的问答数据集，提供了一系列问答任务，用于训练BAT在空间声音感知和推理的各个方面的能力。</p>
<p>BAT的声学前端编码器是一个新颖的空间音频编码器，名为Spatial Audio Spectrogram Transformer（SPATIAL-AST）。该编码器本身在声音事件检测、空间定位和距离估计方面表现出色。通过将SPATIAL-AST与LLaMA-2 7B模型结合，BAT超越了传统的声音事件定位和检测（SELD）任务，使模型能够推理其环境中声音之间的关系。我们的实验表明，BAT在空间声音感知和推理方面表现卓越，展示了LLM在复杂空间音频环境中导航和理解的巨大潜力。我们的演示、数据集、代码和模型权重可在以下地址获取：<a target="_blank" rel="noopener" href="https://zhishengzheng.com/BAT%E3%80%82">https://zhishengzheng.com/BAT。</a></p>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h2><p>大型语言模型（LLMs）的发展使其应用范围超越了文本数据，催生了能够执行图像理解任务（如视觉问答和图像描述）的多模态语言模型（Li等，2023；Liu等，2023；OpenAI，2023；Peng等，2024）。其他研究则将声音感知能力整合到LLMs中，使其能够执行音频问答、语音识别、语音翻译和语音合成等任务（Deshmukh等，2023；Wang等，2023a；Chu等，2023；Borsos等，2023；Gong等，2024）。然而，尽管这些基于LLM的模型在处理大量视觉和音频任务时表现出色，目前尚无模型能够处理真实环境中的空间音频输入。人类具有双耳听觉能力，不仅能够识别听到的声音类型，还能够推断声音的距离、方向以及是否存在来自不同位置的多个声音源。考虑以下场景：定位楼上传来的呼喊声、听到沙发后面传来的手机铃声，或感知身后传来的脚步声。这些情况都需要空间音频感知和推理能力，而上述模型目前尚未具备这一能力。</p>
<p>为弥补这一空缺，我们提出了BAT，这是第一个基于空间音频的大型语言模型，旨在对三维环境中的声音进行推理。鉴于当前缺乏大规模的真实场景空间音频和基于空间音频问答的数据集，我们利用AudioSet（Gemmeke等，2017）的音频片段作为声源，并使用SoundSpaces 2.0（Chen等，2022）模拟多样化的三维混响声学环境，合成了一个大规模的双耳音频数据集。结合该数据集，我们开发了SPATIALSOUNDQA，这是一个包含多种问答任务的集合，旨在训练和评估模型对不同复杂性水平的空间音频的理解能力。</p>
<p>BAT的成功训练和评估依赖于能够准确编码丰富的空间音频信息。然而，现有的编码器通常仅专注于单耳事件检测（Baade等，2022；Huang等，2022；Chen等，2023；2024b）或局限于一阶声全景（Shimada等，2021；Wang等，2023b；Kim等，2023），仅涵盖有限范围的声音事件。其他方法则专注于特定多通道格式下的到达方向（DoA）估计（Yang等，2022a；Wang等，2023c），缺乏全面处理空间音频理解任务的能力。为了解决这些局限性，我们开发了SPATIAL-AST，这是一种新颖的多任务空间编码器，不仅能够执行声音事件检测和空间定位，还在距离感知方面表现出色，为空间音频任务提供了全面的解决方案。</p>
<p>实验结果表明，SPATIAL-AST在多项任务中既高效又参数节约。通过多阶段训练课程，SPATIAL-AST编码器本身在多项任务中表现优异：音频事件分类的平均精度（mAP）达到50.03%，到达方向估计的平均角度误差（MAE）为17.94°，距离估计在实际位置0.5米内的距离误差率（DER）为32.54%。通过将SPATIAL-AST编码器与LLaMA-2（Touvron等，2023b）结合，并采用合适的训练课程，我们的BAT不仅在感知任务中表现卓越，还在混合声源场景中的空间推理能力（如“立体声音响是否位于狗叫声的左侧？”）上表现出色，在空间声音推理问答中达到了76.89%的准确率。</p>
<p>我们的主要贡献总结如下：</p>
<ul>
<li><p><strong>我们提出了第一个基于空间音频的问答数据集SPATIALSOUNDQA</strong>，提供了一系列从感知到推理的三维音频理解任务，为训练模型执行空间音频理解任务提供了一个平台。</p>
</li>
<li><p><strong>我们提出了SPATIAL-AST</strong>，这是一种双耳空间音频编码器架构，能够联合执行声音事件检测、空间定位和距离估计，在这三个任务上均表现出色。</p>
</li>
<li><p><strong>我们引入了BAT</strong>，将SPATIAL-AST与LLaMA-2大型语言模型结合，最终实现了一个能够回答三维环境中多声源复杂推理问题的模型。</p>
</li>
</ul>
<h2 id="2-相关任务"><a href="#2-相关任务" class="headerlink" title="2. 相关任务"></a>2. 相关任务</h2><h3 id="2-1-空间音频"><a href="#2-1-空间音频" class="headerlink" title="2.1 空间音频"></a>2.1 空间音频</h3><p>空间音频是一系列用于创造听者周围三维空间中声音源幻觉的技术。从传统的立体声和环绕声系统到最近的全景声方法，空间音频被广泛应用于从虚拟现实（Mystakidis, 2022）到先进剧院系统等领域。在人工智能和机器学习领域，空间音频为位于三维物理空间中的智能体在准确定位和解释声音源方面带来了独特的挑战（Evers等，2020；Guizzo等，2022）。为了解决这些挑战，研究人员开发了声学仿真技术（Scheibler等，2018；Chen等，2022）和利用空间音频的算法（Yang等，2022b；Wang等，2023c）。</p>
<p>现有的空间音频数据集，如YouTube-360（Morgado等，2020）、YouTube-ASMR（Yang等，2020）、Pano-AVQA（Yun等，2021）和STARSS23（Shimada等，2023），提供了不同程度的空间音频信息。然而，这些数据集往往存在质量不一致、缺乏关键的真值标签（如声音源的距离和方向），以及覆盖范围有限等问题，这阻碍了基于空间音频的机器学习模型的发展。</p>
<h3 id="2-2-声音事件定位与检测"><a href="#2-2-声音事件定位与检测" class="headerlink" title="2.2 声音事件定位与检测"></a>2.2 声音事件定位与检测</h3><p>声音事件定位与检测（Sound Event Localization and Detection, SELD）任务（Adavanne等，2018）将声音源定位与声音事件检测（Sound Event Detection, SED）相结合，该任务后来被DCASE社区纳入其挑战的第3任务。后续研究（Wang等，2020；Zhang等，2021；Wang等，2023b）对GRU（Cho等，2014）、TDNNF（Povey等，2018）和Conformer（Gulati等，2020）等架构进行了改进并应用于SELD任务，取得了优异的性能。</p>
<p>然而，这些模型主要专注于浅层的空间音频感知，而利用大型语言模型（LLMs）实现空间推理的潜力在以往的研究中尚未被探索。这正是我们研究的核心重点所在。</p>
<h3 id="2-3-多模态大语言模型"><a href="#2-3-多模态大语言模型" class="headerlink" title="2.3 多模态大语言模型"></a>2.3 多模态大语言模型</h3><p>近期的模型（OpenAI, 2023；Li等, 2023；Liu等, 2023；Peng等, 2024）为大型语言模型（LLMs）赋予了对图像和视频进行推理和生成的能力。在音频领域，AudioGPT（Huang等, 2023）整合了ChatGPT作为多功能接口，用于处理广泛的音频和语音应用。Pengi（Deshmukh等, 2023）提出使用迁移学习，将所有音频任务框架化为文本生成问题，利用音频和文本输入生成结果，而无需额外的微调。LTU（Gong等, 2024）提出了一个单声道音频问答数据集，并结合音频频谱转换器（AST，Gong等, 2021）特征提取器与LoRA（Hu等, 2022）适配的LLaMA（Touvron等, 2023a）LLM，训练模型对片段中的声音进行推理和回答问题。然而，LTU与我们的工作有显著不同，LTU仅在单声道声音数据上训练，而我们的研究聚焦于通过混响的三维环境渲染的多通道空间声音。</p>
<p>此外，SALMONN（Tang等, 2024）使用OpenAI的Whisper模型（Radford等, 2023）和BEATs（Chen等, 2023）作为双听觉编码器，分别提取语音和音频表征，然后将其连接到LLMs，以实现响应生成。Qwen-audio（Chu等, 2023）专注于利用大规模数据集训练基于Whisper的音频编码器，在各种任务和音频类型中实现了卓越的通用音频理解能力。</p>
<p>尽管这些模型在音频领域表现出色，但它们均缺乏对复杂、混响、多样化的三维环境中空间音频进行感知和推理的能力。</p>
<h2 id="3-SPATIALSOUNDQA"><a href="#3-SPATIALSOUNDQA" class="headerlink" title="3 SPATIALSOUNDQA"></a>3 SPATIALSOUNDQA</h2><p>BAT 的训练需要一个基于空间音频的问答数据集。然而，目前的数据集，如 SpatialVLM（Chen 等，2024a）和 Pano-AVQA（Yun 等，2021），主要聚焦于视觉，缺乏空间音频信息（或完全没有音频）。为填补这一空白，我们提出了 <strong>SPATIALSOUNDQA</strong>，这是第一个基于空间音频的问答数据集，专为突出空间音频的复杂性而设计，完全不依赖视觉信息，并满足空间音频感知与推理的独特需求。</p>
<h3 id="3-1-空间音频的生成"><a href="#3-1-空间音频的生成" class="headerlink" title="3.1 空间音频的生成"></a>3.1 空间音频的生成</h3><p>在现实世界中收集和标注空间音频是一项耗时的任务，且由于环境声学的多变性和录音设备的限制，增加了其复杂性。为了高效创建一个在环境和声音源类型上具有高度多样性，同时包含丰富的关于这些声音源的真值元数据的数据集，我们采用了基于仿真的方法来生成数据集的空间音频数据。</p>
<p><strong>空间音频仿真器</strong><br>我们使用最先进的音频仿真平台 SoundSpaces 2.0（Chen 等，2022）。该平台基于几何的实时声音渲染，能够实现真实的声学混响，适用于任意的声源-接收器位置。许多仿真参数可以轻松修改，例如房间墙壁和内部物体的材质属性，以及接收器麦克风阵列的几何结构。这使我们能够创建一个多样化且高度真实的数据集，同时还可以轻松访问真值声音生成参数，例如环境中每个声源的空间位置和方向。</p>
<p>具体而言，我们利用 Matterport3D（Chang 等，2017）作为我们的环境网格数据来源，该数据集包含了 90 个完整建筑物的高精度网格渲染，每个建筑平均包含 24.5 个房间，分布在 2.61 层楼，平均楼层面积为 517.34 平方米。在给定任意声源位置$s$、单声道声源 $A^s$,接收器位置 $r$、接收器朝向 $θ$ 的特定网格环境中，麦克风接收到的音频信号 $A^r$ 是房间脉冲响应与声源信号的卷积，具体公式如下所示：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201204152041.png" alt="image-20241201204152041" style="zoom: 50%;" />

<p>其中$t \in [1,T]$ 表示时间样本index, $m\in[1,2,…,n]$ 表示麦克风index，$R_m(t,s,r,\theta)$ 是声源和接收器之间的脉冲响应。</p>
<p>为了在缺乏视觉线索的情况下最大程度减少听觉定位中的感知失调，我们确保声音源和接收器都位于同一个房间内。接收器的位置设置在适合保持直立姿态的坐标位置，而声音源则随机分布于房间的各个位置。最终，总共生成了 <strong>21,131 个混响</strong>，每个房间平均生成约 <strong>9.58 个混响</strong>。</p>
<p><strong>声音源</strong><br>以往的空间音频数据集（Yang 等，2020；Shimada 等，2023）通常涵盖有限范围的音频事件，通常局限于音乐、语音和家居声音等类别。为了拓宽研究范围，更好地代表现实世界中的声学场景，我们从 AudioSet（Gemmeke 等，2017）中抽样，指定我们的单声道声源 $ A^s$。AudioSet 包含约 200 万个 10 秒的 YouTube 片段，这些片段用于音频分类，每个片段都被弱标注为 527 类音频事件，且常常包含单个片段中的多个重叠事件。然而，在这 527 类事件中，有些类别只能通过视觉线索区分（例如，“单反相机”与“相机”）。</p>
<p>为了增强数据集的可靠性，我们通过人工检查排除了需要视觉信息的标签。此外，为了减小低质量训练样本的影响，我们移除了质量低于 50% 的标签。最终，考虑到输入音频会经过混响卷积，我们进一步排除了大多数与噪声相关的标签，如“噪声”、“回声”以及“户外、城市或人造环境”。经过筛选，我们获得了适合本研究情境的数据集，共包含 355 个仅通过音频线索即可辨别的音频事件标签。</p>
<p>在这些筛选条件下，AudioSet-2M 数据集中剩余 <strong>1,861,750 个片段</strong>，经过进一步筛选的 AudioSet-20K 数据集包含 <strong>18,373 个片段</strong>，我们的评估集包含 <strong>17,148 个片段</strong>。最后，为消除源音频的音量差异，我们对所有片段应用了响度归一化处理。</p>
<h3 id="3-2-问答对生成"><a href="#3-2-问答对生成" class="headerlink" title="3.2 问答对生成"></a>3.2 问答对生成</h3><p>我们设计了 <strong>SPATIALSOUNDQA</strong> 数据集，包含多样化的问答对，所有问题都围绕空间音频感知与推理的挑战展开。与 LTU（Gong 等，2024）类似，数据集中的每个样本结构化为一个三元组 <code>(audio, question, answer)</code>，其中音频和问题作为模型的输入，答案作为目标标签。<strong>表 1</strong> 展示了 SPATIALSOUNDQA 涵盖的任务范围，从基本感知到复杂的空间推理。为增加问题的多样性，这些任务中的问题通过 GPT-4 进行释义，以确保问题种类的广泛性。而答案则通过系统的规则化方法生成，以确保数据集的一致性。以下段落详细列举了各类问题类型。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201205010331.png" alt="image-20241201205010331"></p>
<hr>
<h3 id="声音事件检测（Type-A-和-Type-C）"><a href="#声音事件检测（Type-A-和-Type-C）" class="headerlink" title="声音事件检测（Type A 和 Type C）"></a><strong>声音事件检测（Type A 和 Type C）</strong></h3><p>对于此任务，问题形式为 <strong>“你能在音频记录中检测到哪些声音事件？”</strong> 以及其释义版本。答案由音频片段中出现的声音类别构成，按字母顺序排列。</p>
<ul>
<li><strong>问题类型 A</strong> 使用包含 <strong>1 个声音源</strong> 的音频。  </li>
<li><strong>问题类型 C</strong> 使用包含 <strong>2 个声音源</strong> 的音频。</li>
</ul>
<p>通过这种方式，我们能够覆盖从单声源到多声源的声音事件检测任务。</p>
<h3 id="方向与距离估计（类型-B-和-D）"><a href="#方向与距离估计（类型-B-和-D）" class="headerlink" title="方向与距离估计（类型 B 和 D）"></a><strong>方向与距离估计（类型 B 和 D）</strong></h3><p>此任务旨在确定声音源的方向和距离。为建立真值，三维空间被划分为八个区域。每个区域通过一个三元组定义，表示相对于接收器的方向轴（左&#x2F;右、前&#x2F;后、上&#x2F;下）。此外，距离以 0.5 米为增量，范围从 0 到 10 米。典型问题可能是：“音乐的声音从哪里传来？”并辅以 GPT 生成的释义版本。答案格式限定为“left, front, above; 2.5m”这样的表达式。问题类型 B 使用包含 1 个声音源的音频，而类型 D 使用包含 2 个声音源的音频。</p>
<h3 id="空间推理（类型-E）"><a href="#空间推理（类型-E）" class="headerlink" title="空间推理（类型 E）"></a><strong>空间推理（类型 E）</strong></h3><p>该任务区别于之前专注于感知的任务，加入了同时发生两个声音事件的场景，这些声音来源于不同的距离和方向。在生成这些场景时，我们确保每个混合音频包含两个完全不同的声音事件和方向。此任务的挑战在于辨别这些重叠声音的空间差异。例如，问题可能是：“狗叫声左边的声音是什么？”此任务需要感知和复杂推理能力。模型必须隐式地根据声音类别区分声音源，空间定位每个声音源，然后在问题的上下文中分析这些声音源之间的关系。</p>
<p>然而，编码器 SPATIAL-AST 在预训练阶段未接触过此类混合数据类型，这意味着它缺乏音频分离能力。因此，该任务旨在测试模型的上下文学习能力，并有效利用编码器的潜在表示来解决这一复杂的多源音频推理挑战。为简化评估，我们为这些问题选择了二元（是&#x2F;否）响应格式。通过采用二元答案，我们可以更有效地衡量模型的推理性能，并在不同问题类型间保持一致的评估标准。</p>
<h2 id="4-方法"><a href="#4-方法" class="headerlink" title="4.方法"></a>4.方法</h2><p>图 1 展示了 BAT 模型的整体架构。我们首先介绍 SPATIAL-AST，这是一个新颖的空间音频编码器，作为 BAT 的前端模块。接着，我们讨论如何将其与 LLaMA-2 大型语言模型（LLM）集成，以在 SPATIAL-AST 的听觉感知能力基础上构建空间推理能力。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201210348337.png" alt="image-20241201210348337"></p>
<h3 id="4-1-空间音频编码器：Spatial-Ast"><a href="#4-1-空间音频编码器：Spatial-Ast" class="headerlink" title="4.1 空间音频编码器：Spatial-Ast"></a>4.1 空间音频编码器：Spatial-Ast</h3><p>我们提出了一种新的架构SPATIAL - AST来捕获空间音频信息。该模型显示在图1的左边。</p>
<p><strong>前端</strong><br>我们的编码器集成了 <strong>梅尔频谱图（MelSpectrogram）</strong> 和 <strong>双耳相位差（Interaural Phase Difference, IPD）</strong>。如图 1 中的 SPATIAL-AST 部分所示，我们首先使用短时傅里叶变换（STFT）将时域信号 $x(n)$ 转换为频域表示 $ X(t, f) $，其表示形式如公式 2 所示：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201210548555.png" alt="image-20241201210548555" style="zoom:50%;" />

<p>这里$w ( n )$表示一个$N$点窗函数，$i$表示双耳记录中的左或右声道。</p>
<p>随后，我们基于这些频域信号$X_i ( t , f)$计算了Mel谱图和双耳相位差( IPD )。根据式( 3 )计算每个通道的Mel - Spectrogram，由左、右通道的相位谱图得到IPD，如式( 4 )所示：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201210726532.png" alt="image-20241201210726532" style="zoom:50%;" />

<p>其中$melW$ 是一个$M-bin$ 的Mel滤波器组。</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201211504959.png" alt="image-20241201211504959" style="zoom:50%;" />

<p>为了解决双耳相位差（IPD）中的相位缠绕导致的数值不稳定问题，我们应用了余弦和正弦变换，并使用权重 $melW$ 对其进行调整，以与梅尔频谱图的维度对齐。最终的前端输出 $Z $ 是这些处理后组件的拼接结果，包括左声道和右声道的梅尔频谱图，以及 IPD 的余弦和正弦变换，所有部分按照公式 5 组合：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201211642830.png" alt="image-20241201211642830" style="zoom:50%;" />

<p><strong>主干网络</strong><br>初始输入 $Z $ 首先通过一个 $3 \times 3 $ 的二维卷积层，随后经过一个批归一化层（Batch Normalization，Ioffe &amp; Szegedy, 2015）和一个 GELU 激活函数（Hendrycks &amp; Gimpel, 2016）层，用于融合跨通道信息。接着，我们采用一个内核大小为 $16 \times 16 $ 的 Patch-Embed CNN，并在时间和频率维度上设置对应的步幅，确保特征被分割为非重叠的补丁（patch）tokens。</p>
<p>为了提取空间线索，我们在音频 tokens 的开头拼接了三个 [CLS] tokens，分别用于提取音频的类别、距离和方向信息。随后，所有 tokens 被输入 12 层 Transformer 编码器块。最后，我们使用三个独立的线性层分别处理这三个 [CLS] tokens，以获得对应任务的预测结果。</p>
<p><strong>预训练目标</strong><br>空间编码器被预训练以处理三个任务：声音事件检测、距离预测和方向预测。其中方向预测进一步细分为方位角（azimuth）和俯仰角（elevation）的预测。我们对所有三个任务均采用交叉熵损失函数。为了离散化距离和方向预测的目标值，我们将距离分为 <strong>0.5 米的区间</strong>，将角度（包括方位角和俯仰角）分为 <strong>1 度的区间</strong>。</p>
<p>空间编码器的最终预训练目标是：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201212043650.png" alt="image-20241201212043650" style="zoom:50%;" />

<p>其中$L_{cls}$，$L_{dis}$，$L_{doa}$分别表示探测、距离和方向的损失，而$λ_ 1$，$λ_2$，$λ_3$是超参数。</p>
<h3 id="4-2-BAT：用于空间声音推理的模型"><a href="#4-2-BAT：用于空间声音推理的模型" class="headerlink" title="4.2: BAT：用于空间声音推理的模型"></a>4.2: BAT：用于空间声音推理的模型</h3><p>SPATIAL-AST 本身能够预测输入空间音频的类型、方向和距离。为了扩展其能力，使其能够推理环境中多个声音源的空间关系，我们将 SPATIAL-AST 与 LLaMA-2 7B LLM（Touvron 等，2023b）相结合。输入的空间音频首先由 SPATIAL-AST 编码器处理，随后通过一个投影模块将其输出的 token 映射到 LLaMA-2 的文本嵌入空间。为了实现高效的微调，我们采用了 LLaMA-adapter v2（Gao 等，2023）。</p>
<p>BAT 的微调目标是基于配对的问题文本和对应的音频输入，预测答案文本。模型通过最大化预测下一个答案 token 的概率，在序列中每个位置 $1 \leq \tau \leq T$ 应用交叉熵损失，其数学表示如下：</p>
<img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201212418403.png" alt="image-20241201212418403" style="zoom:50%;" />

<p>其中$a_l$ 表示音频嵌入的第$l$个token，$y_τ$表示文本输出的第$τ$个token。</p>
<p>为了训练 BAT，我们设计了一套从感知到推理的课程学习方案，具体如表 2 所示。训练过程从“热身”阶段开始，重点是单声源感知任务（问题类型 A 和 B），使模型能够适应音频模态输入，并推断单个音源的基本属性，包括其类别、距离和方向。在第二阶段，我们引入多声源场景（问题类型 C 和 D），模型通过回答关于混合音频中特定声源的问题，学习隐式地执行声源分离。最后阶段解锁了 BAT 的全部能力，整合推理任务（问题类型 E），如解释多个声音源之间的空间关系和距离。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201212738854.png" alt="image-20241201212738854"></p>
<p>该课程学习的重要性在附录 H 的表 10 中得到了验证。</p>
<h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5. 实验"></a>5. 实验</h2><h3 id="5-1-实施细节"><a href="#5-1-实施细节" class="headerlink" title="5.1 实施细节"></a>5.1 实施细节</h3><p><strong>输入音频处理</strong><br>首先对 10 秒的音频源进行响度归一化，通过缩放确保每个片段具有相同的总声音能量。经过 SoundSpaces 提供的房间脉冲响应卷积后，对片段进行修剪或填充至 10 秒，以与 AudioMAE（Huang 等，2022）保持一致。生成的波形为双耳音频，具有两个声道，采样率为 32kHz。短时傅里叶变换（STFT）和梅尔频谱图计算使用窗口大小为 1024，步幅为 320，并设 128 个梅尔频带。对于来自 AudioSet 的 10 秒录音，拼接后的梅尔频谱图和 IPD 特征维度为 ( (4, 1024, 128) )。</p>
<p><strong>空间音频编码器</strong><br>在训练过程中，我们在时间和频率维度应用 0.25 的补丁掩码比率，未被掩盖的 tokens 与三个可学习的 [CLS] tokens 进行拼接。Transformer 块的权重通过官方预训练的 AudioMAE（Huang 等，2022）检查点初始化。编码器在 8 张 RTX 3090 GPU 上进行训练，每个 epoch 约需 10 分钟。</p>
<p>考虑到声音事件检测比距离和方向预测更具挑战性，我们分两阶段训练编码器。在第一阶段，仅关注检测损失，将公式 6 中的权重 $\lambda_2$ 和 $ \lambda_3 $ 设置为 0。在第二阶段，重新引入距离和方向的损失，并调整权重为 $\lambda_1 &#x3D; 1250$, $\lambda_2 &#x3D; 1$,$ \lambda_3 &#x3D; 2 $，以提升模型的综合能力。这种两阶段预训练方法对于在不同任务中实现平衡性能至关重要。附录 E 提供了不同权重对模型任务性能的影响分析。</p>
<p><strong>大型语言模型（LLM）</strong><br>与 LLaMA-adapter v2 的原始两阶段可学习参数设计不同，我们同时训练了零初始化的注意力、投影、归一化、偏置和缩放参数，以简化训练过程。训练在 8 张 V100 GPU 上完成。生成时使用贪婪解码，温度设置为 0.1，nucleus sampling（Holtzman 等，2020）中的 top-p 设置为 0.75。</p>
<p><strong>基线模型</strong><br>作为编码器基线对比，我们选择了已建立的模型，如 AudioMAE（Huang 等，2022）和 SELDnet（Adavanne 等，2018）。为确保公平比较，将 SELDnet 的特征提取网络扩展为包含 12 层 Transformer 块，使比较架构的模型参数大约相当于 90M。此外，为 BAT 提供了一个使用单声道输入的版本进行对比，其架构与 LTU（Gong 等，2024）类似，用作基线。</p>
<p><strong>评估指标</strong><br>在空间音频编码器的评估中（见表 3），我们使用以下指标：</p>
<ul>
<li><p><strong>平均精度（mAP）</strong>：用于声音事件检测。</p>
</li>
<li><p><strong>20°误差率（ER20°）</strong> 和 <strong>平均角度误差（MAE）</strong>：用于到达方向（DoA）准确性评估，分别评估预测与参考点偏差是否超过 20°，以及预测与真值之间的平均角度偏差。</p>
</li>
<li><p><strong>距离误差率（DER）</strong>：用于测量预测距离是否在实际位置 0.5 米以内的准确性。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201213526733.png"></p>
<p>在 BAT 的评估中（见表 4），我们应用了类似的指标，包括 mAP 和 DER。对于三维空间划分为八个区域的 DoA 任务，我们使用准确率（Acc）作为性能指标。此外，对于问题类型 E 的推理任务，采用二元准确率（BA）作为评估指标。</p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201213904318.png" alt="image-20241201213904318"></p>
<h3 id="5-2-Spatial-Ast在SELD任务上的性能"><a href="#5-2-Spatial-Ast在SELD任务上的性能" class="headerlink" title="5.2 Spatial - Ast在SELD任务上的性能"></a>5.2 Spatial - Ast在SELD任务上的性能</h3><p>表 3 展示了我们提出的 SPATIAL-AST 音频编码器的实验结果。我们将 SPATIAL-AST 与当前最先进的模型（如 AudioMAE（Huang 等，2022）和 SELDnet（Adavanne 等，2018））进行了比较，同时比较了单声道和双耳数据在空间线索提取上的表现。此外，我们还探讨了两种预训练方法（联合训练和两阶段训练）的影响，并强调了 IPD 的重要性。</p>
<hr>
<p><strong>单声道 vs. 双耳音频</strong><br>为了支持单声道输入，我们修改了 $3 \times 3$ 卷积模块，将其输入通道设置为 1，仅输入单声道梅尔频谱图。尽管双耳数据通常提供更丰富的空间信息，但我们的编码器在单声道输入下表现出强大的性能，尤其是在声音事件检测（SED）和距离预测（DP）任务中。</p>
<ul>
<li>在单声道输入下，编码器的平均精度（mAP）达到 <strong>51.39</strong>，超过了双耳输入的 <strong>49.94</strong> 和 AudioMAE 的 <strong>47.18</strong>。</li>
<li>此外，使用单声道输入的编码器在距离误差率（DER）上也优于使用双耳输入的模型。</li>
<li>然而，由于单声道音频无法捕获空间信息，其在到达方向（DoA）任务上的表现较差，ER20° 为 <strong>95.82</strong>，MAE 为 <strong>88.52</strong>。</li>
</ul>
<p>单声道数据的表现表明其在 SED 和 DP 任务中已经足够有效。这一有效性可以归因于双耳音频对声音类别感知不一定带来额外的益处。附录 D 中的单任务实验结果进一步支持了这一结论。</p>
<hr>
<p><strong>联合训练 vs. 两阶段训练</strong>  </p>
<ul>
<li><strong>单声道数据</strong>：联合训练方法在检测和距离预测任务上优于两阶段训练，达到 mAP 为 <strong>51.39</strong>，DER 为 <strong>26.87</strong>。</li>
<li><strong>双耳数据</strong>：两阶段训练方法在第二阶段的表现显著提升，无论是否使用 IPD。特别是使用 IPD 时，性能提升尤为明显，MAE 降至 <strong>23.89</strong>，DER 降至 <strong>17.94</strong>，这表明多麦克风相位信息对空间音频感知的重要性。这也强调了在双耳设置中采用两阶段训练方法的必要性。</li>
</ul>
<hr>
<p><strong>模型架构</strong><br>相比 AudioMAE 和 SELDnet，我们的编码器在混响环境下表现出更好的性能。由于 AudioMAE 针对单声道输入设计，我们在单声道场景中进行了对比。在这些场景中，我们提出的模型在联合训练和两阶段训练方法上均优于其他模型。尤其值得注意的是，我们的模型在微调训练时间远少于其他模型的情况下实现了这一成就。</p>
<p>为了简化比较，我们将我们的模型与 SELDnet 的联合训练进行了对比，结果显示我们的模型在所有指标上均优于 SELDnet。</p>
<h3 id="5-3-BAT在SPATIALSOUNDQA上的表现"><a href="#5-3-BAT在SPATIALSOUNDQA上的表现" class="headerlink" title="5.3 BAT在SPATIALSOUNDQA上的表现"></a>5.3 BAT在SPATIALSOUNDQA上的表现</h3><p>表 4 展示了 BAT 模型在 SPATIALSOUNDQA 数据集上不同配置的性能表现，并按不同问题类型进行了划分。BAT 在所有问题类型上表现出色，远超随机基线（random baseline）。我们观察到，单阶段 BAT 的性能普遍低于三阶段 BAT 的性能。</p>
<p>值得注意的是，当训练集中移除与声源分离相关的问题类型 C 和 D 时，模型的推理能力出现显著下降。从二元准确率（BA）的结果可以看出，在推理任务中的表现接近随机猜测。对单阶段 BAT 输出的分析表明，模型在处理推理问题时难以区分声音源及其对应的位置。这一发现突出了声源分离任务在训练 BAT 以有效处理复杂推理中的重要性，并强调了多阶段课程学习的重要性，通过逐步引入模型更加复杂的空间音频感知和推理任务。</p>
<p>表中“Monaural BAT”一行显示了使用单声道音频训练的 BAT 的性能。其架构与 LTU（Gong 等，2024）类似，因此我们将其作为基线进行比较。结果表明，单声道 BAT 在推理任务中面临显著挑战，二元准确率（BA）仅为 **54.53%**。这表明单声道音频输入不足以提供复杂推理任务所需的空间信息，进一步强调了 SPATIALSOUNDQA 在学习空间音频推理能力中的重要性。更多消融实验结果详见附录 H。</p>
<h2 id="6-局限性和未来工作"><a href="#6-局限性和未来工作" class="headerlink" title="6. 局限性和未来工作"></a>6. 局限性和未来工作</h2><p>我们相信 BAT 将对空间音频感知与推理以及多模态大型语言模型（LLM）的发展做出重要贡献。然而，与任何研究工具一样，我们的方法中也存在一些固有的局限性和假设，这些需要被充分认识。</p>
<p>我们面临的主要挑战之一是如何从空间音频中提取尽可能多的信息，特别是如何更好地利用相位信息。目前，我们的模型最多支持两个声音源，主要聚焦于音频的处理。展望未来，我们可以进一步扩展到多声源场景，并整合音频和语音处理，以实现更全面的能力。此外，尽管我们当前的框架局限于双耳音频，探索全景声（Ambisonics）可能会提供更沉浸和更真实的空间音频体验。</p>
<p>另外，将 SPATIALSOUNDQA 扩展为更开放式的问答形式，将更符合人类的使用习惯和偏好，允许更广泛的问题和回答范围。</p>
<p>另一个重要的考虑是整合其他模态，例如视觉信息，以补充听觉线索。这种多模态方法可能会增强模型对复杂环境的理解和解释能力。</p>
<p>最后，模拟环境与现实世界（sim2real）的差距仍是需要进一步研究的方面。观察我们的模型在现实场景中的表现，而非仅限于模拟环境，将对评估其实际应用性和有效性至关重要。</p>
<h2 id="7-结论"><a href="#7-结论" class="headerlink" title="7. 结论"></a>7. 结论</h2><p>在本研究中，我们提出了 BAT，这是首个能够处理空间音频输入的大型语言模型（LLM）。为了训练和评估 BAT，我们引入了 SPATIALSOUNDQA，这是首个大规模的基于空间音频的问答数据集。同时，我们提出了 SPATIAL-AST，一种新颖的空间音频编码器，能够高效地处理声音事件检测、空间定位和距离估计。BAT 和 SPATIALSOUNDQA 展示了 LLM 在空间音频推理方面的巨大潜力。</p>
<p>此外，SPATIALSOUNDQA 为未来的研究提供了丰富的可能性，例如对环境本身（材料、形状、布局）的推理，对移动声音的建模以解决跟踪问题，或结合 SoundSpaces2.0 原生支持的视觉模态。</p>
<h2 id="影响部分"><a href="#影响部分" class="headerlink" title="影响部分"></a>影响部分</h2><p>我们的研究围绕 BAT（首个基于空间音频的大型语言模型）及 SPATIALSOUNDQA 数据集展开，在多个领域具有广泛的影响潜力：</p>
<ol>
<li><p><strong>空间音频的进步</strong>：<br>我们的研究解决了空间音频感知与推理中的关键空白。通过开发专为空间音频设计的 LLM，我们为在空间声音线索至关重要的应用（如虚拟现实、游戏和音频工程）开辟了新可能。这将推动这些领域更加沉浸式和逼真的体验。</p>
</li>
<li><p><strong>多模态大型语言模型的增强</strong>：<br>将空间音频整合到 LLM 中标志着迈向真正多模态 AI 系统的重要一步。我们的模型不仅展示了 LLM 处理复杂空间音频信息的能力，还为未来研究结合听觉与其他感官模态（如视觉或触觉）铺平了道路，从而创建更复杂的 AI 模型。</p>
</li>
<li><p><strong>赋能具身 AI</strong>：<br>对空间声音进行解释和推理的能力可以显著增强具身 AI 系统（如机器人或自动驾驶车辆）。这些系统可以利用空间音频线索更好地导航和与环境交互，从而在现实场景中实现更高效和更安全的应用。</p>
</li>
</ol>
<h3 id="A-为什么我们要用LLM"><a href="#A-为什么我们要用LLM" class="headerlink" title="A.为什么我们要用LLM"></a>A.为什么我们要用LLM</h3><p>在我们的空间音频推理框架中使用大型语言模型（LLMs）是必要的，原因如下：</p>
<ol>
<li><p><strong>支持自然语言形式的复杂问题</strong>：<br>LLMs 能够以自然语言提出复杂问题，例如“狗比立体声音响离你更远吗？”或“狗是在立体声音响的左边还是右边？”。这一能力消除了对一系列独立处理步骤的需求，如声源分离、分类、到达方向（DoA）、距离分类器和基于规则的模板，这些步骤会极大限制能够解决的问题类型。</p>
</li>
<li><p><strong>支持端到端联合训练</strong>：<br>LLMs 使空间音频编码器和推理模块可以以端到端的方式联合训练，减少了分段式处理方法中固有的错误传播风险。</p>
</li>
<li><p><strong>提升泛化能力和适应性</strong>：<br>LLMs 能够在不同但表达相似的问题之间进行泛化，增强了模型在响应各种空间音频相关查询时的适应性和多样性。</p>
</li>
<li><p><strong>拓展 LLM 应用领域</strong>：<br>此任务还旨在进一步扩展 LLM 的应用边界，展示其在传统文本和图像处理之外领域的多功能性和有效性。</p>
</li>
</ol>
<h3 id="B-超参数"><a href="#B-超参数" class="headerlink" title="B. 超参数"></a>B. 超参数</h3><p>我们在表5中给出了SPATIAL - AST &amp; BAT的具体训练超参数配置。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201214954542.png" alt="image-20241201214954542"></p>
<h3 id="C-房间声学参数"><a href="#C-房间声学参数" class="headerlink" title="C. 房间声学参数"></a>C. 房间声学参数</h3><p>表 6 展示了在 Matterport3D（Chang 等，2017）训练设置中常见房间类型的平均 RT60 值。RT60 是一个标准的声学测量指标，定义为声音压力级衰减 60 dB 所需的时间（Hak 等，2012）。由于房间大小、物体布置和建筑材料等因素的影响，不同房间的 RT60 值各不相同，这些因素共同决定了房间的声学特性。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201215048143.png" alt="image-20241201215048143"></p>
<h3 id="D-单任务环境下Spatial-Ast的性能"><a href="#D-单任务环境下Spatial-Ast的性能" class="headerlink" title="D. 单任务环境下Spatial - Ast的性能"></a>D. 单任务环境下Spatial - Ast的性能</h3><p>我们在单任务场景下评估编码器的性能，具体关注分类、距离预测和到达方向（DoA）。如表 7 所示，我们分析了包含或省略双耳相位差（IPD）以及单声道与双耳格式对每项任务性能的影响。研究结果显示，IPD 通常会提升所有任务的性能，这表明空间信息的价值。</p>
<p>此外，我们发现单声道数据在声音事件检测和距离预测等任务中提供了足够的信息，在距离预测任务中取得了最佳表现，并在声音事件检测中与双耳数据表现相当。然而，在涉及方向感知的任务中，双耳数据显著提升了性能。这进一步突显了编码器处理多任务的潜力，以及音频格式在空间音频感知中的重要性。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201215150359.png" alt="image-20241201215150359"></p>
<h3 id="E-空间音频编码器Spatial-Ast的消融"><a href="#E-空间音频编码器Spatial-Ast的消融" class="headerlink" title="E. 空间音频编码器Spatial - Ast的消融"></a>E. 空间音频编码器Spatial - Ast的消融</h3><p><strong>损失权重</strong><br>如公式 6 所示，我们固定 $\lambda_2 &#x3D; 1$  和 $\lambda_3 &#x3D; 2 $，通过调整 $ \lambda_1 $ 的值，监测模型性能的变化。如图 2 所示，随着 $ \lambda_1$ 的增加，平均精度（mAP）持续提高。然而，在到达方向（DoA）的平均角度误差（MAE）指标上，性能逐渐下降。这表明在这两个指标之间存在权衡。</p>
<p>为了优先考虑分类任务，我们选择能够实现最高 mAP 的设置。在后续实验中，我们将超参数固定为 $\lambda_1 &#x3D; 1250$, $\lambda_2 &#x3D; 1$, $\lambda_3 &#x3D; 2 $。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201215329123.png" alt="image-20241201215329123"><br><strong>音频归一化与混响</strong>  </p>
<p>在此消融实验中，我们评估了音频归一化和混响对编码器性能的影响。实验使用单声道音频，并将 $\lambda_1$,$ \lambda_2$, $\lambda_3$  固定为 1250、1 和 2，以简化分析，结果如表 8 所示。</p>
<p>添加混响确实对模型检测声音事件的能力产生影响，因为混响增加了任务难度，但未提供额外的事件信息。对比第 1、2 行与第 3、4 行可见，移除混响显著提升了平均精度（mAP），从 <strong>51.13%</strong> 提高到 **52.64%**。然而，值得注意的是，没有混响的音频缺乏空间信息，这导致距离感知的距离误差率（DER）接近随机猜测的水平。</p>
<p>根据图 3 所述的距离分布，随机采样的 DER 约为 **67%**，当采样限制为特定距离（如 1.0、1.5 和 2.0 米）时，DER 降至约 **53%**。音频归一化的实施进一步提升了编码器在分类和距离任务中的性能。对比第 3 行和第 4 行，mAP 从 <strong>51.13%</strong> 略微提升至 **51.39%**，而 DER 则显著下降，从 <strong>39.57%</strong> 降至 **26.87%**，这突显了音频归一化在本模型中的优势。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201215335608.png" alt="image-20241201215335608"></p>
<h3 id="F-SPATIALSOUNDQA-Generation"><a href="#F-SPATIALSOUNDQA-Generation" class="headerlink" title="F. SPATIALSOUNDQA Generation"></a>F. SPATIALSOUNDQA Generation</h3><p>在生成 <strong>SPATIALSOUNDQA</strong> 的过程中，我们模拟了多个声音事件同时发生的空间音频场景，每个声音事件来自接收器的不同方向和距离。例如，如图 4 所示，假设两个声音源 A 和 B 同时活动。声音事件 A 的方位角为 115 度，俯仰角为 40 度，距离接收器 2.5 米。而声音源 B 的坐标为 -40 度的方位角和 -25 度的俯仰角，距离接收器 6 米。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201215622444.png" alt="image-20241201215622444"></p>
<p>需要注意的是，在实际数据集构建中，方向和距离参数通常不像此示例中那样截然不同。基于这些真值空间参数，我们可以构建关于两个声音源之间空间关系的多样化问题和答案，如表 1 所示。此外，我们为每种问题类型补充了详细信息，包括初始提示和对应的答案模板。</p>
<h3 id="G-Prompt-细节"><a href="#G-Prompt-细节" class="headerlink" title="G. Prompt 细节"></a>G. Prompt 细节</h3><p>对于我们的基于空间音频的大型语言模型 BAT，我们设计了一个特定的提示结构，以引导其对各种空间音频任务的响应。提示的结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;Based on the audio you’ve heard, refer to the instruction and provide a response.</span><br><span class="line"></span><br><span class="line">### Instruction:</span><br><span class="line">&#123;instruction&#125;</span><br><span class="line"></span><br><span class="line">### Response:&quot;</span><br></pre></td></tr></table></figure>

<p>在生成输入时，我们将 <code>&#123;instruction&#125;</code> 替换为具体的问题，并在输入的开头拼接一组固定长度的可学习查询（query）tokens。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201215722854.png" alt="image-20241201215722854"></p>
<h3 id="H-BAT的消融"><a href="#H-BAT的消融" class="headerlink" title="H. BAT的消融"></a>H. BAT的消融</h3><p>在表 10 中，我们详细分析了不同训练课程对 BAT 推理能力的影响。起初，模型在推理任务中表现出困难，特别是在面对不熟悉的推理类问题时，其无法准确生成二元（是&#x2F;否）响应。这从模型在推理任务中的表现可以看出，其初始二元准确率（BA）较低。</p>
<p>令人意外的是，单阶段版本的 BAT 在推理任务中取得了相当不错的整体 BA，达到 **73.55%**。然而，当我们从单阶段训练中移除与声源分离相关的问题类型 C 和 D 后，模型的推理能力显著下降，表现接近随机水平，BA 仅为 **54.68%**。</p>
<p>有趣的是，当 BAT 在最终推理阶段之前，先接受声源分离任务的训练，即使在最后阶段不再包含分离相关的问题，它依然能够有效地进行推理。表 10 的最后一行展示了通过我们的三阶段课程学习所实现的最佳结果，表明全面的训练方法对于赋予 BAT 稳健的推理能力至关重要。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201215839719.png" alt="image-20241201215839719"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Gavin-rep/Pictures@master/image-20241201215851890.png" alt="image-20241201215851890"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Gavin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/12/01/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ABAT%EF%BC%9ALearning%20to%20reason%20about%20Spatial%20Sounds%20with%20Large%20Language%20Models/">http://example.com/2024/12/01/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ABAT%EF%BC%9ALearning%20to%20reason%20about%20Spatial%20Sounds%20with%20Large%20Language%20Models/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Gavin</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A3%B0%E6%BA%90%E5%AE%9A%E4%BD%8D/">声源定位</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post_share"><div class="social-share" data-image="https://i.ibb.co/bXjJh1J/image.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/12/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9Awav2pos-%20Sound%20Source%20Localization%20using%20%20Masked%20Autoencoders/" title="文献阅读: wav2pos: Sound Source Localization using  Masked Autoencoders"><img class="cover" src="https://i.ibb.co/XkLzxY7/image.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">文献阅读: wav2pos: Sound Source Localization using  Masked Autoencoders</div></div></a></div><div class="next-post pull-right"><a href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ALanguage%20Quantized%20AutoEncoders%EF%BC%9ATowards%20Unsupervised%20Text-Image%20Alignment/" title="文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment"><img class="cover" src="https://i.ibb.co/zGtqjf2/image.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/12/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9Awav2pos-%20Sound%20Source%20Localization%20using%20%20Masked%20Autoencoders/" title="文献阅读: wav2pos: Sound Source Localization using  Masked Autoencoders"><img class="cover" src="https://i.ibb.co/XkLzxY7/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-15</div><div class="title">文献阅读: wav2pos: Sound Source Localization using  Masked Autoencoders</div></div></a></div><div><a href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ALanguage%20Quantized%20AutoEncoders%EF%BC%9ATowards%20Unsupervised%20Text-Image%20Alignment/" title="文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment"><img class="cover" src="https://i.ibb.co/zGtqjf2/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-19</div><div class="title">文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.ibb.co/0fP2mb0/image.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Gavin</div><div class="author-info__description">我的过去常常追赶着我</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/alexandergwm"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/alexandergwm" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:wenmiaogao@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">May you stay young forever, do the thing in your own zone.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text">1.介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%9B%B8%E5%85%B3%E4%BB%BB%E5%8A%A1"><span class="toc-number">3.</span> <span class="toc-text">2. 相关任务</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E7%A9%BA%E9%97%B4%E9%9F%B3%E9%A2%91"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 空间音频</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%A3%B0%E9%9F%B3%E4%BA%8B%E4%BB%B6%E5%AE%9A%E4%BD%8D%E4%B8%8E%E6%A3%80%E6%B5%8B"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 声音事件定位与检测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 多模态大语言模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-SPATIALSOUNDQA"><span class="toc-number">4.</span> <span class="toc-text">3 SPATIALSOUNDQA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E7%A9%BA%E9%97%B4%E9%9F%B3%E9%A2%91%E7%9A%84%E7%94%9F%E6%88%90"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 空间音频的生成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E9%97%AE%E7%AD%94%E5%AF%B9%E7%94%9F%E6%88%90"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 问答对生成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A3%B0%E9%9F%B3%E4%BA%8B%E4%BB%B6%E6%A3%80%E6%B5%8B%EF%BC%88Type-A-%E5%92%8C-Type-C%EF%BC%89"><span class="toc-number">4.3.</span> <span class="toc-text">声音事件检测（Type A 和 Type C）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E5%90%91%E4%B8%8E%E8%B7%9D%E7%A6%BB%E4%BC%B0%E8%AE%A1%EF%BC%88%E7%B1%BB%E5%9E%8B-B-%E5%92%8C-D%EF%BC%89"><span class="toc-number">4.4.</span> <span class="toc-text">方向与距离估计（类型 B 和 D）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A9%BA%E9%97%B4%E6%8E%A8%E7%90%86%EF%BC%88%E7%B1%BB%E5%9E%8B-E%EF%BC%89"><span class="toc-number">4.5.</span> <span class="toc-text">空间推理（类型 E）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%96%B9%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">4.方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E7%A9%BA%E9%97%B4%E9%9F%B3%E9%A2%91%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%9ASpatial-Ast"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 空间音频编码器：Spatial-Ast</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-BAT%EF%BC%9A%E7%94%A8%E4%BA%8E%E7%A9%BA%E9%97%B4%E5%A3%B0%E9%9F%B3%E6%8E%A8%E7%90%86%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.2.</span> <span class="toc-text">4.2: BAT：用于空间声音推理的模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%AE%9E%E9%AA%8C"><span class="toc-number">6.</span> <span class="toc-text">5. 实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E5%AE%9E%E6%96%BD%E7%BB%86%E8%8A%82"><span class="toc-number">6.1.</span> <span class="toc-text">5.1 实施细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Spatial-Ast%E5%9C%A8SELD%E4%BB%BB%E5%8A%A1%E4%B8%8A%E7%9A%84%E6%80%A7%E8%83%BD"><span class="toc-number">6.2.</span> <span class="toc-text">5.2 Spatial - Ast在SELD任务上的性能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-BAT%E5%9C%A8SPATIALSOUNDQA%E4%B8%8A%E7%9A%84%E8%A1%A8%E7%8E%B0"><span class="toc-number">6.3.</span> <span class="toc-text">5.3 BAT在SPATIALSOUNDQA上的表现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%B1%80%E9%99%90%E6%80%A7%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C"><span class="toc-number">7.</span> <span class="toc-text">6. 局限性和未来工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E7%BB%93%E8%AE%BA"><span class="toc-number">8.</span> <span class="toc-text">7. 结论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%B1%E5%93%8D%E9%83%A8%E5%88%86"><span class="toc-number">9.</span> <span class="toc-text">影响部分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E8%A6%81%E7%94%A8LLM"><span class="toc-number">9.1.</span> <span class="toc-text">A.为什么我们要用LLM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-number">9.2.</span> <span class="toc-text">B. 超参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-%E6%88%BF%E9%97%B4%E5%A3%B0%E5%AD%A6%E5%8F%82%E6%95%B0"><span class="toc-number">9.3.</span> <span class="toc-text">C. 房间声学参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#D-%E5%8D%95%E4%BB%BB%E5%8A%A1%E7%8E%AF%E5%A2%83%E4%B8%8BSpatial-Ast%E7%9A%84%E6%80%A7%E8%83%BD"><span class="toc-number">9.4.</span> <span class="toc-text">D. 单任务环境下Spatial - Ast的性能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#E-%E7%A9%BA%E9%97%B4%E9%9F%B3%E9%A2%91%E7%BC%96%E7%A0%81%E5%99%A8Spatial-Ast%E7%9A%84%E6%B6%88%E8%9E%8D"><span class="toc-number">9.5.</span> <span class="toc-text">E. 空间音频编码器Spatial - Ast的消融</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#F-SPATIALSOUNDQA-Generation"><span class="toc-number">9.6.</span> <span class="toc-text">F. SPATIALSOUNDQA Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#G-Prompt-%E7%BB%86%E8%8A%82"><span class="toc-number">9.7.</span> <span class="toc-text">G. Prompt 细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#H-BAT%E7%9A%84%E6%B6%88%E8%9E%8D"><span class="toc-number">9.8.</span> <span class="toc-text">H. BAT的消融</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/12/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9Awav2pos-%20Sound%20Source%20Localization%20using%20%20Masked%20Autoencoders/" title="文献阅读: wav2pos: Sound Source Localization using  Masked Autoencoders"><img src="https://i.ibb.co/XkLzxY7/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: wav2pos: Sound Source Localization using  Masked Autoencoders"/></a><div class="content"><a class="title" href="/2024/12/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9Awav2pos-%20Sound%20Source%20Localization%20using%20%20Masked%20Autoencoders/" title="文献阅读: wav2pos: Sound Source Localization using  Masked Autoencoders">文献阅读: wav2pos: Sound Source Localization using  Masked Autoencoders</a><time datetime="2024-12-15T04:12:53.312Z" title="发表于 2024-12-15 12:12:53">2024-12-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/01/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ABAT%EF%BC%9ALearning%20to%20reason%20about%20Spatial%20Sounds%20with%20Large%20Language%20Models/" title="文献阅读: BAT: Learning to Reason about Spatial Sounds with Large Language Models"><img src="https://i.ibb.co/bXjJh1J/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: BAT: Learning to Reason about Spatial Sounds with Large Language Models"/></a><div class="content"><a class="title" href="/2024/12/01/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ABAT%EF%BC%9ALearning%20to%20reason%20about%20Spatial%20Sounds%20with%20Large%20Language%20Models/" title="文献阅读: BAT: Learning to Reason about Spatial Sounds with Large Language Models">文献阅读: BAT: Learning to Reason about Spatial Sounds with Large Language Models</a><time datetime="2024-12-01T12:04:59.068Z" title="发表于 2024-12-01 20:04:59">2024-12-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ALanguage%20Quantized%20AutoEncoders%EF%BC%9ATowards%20Unsupervised%20Text-Image%20Alignment/" title="文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment"><img src="https://i.ibb.co/zGtqjf2/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment"/></a><div class="content"><a class="title" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ALanguage%20Quantized%20AutoEncoders%EF%BC%9ATowards%20Unsupervised%20Text-Image%20Alignment/" title="文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment">文献阅读: Language Quantized AutoEncoders:  Towards Unsupervised Text-Image Alignment</a><time datetime="2024-11-19T14:09:10.711Z" title="发表于 2024-11-19 22:09:10">2024-11-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/" title="文献阅读: Masked Autoencoders that Listen"><img src="https://i.ibb.co/9HZr35S/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Masked Autoencoders that Listen"/></a><div class="content"><a class="title" href="/2024/11/19/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20that%20listen/" title="文献阅读: Masked Autoencoders that Listen">文献阅读: Masked Autoencoders that Listen</a><time datetime="2024-11-19T13:34:57.816Z" title="发表于 2024-11-19 21:34:57">2024-11-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners"><img src="https://i.ibb.co/gD7cJGh/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Masked Autoencoders Are Scalable Vision Learners"/></a><div class="content"><a class="title" href="/2024/11/17/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners%20/" title="文献阅读: Masked Autoencoders Are Scalable Vision Learners">文献阅读: Masked Autoencoders Are Scalable Vision Learners</a><time datetime="2024-11-17T12:30:01.804Z" title="发表于 2024-11-17 20:30:01">2024-11-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Gavin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Have a nice day!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>